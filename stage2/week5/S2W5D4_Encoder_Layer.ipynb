{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b034b64e",
   "metadata": {},
   "source": [
    "# S2W5D4: ç»„è£…æ—¶åˆ» - éª¨æ¶ä¸è‚Œè‚‰\n",
    "\n",
    "ä»Šæ—¥ä»»åŠ¡ï¼š\n",
    "\n",
    "1. **æ®‹å·®è¿æ¥ (Residual Connection)**\n",
    "2. **å±‚å½’ä¸€åŒ– (Layer Normalization)**\n",
    "3. **å‰é¦ˆç½‘ç»œ (Feed-Forward Network)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b0253f",
   "metadata": {},
   "source": [
    "## 1 ç†è®ºä¸‰å‰‘å®¢\n",
    "\n",
    "### 1.1 æ®‹å·®è¿æ¥ï¼ˆAddï¼‰\n",
    "\n",
    "- **å…¬å¼**ï¼š$Output = x + \\text{Sublayer}(x)$\n",
    "- **ä½œç”¨**ï¼š\n",
    "    - æ·±åº¦ç¥ç»ç½‘ç»œæœ€æ€•æ¢¯åº¦ä¼ ä¸å›å»ï¼Œæœ‰äº†åŠ å·ï¼Œæ¢¯åº¦å¯ä»¥ç›´æ¥æ²¿ç€$x$è¿™æ¡â€œé«˜é€Ÿå…¬è·¯â€æ— æŸåœ°ä¼ å›åˆ°åº•å±‚ã€‚\n",
    "    - è¿™æ˜¯ResNetçš„æ ¸å¿ƒé—äº§ï¼Œä¹Ÿæ˜¯ç°ä»£æ·±åº¦å­¦ä¹ èƒ½åšæ·±çš„åŸºç¡€ã€‚\n",
    "\n",
    "### 1.2 å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalization - Normï¼‰\n",
    "\n",
    "- **å…¬å¼**ï¼š$\\text{LayerNorm}(x)$\n",
    "- **ä½œç”¨**ï¼š\n",
    "    - æŠŠæ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾å‘é‡æ‹‰æˆå‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1 çš„åˆ†å¸ƒã€‚\n",
    "    - **é¢è¯•å¿…é—®**: **ä¸ºä»€ä¹ˆä¸ç”¨ BatchNorm (BN)ï¼Ÿ**\n",
    "        - **BN (æŒ‰æ‰¹å½’ä¸€åŒ–)**: ä¾èµ– Batch Sizeï¼Œä¸”å‡è®¾åŒä¸€ä¸ª Batch é‡Œçš„æ•°æ®åˆ†å¸ƒä¸€è‡´ã€‚è¿™å¯¹ NLP ä¸å‹å¥½ï¼ˆå¥å­é•¿åº¦ä¸ä¸€ï¼ŒToken å·®å¼‚å¤§ï¼‰ã€‚\n",
    "        - **LN (æŒ‰å±‚å½’ä¸€åŒ–)**: â€œè‡ªå·±ç®¡è‡ªå·±â€ã€‚ä¸ç®¡ Batch æœ‰å¤šå¤§ï¼Œæˆ‘åªæŠŠ**è¿™ä¸€å¥è¯**é‡Œçš„ç‰¹å¾åšå½’ä¸€åŒ–ã€‚è¿™å¯¹ RNN/Transformer è¿™ç§å˜é•¿åºåˆ—æœ€åˆé€‚ã€‚\n",
    "\n",
    "### 1.3 å‰é¦ˆç½‘ç»œ\n",
    "\n",
    "- **ç»“æ„**ï¼š`Linear` -\\> `ReLU` -\\> `Linear`\n",
    "- **ä½œç”¨**ï¼š\n",
    "    - Attention è´Ÿè´£â€œçœ‹â€å’Œâ€œæ”¶é›†ä¿¡æ¯â€ï¼ˆä½ æ˜¯è°ï¼Œæˆ‘åœ¨å“ªï¼‰ã€‚\n",
    "    - FFN è´Ÿè´£â€œæ€è€ƒâ€å’Œâ€œå¤„ç†ä¿¡æ¯â€ï¼ˆæˆ‘è¯¥æ€ä¹ˆç†è§£è¿™äº›ä¿¡æ¯ï¼‰ã€‚\n",
    "    - **ç»´åº¦å˜åŒ–**: é€šå¸¸ä¼šå…ˆæŠŠç»´åº¦æ”¾å¤§ 4 å€ï¼ˆ$512 \\rightarrow 2048$ï¼‰ï¼Œç„¶åå†ç¼©å›æ¥ï¼ˆ$2048 \\rightarrow 512$ï¼‰ã€‚è¿™å« **â€œå‡ç»´æ‰“å‡»â€**ï¼Œä¸ºäº†åœ¨è¿™ä¸ªé«˜ç»´ç©ºé—´é‡Œç†æ¸…éçº¿æ€§å…³ç³»ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0584f",
   "metadata": {},
   "source": [
    "## 2 ä»£ç å®ç°ï¼ˆBuilding Blocksï¼‰\n",
    "\n",
    "åœ¨è¿™é‡Œå°†å®ç°ä¸¤ä¸ªç±»ï¼š`PositionwiseFeedForward` å’Œ `EncoderLayer`ã€‚\n",
    "\n",
    "### 2.1 å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7871c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        d_modelï¼šæ¨¡å‹è¾“å…¥ç»´åº¦ï¼ˆ512ï¼‰\n",
    "        d_ffï¼šéšè—å±‚çš„ç»´åº¦ï¼ˆé€šå¸¸æ˜¯4å€d_model=2048ï¼‰\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        # ç¬¬ä¸€å±‚ï¼šçº¿æ€§å˜æ¢ + ReLU\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        # ç¬¬äºŒå±‚ï¼šçº¿æ€§å˜æ¢\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU() # æˆ–è€… GELU (BERTå¸¸ç”¨)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Seq, d_model]\n",
    "        \n",
    "        # 1. å‡ç»´: 512 -> 2048\n",
    "        # å…¬å¼: ReLU(xW1 + b1)W2 + b2\n",
    "        inter = self.activation(self.w_1(x))\n",
    "        inter = self.dropout(inter)\n",
    "        \n",
    "        # 2. é™ç»´: 2048 -> 512\n",
    "        output = self.w_2(inter)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c974e9b",
   "metadata": {},
   "source": [
    "## 2.2 å®Œæ•´çš„ Encoder Layer (ç»„è£…ï¼)\n",
    "\n",
    "è¿™æ˜¯ Transformer ä¸­é‡å¤å †å äº† $N=6$ æ¬¡çš„é‚£ä¸ªå•å…ƒã€‚\n",
    "ç»“æ„å£è¯€ï¼š**Attention -\\> Add & Norm -\\> FFN -\\> Add & Norm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96b2821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.models.transformer_components import MultiHeadAttention\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        # 1. Self-Attention æ¨¡å—\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "        # 2. Feed-Forward æ¨¡å—\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # 3. ä¸¤ä¸ªAdd & Normæ¨¡å—\n",
    "        # LayerNormçš„å‚æ•°æ˜¯ç‰¹å¾ç»´åº¦d_model\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # 4. Dropout (é˜²æ­¢è¿‡æ‹Ÿåˆçš„ç¥å™¨)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [Batch, Seq, d_model]\n",
    "        mask: [Batch, 1, 1, Seq] (ç”¨äºé®æŒ¡ Pad)\n",
    "        \"\"\"\n",
    "        # --- å­å±‚1ï¼šMulti-Head Attention ---\n",
    "        # 1. è®¡ç®—Attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "\n",
    "        # 2. Add & Norm\n",
    "        # æ®‹å·®è¿æ¥: x + Dropout(Sublayer(x))\n",
    "        # è¿™é‡Œçš„é¡ºåºæ˜¯ Post-Norm: Norm(x + Sublayer(x)) -> åŸå§‹è®ºæ–‡å†™æ³•\n",
    "        # ç°ä»£å¾ˆå¤šæ¨¡å‹(GPT)ç”¨ Pre-Norm: x + Sublayer(Norm(x))\n",
    "        # æˆ‘ä»¬æŒ‰ç»å…¸çš„æ¥ï¼š\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # --- å­å±‚ 2: Feed-Forward ---\n",
    "        # 1. è®¡ç®— FFN\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # 2. Add & Norm\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd07aa",
   "metadata": {},
   "source": [
    "## 3 éªŒè¯ä¸æµ‹è¯• (Verification)\n",
    "\n",
    "éªŒè¯å®ƒæ˜¯ç§¯æœ¨çš„å…³é”®ï¼š**è¾“å…¥è¿›å»æ˜¯ä»€ä¹ˆå½¢çŠ¶ï¼Œåå‡ºæ¥è¿˜å¾—æ˜¯ä»€ä¹ˆå½¢çŠ¶**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a31654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "âœ… EncoderLayer æµ‹è¯•é€šè¿‡ï¼å®ƒå¯ä»¥æ— é™å †å äº†ã€‚\n",
      "è¿™ä¸€å±‚ Encoder åŒ…å«äº† 3152384 ä¸ªå‚æ•°ã€‚\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048 # 4å€\n",
    "dropout = 0.1\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# 1. å®ä¾‹åŒ– EncoderLayer\n",
    "encoder_layer = EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "# 2. åˆ›å»ºå‡æ•°æ®\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 3. å‰å‘ä¼ æ’­\n",
    "output = encoder_layer(x)\n",
    "\n",
    "# 4. æ£€æŸ¥\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "# éªŒè¯è¾“å…¥è¾“å‡ºåŒæ„\n",
    "assert x.shape == output.shape\n",
    "print(\"âœ… EncoderLayer æµ‹è¯•é€šè¿‡ï¼å®ƒå¯ä»¥æ— é™å †å äº†ã€‚\")\n",
    "\n",
    "# æŸ¥çœ‹å‚æ•°é‡ (æ„Ÿå—ä¸€ä¸‹å±‚çº§)\n",
    "num_params = sum(p.numel() for p in encoder_layer.parameters())\n",
    "print(f\"è¿™ä¸€å±‚ Encoder åŒ…å«äº† {num_params} ä¸ªå‚æ•°ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708253c4",
   "metadata": {},
   "source": [
    "## 4 æ·±åº¦è§£æ & é¢è¯•è€ƒç‚¹ (Interview Checkpoints)\n",
    "\n",
    "### Q1: LayerNorm å’Œ BatchNorm åˆ°åº•æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿ(é«˜é¢‘)\n",
    "\n",
    "  * **å½¢è±¡æ¯”å–»**:\n",
    "      * **BatchNorm**: è€ƒè¯•ç»“æŸåï¼Œè€å¸ˆç®—å…¨ç­åŒå­¦(Batch)çš„å¹³å‡åˆ†ã€‚ä½ çš„åˆ†æ•°æ˜¯ç›¸å¯¹äºå…¨ç­çš„æ’åã€‚å¦‚æœ Batch å¾ˆå°ï¼ˆåªæœ‰ä½ ä¸€ä¸ªäººï¼‰ï¼Œè¿™å¹³å‡åˆ†å°±æ²¡æ„ä¹‰äº†ã€‚\n",
    "      * **LayerNorm**: è€ƒè¯•ç»“æŸåï¼Œç®—**ä½ è‡ªå·±**(Sample)è¯­æ•°è‹±ä¸‰ç§‘çš„å¹³å‡åˆ†ã€‚ä¸ç®¡ç­é‡Œæœ‰å¤šå°‘äººï¼Œä½ çš„â€œåç§‘ç¨‹åº¦â€æ˜¯ç¡®å®šçš„ã€‚\n",
    "  * **ç»“è®º**: NLP æ ·æœ¬ä¹‹é—´ç‹¬ç«‹æ€§å¼ºï¼Œä¸”é•¿åº¦ä¸ä¸€ï¼Œä¸é€‚åˆæ¨ªå‘æ¯”è¾ƒï¼ˆBNï¼‰ï¼Œåªé€‚åˆçºµå‘æ ‡å‡†åŒ–ï¼ˆLNï¼‰ã€‚\n",
    "\n",
    "### Q2: ä¸ºä»€ä¹ˆè¦å…ˆæŠŠç»´åº¦æ”¾å¤§ 4 å€å†ç¼©å›æ¥ (FFN)ï¼Ÿ\n",
    "\n",
    "  * **SVM æ€æƒ³**: ä½ç»´ç©ºé—´çº¿æ€§ä¸å¯åˆ†çš„æ•°æ®ï¼Œæ˜ å°„åˆ°é«˜ç»´ç©ºé—´å¾€å¾€å°±çº¿æ€§å¯åˆ†äº†ã€‚\n",
    "  * **è®°å¿†å®¹é‡**: FFN çš„ä¸¤å±‚ Linear å æ®äº† Transformer æ¨¡å‹ **2/3 çš„å‚æ•°é‡**ã€‚å®ƒæ˜¯æ¨¡å‹å­˜å‚¨â€œçŸ¥è¯†â€çš„ä¸»è¦åœºæ‰€ï¼ˆæ¯”å¦‚â€œå·´é»æ˜¯æ³•å›½çš„é¦–éƒ½â€è¿™ç§çŸ¥è¯†å¾€å¾€å­˜åœ¨ FFN çš„æƒé‡é‡Œï¼‰ã€‚\n",
    "\n",
    "### Q3: è¿™é‡Œçš„ Add & Norm é¡ºåºé—®é¢˜ï¼Ÿ(è¿›é˜¶)\n",
    "\n",
    "  * **Post-Norm (åŸå§‹è®ºæ–‡)**: `Norm(x + SubLayer(x))`ã€‚ä¹Ÿå°±æ˜¯æˆ‘ä»¬ä»Šå¤©å†™çš„ã€‚ä¼˜ç‚¹æ˜¯æ”¶æ•›åæ•ˆæœå¥½ï¼Œç¼ºç‚¹æ˜¯è®­ç»ƒåˆæœŸæ¢¯åº¦å®¹æ˜“ç‚¸ï¼ˆéœ€è¦ Warmupï¼‰ã€‚\n",
    "  * **Pre-Norm (GPT/Llama)**: `x + SubLayer(Norm(x))`ã€‚æŠŠ Norm æ”¾åœ¨å­å±‚å‰é¢ã€‚ä¼˜ç‚¹æ˜¯è®­ç»ƒæå…¶ç¨³å®šï¼Œä¸éœ€è¦ Warmup ä¹Ÿèƒ½è®­ï¼Œæ˜¯ç°åœ¨å¤§æ¨¡å‹çš„æ ‡é…ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8f308",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ğŸš€ ä»Šæ—¥è¡ŒåŠ¨\n",
    "\n",
    "1.  **ä»£ç **: å°† `PositionwiseFeedForward` å’Œ `EncoderLayer` å†™å…¥ Notebook å¹¶è·‘é€šã€‚\n",
    "2.  **å·¥ç¨‹**: æµ‹è¯•é€šè¿‡åï¼Œè¯·æŠŠè¿™ä¸¤ä¸ªç±»ä¹Ÿè¿½åŠ åˆ°ä½ çš„ `src/models/transformer_components.py` æ–‡ä»¶ä¸­ï¼Œè®©ä½ çš„å†›ç«åº“è¶Šæ¥è¶Šä¸°å¯Œã€‚\n",
    "3.  **å…·èº«æ€è€ƒ**:\n",
    "      * åœ¨æœºå™¨äººä¸­ï¼Œ`d_model` ä»£è¡¨äº†å®ƒå¯¹ä¸–ç•Œçš„ç†è§£å¸¦å®½ã€‚\n",
    "      * `EncoderLayer` å †å¾—è¶Šå¤šï¼Œå®ƒçš„æ¨ç†èƒ½åŠ›è¶Šå¼ºï¼ˆèƒ½ç†è§£è¶Šå¤æ‚çš„æŒ‡ä»¤ï¼‰ã€‚\n",
    "      * ä½†æ˜¯ï¼Œå±‚æ•°è¶Šå¤šï¼Œæ¨ç†å»¶è¿Ÿï¼ˆLatencyï¼‰è¶Šé«˜ã€‚ä½œä¸ºç»„é•¿ï¼Œä½ éœ€è¦åœ¨\\*\\*â€œå˜èªæ˜â€**å’Œ**â€œååº”å¿«â€\\*\\*ä¹‹é—´åšæƒè¡¡ã€‚\n",
    "\n",
    "å‡†å¤‡å¥½åï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚æ˜å¤©ï¼ˆDay 5ï¼‰ï¼Œæˆ‘ä»¬å°†æŠŠ $N$ ä¸ª EncoderLayer å †èµ·æ¥ï¼ŒåŠ ä¸Š Embeddingï¼Œç»„æˆ**å®Œæ•´çš„ Transformer Encoder**ï¼Œä¹Ÿå°±æ˜¯å¤§åé¼é¼çš„ **BERT** æ¶æ„ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
