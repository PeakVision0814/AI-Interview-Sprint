{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4cef3a",
   "metadata": {},
   "source": [
    "# S2W5D3: å¤šå¤´æœºåˆ¶ - ä¸“å®¶å›¢é˜Ÿçš„è¯ç”Ÿ\n",
    "\n",
    "å¦‚æœè¯´æ˜¨å¤©çš„å†…å®¹æ˜¯â€œä¸€ä¸ªäººåœ¨è¯»ä¹¦â€ï¼Œä»Šå¤©çš„ä»£ç å°±æ˜¯ **â€œä¸€ä¸ªä¸“å®¶å›¢é˜Ÿåœ¨å¼€ä¼šâ€**ã€‚è¿™æ˜¯ Transformer å¼ºå¤§çš„æ ¹æœ¬åŸå› ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db083a84",
   "metadata": {},
   "source": [
    "## 1 æ ¸å¿ƒé€»è¾‘ï¼šä»å•å¤´åˆ°å¤šå¤´\n",
    "\n",
    "### 1.1 ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ\n",
    "\n",
    "åœ¨é¢è¯•ä¸­å¯èƒ½ä¼šè¢«é—®åˆ°ï¼šâ€œä¸ºä»€ä¹ˆä¸æŠŠç»´åº¦åšå¤§ä¸€ç‚¹ï¼Œéè¦ææˆå¤šå¤´ï¼Ÿâ€\n",
    "- **å•å¤´ (Single Head)**: å°±åƒä½ çœ‹ä¸€å¼ ç…§ç‰‡ï¼Œåªèƒ½èšç„¦åœ¨ä¸€ä¸ªç‚¹ä¸Šã€‚\n",
    "- **å¤šå¤´ (Multi-Head)**: å°±åƒä½ æˆ´äº† AR çœ¼é•œï¼ŒåŒæ—¶å¼€å¯äº† 8 ä¸ªçª—å£ï¼š\n",
    "    - **Head 1**: å…³æ³¨**è¯­æ³•ç»“æ„**ï¼ˆä¸»è¯­æ‰¾è°“è¯­ï¼‰ã€‚\n",
    "    - **Head 2**: å…³æ³¨**æŒ‡ä»£å…³ç³»**ï¼ˆâ€œå®ƒâ€æ˜¯æŒ‡å‰é¢çš„å“ªä¸ªäººï¼‰ã€‚\n",
    "    - **Head 3**: å…³æ³¨**æ—¶åºä¿¡æ¯**ï¼ˆè¿‡å»è¿˜æ˜¯æœªæ¥ï¼‰ã€‚\n",
    "    - ....\n",
    "    -  **ç»“è®º**: å®ƒä»¬å°†è¾“å…¥ç‰¹å¾åˆ‡åˆ†æˆå¤šä¸ªå­ç©ºé—´ (Subspaces)ï¼Œè®©æ¨¡å‹å¹¶è¡Œåœ°æ•æ‰ä¸åŒç»´åº¦çš„ç‰¹å¾ã€‚\n",
    "\n",
    "### 1.2 æ•°æ®æµå›¾ (The Pipeline)\n",
    "\n",
    "1.  **è¾“å…¥**: $X$ `[Batch, Seq, D_model]`\n",
    "2.  **åˆ†å‘ (Linear Projections)**: é€šè¿‡ $W_q, W_k, W_v$ å˜æ¢ï¼Œå¹¶åˆ‡åˆ†æˆ `n_heads` ä¸ªéƒ¨åˆ†ã€‚\n",
    "3.  **å¹¶è¡Œè®¡ç®—**: 8 ä¸ªå¤´åŒæ—¶åš Self-Attentionï¼ˆå¤ç”¨æ˜¨å¤©çš„å‡½æ•°ï¼‰ã€‚\n",
    "4.  **æ‹¼æ¥ (Concat)**: æŠŠ 8 ä¸ªå¤´çš„ç»“æœæ‹¼å›å»ã€‚\n",
    "5.  **èåˆ (Final Linear)**: é€šè¿‡ $W_o$ æŠŠæ‹¼æ¥åçš„ç»“æœæ··åˆä¸€æ¬¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a3071",
   "metadata": {},
   "source": [
    "## 2 ä»£ç å®ç°\n",
    "\n",
    "è¿™ä¸€æ®µä»£ç æ˜¯ Transformer æ¨¡å‹çš„ **éª¨æ¶**ã€‚è¯·ä»”ç»†æ•²å…¥ï¼Œé‡ç‚¹å…³æ³¨ `view` (é‡å¡‘å½¢çŠ¶) å’Œ `permute` (äº¤æ¢ç»´åº¦) çš„æ“ä½œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58433497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- è®¾ç½®è·¯å¾„ï¼Œç¡®ä¿èƒ½æ‰«æåˆ° src ç›®å½• ---\n",
    "# è·å–å½“å‰ notebook æ‰€åœ¨çš„æ–‡ä»¶å¤¹ (week5)\n",
    "current_dir = os.getcwd()\n",
    "# è·å–é¡¹ç›®æ ¹ç›®å½• (å‡è®¾ structure æ˜¯ project/stage2/week5ï¼Œéœ€è¦å¾€ä¸Šè·³ä¸¤çº§)\n",
    "# è¿™ä¸€æ­¥æ ¹æ®ä½ çš„å®é™…è·¯å¾„å¯èƒ½éœ€è¦å¾®è°ƒï¼Œé€šå¸¸æŠŠ project_root åŠ å…¥ path æœ€ç¨³å¦¥\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.models.transformer_components import scaled_dot_product_attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Day 3: å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        [Batch, Seq, D_model] -> [Batch, Heads, Seq, D_k]\n",
    "        \"\"\"\n",
    "        x = x.view(batch_size, -1, self.n_heads, self.d_k)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # 1. Linear Projections + Split Heads\n",
    "        Q = self.split_heads(self.w_q(q), batch_size)\n",
    "        K = self.split_heads(self.w_k(k), batch_size)\n",
    "        V = self.split_heads(self.w_v(v), batch_size)\n",
    "        \n",
    "        # 2. Scaled Dot-Product Attention (å¤ç”¨ä¸Šé¢çš„å‡½æ•°)\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 3. Concat Heads\n",
    "        # [Batch, Heads, Seq, D_k] -> [Batch, Seq, Heads, D_k]\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3)\n",
    "        # Flatten: [Batch, Seq, D_model]\n",
    "        attention_output = attention_output.contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 4. Final Linear\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ace8f8",
   "metadata": {},
   "source": [
    "## 3 ä»£ç ç»†èŠ‚è§£æ (Code Walkthrough)\n",
    "\n",
    "è¿™é‡Œæœ‰å‡ ä¸ª PyTorch çš„**æš—å‘**ï¼Œå¿…é¡»ææ¸…æ¥šï¼š\n",
    "\n",
    "### 3.1 `view` vs `reshape`\n",
    "\n",
    "  * ä»£ç é‡Œç”¨äº† `view`ã€‚`view` åªèƒ½å¤„ç†**å†…å­˜è¿ç»­ (Contiguous)** çš„å¼ é‡ã€‚\n",
    "  * å¦‚æœæˆ‘ä»¬åšè¿‡ `permute` æˆ– `transpose`ï¼Œå¼ é‡åœ¨å†…å­˜é‡Œå°±ä¹±äº†ï¼ˆä¸å†è¿ç»­ï¼‰ã€‚\n",
    "  * æ‰€ä»¥åœ¨ Step 3.2ï¼Œæˆ‘ä»¬å¿…é¡»å…ˆè°ƒç”¨ **`.contiguous()`** æŠŠå†…å­˜ç†é¡ºï¼Œç„¶åå† `view`ï¼Œå¦åˆ™å¿…æŠ¥é”™ã€‚\n",
    "\n",
    "#### 3.2  ä¸ºä»€ä¹ˆè¦æœ€ååŠ ä¸€ä¸ª `w_o` (Output Linear)ï¼Ÿ\n",
    "\n",
    "  * **ç‰©ç†ç›´è§‰**: 8 ä¸ªä¸“å®¶å¼€å®Œä¼šï¼Œæ¯ä¸ªäººéƒ½ç»™å‡ºäº†è‡ªå·±çš„ç»“è®ºï¼ˆConcat åçš„ç»“æœï¼‰ã€‚\n",
    "  * **W\\_o çš„ä½œç”¨**: è¿™æ˜¯ä¸€ä¸ª **â€œä¼šè®®è®°å½•å‘˜â€** æˆ– **â€œå†³ç­–è€…â€**ã€‚å®ƒè´Ÿè´£æŠŠè¿™ 8 ä¸ªä¸“å®¶çš„æ„è§**åŠ æƒèåˆ**ï¼Œç”Ÿæˆæœ€ç»ˆçš„æ€»ç»“æŠ¥å‘Šã€‚\n",
    "  * **å…·èº«è§†è§’**: å‡å¦‚ Head 1 çœ‹åˆ°äº†çº¢è‰²ï¼ŒHead 2 çœ‹åˆ°äº†åœ†å½¢ã€‚æ‹¼æ¥åæ˜¯ `[çº¢, åœ†]`ã€‚`W_o` è´Ÿè´£æŠŠå®ƒä»¬èåˆæˆæ¦‚å¿µ `[è‹¹æœ]`ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece4274",
   "metadata": {},
   "source": [
    "## 4 éªŒè¯ç¯èŠ‚ (Verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc91dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  torch.Size([4, 10, 512])\n",
      "Output shape: torch.Size([4, 10, 512])\n",
      "Weights shape: torch.Size([4, 8, 10, 10])\n",
      "âœ… æµ‹è¯•é€šè¿‡ï¼ç»´åº¦å˜æ¢å®Œç¾ã€‚\n"
     ]
    }
   ],
   "source": [
    "# --- æµ‹è¯•ä»£ç  ---\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 4\n",
    "\n",
    "# 1. å®ä¾‹åŒ–\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "# 2. åˆ›å»ºå‡æ•°æ®\n",
    "# åœ¨ Self-Attention ä¸­ï¼ŒQ=K=V=X\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 3. å‰å‘ä¼ æ’­\n",
    "output, weights = mha(x, x, x)\n",
    "\n",
    "print(\"Input shape: \", x.shape)\n",
    "print(\"Output shape:\", output.shape) # å¿…é¡»ä¹Ÿæ˜¯ [4, 10, 512]\n",
    "print(\"Weights shape:\", weights.shape) # [4, 8, 10, 10] (Batch, Heads, Seq, Seq)\n",
    "\n",
    "# éªŒè¯è¾“å…¥è¾“å‡ºç»´åº¦æ˜¯å¦ä¸€è‡´\n",
    "assert output.shape == x.shape\n",
    "print(\"âœ… æµ‹è¯•é€šè¿‡ï¼ç»´åº¦å˜æ¢å®Œç¾ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55d6b56",
   "metadata": {},
   "source": [
    "## 5\\. é¢è¯•å¿…é—® & å…·èº«å±•æœ›\n",
    "\n",
    "### ğŸ¤ é¢è¯•é«˜é¢‘é¢˜\n",
    "\n",
    "**Q: Multi-Head Attention çš„è®¡ç®—å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿå’Œ Single-Head ä¸€æ ·å—ï¼Ÿ**\n",
    "\n",
    "  * **A**: **å‡ ä¹ä¸€æ ·ã€‚**\n",
    "      * è™½ç„¶å˜æˆäº† 8 ä¸ªå¤´ï¼Œä½†æ¯ä¸ªå¤´çš„ç»´åº¦é™åˆ°äº† $1/8$ ($64$ vs $512$)ã€‚\n",
    "      * æ€»çš„è®¡ç®—é‡ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰ä¸»è¦å–å†³äºæ€»ç»´åº¦ $d_{model}$ã€‚\n",
    "      * $W_q, W_k, W_v$ çš„å‚æ•°é‡å’ŒæŠŠå®ƒä»¬åˆ‡å¼€ç®—æ˜¯ä¸€æ ·çš„ã€‚\n",
    "      * *å¾®å°çš„åŒºåˆ«åœ¨äºå¹¶è¡Œè®¡ç®—çš„å¼€é”€ï¼Œä½†åœ¨ç†è®º FLOPS ä¸Šæ˜¯ä¸€è‡´çš„ã€‚*\n",
    "\n",
    "### ğŸ¤– å…·èº«æ™ºèƒ½ (Embodied AI) è¿æ¥\n",
    "\n",
    "æƒ³è±¡ä½ æ­£åœ¨åšä¸€ä¸ª **å¤šæ¨¡æ€æœºå™¨äºº (Robot Transformer)**ï¼š\n",
    "\n",
    "  * **Head 1-4**: è´Ÿè´£å¤„ç† **è§†è§‰ (Vision)** â€”â€” çœ‹ç€å‰é¢çš„è·¯ã€‚\n",
    "  * **Head 5-8**: è´Ÿè´£å¤„ç† **æœ¬ä½“æ„ŸçŸ¥ (Proprioception)** â€”â€” æ„Ÿå—æ‰‹è‡‚çš„å…³èŠ‚è§’åº¦ã€‚\n",
    "  * **Concat**: åœ¨ Transformer å†…éƒ¨ï¼Œè§†è§‰ä¿¡æ¯å’Œå…³èŠ‚ä¿¡æ¯è¢«â€œæ‹¼æ¥â€åœ¨ä¸€èµ·ã€‚\n",
    "  * **Attention**: æœºå™¨äººå­¦ä¼šäº†ï¼šâ€œå½“æˆ‘çœ‹åˆ°é‚£ä¸ªæ¯å­ (Vision) æ—¶ï¼Œæˆ‘çš„æ‰‹è‡‚åº”è¯¥å¤„äºæŠ“å–çŠ¶æ€ (Proprioception)ã€‚â€\n",
    "\n",
    "è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ Transformer èƒ½é€šåƒ CV å’Œ NLPï¼Œä¹Ÿèƒ½æ§åˆ¶æœºå™¨äººçš„åŸå› â€”â€”**Multi-Head æœºåˆ¶å¤©ç”Ÿé€‚åˆèåˆå¤šæºä¿¡æ¯ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756e907",
   "metadata": {},
   "source": [
    "### ğŸš€ ä»Šæ—¥ä»»åŠ¡\n",
    "\n",
    "1.  **ä»£ç å®ç°**: äº²æ‰‹å†™ä¸€é `MultiHeadAttention` ç±»ï¼Œç‰¹åˆ«æ˜¯ `split_heads` é‡Œçš„ç»´åº¦å˜æ¢ã€‚\n",
    "2.  **Debug**: è¯•ç€å»æ‰ `.contiguous()`ï¼Œçœ‹çœ‹æŠ¥é”™é•¿ä»€ä¹ˆæ ·ï¼ˆå¢åŠ  Debug ç»éªŒï¼‰ã€‚\n",
    "3.  **ç†è§£**: ç›¯ç€æµ‹è¯•ä»£ç çš„è¾“å‡ºå½¢çŠ¶ï¼Œç¡®è®¤å®ƒå’Œè¾“å…¥ä¸€æ¨¡ä¸€æ ·ã€‚è¿™å°±æ˜¯ Transformer Block çš„\\*\\*â€œç§¯æœ¨ç‰¹æ€§â€\\*\\*â€”â€”è¾“å…¥è¾“å‡ºåŒå‹ï¼Œæ‰€ä»¥å¯ä»¥æ— é™å †å ï¼\n",
    "\n",
    "å†™å®Œåï¼Œè¯·å‘Šè¯‰æˆ‘â€œ**MHA æå®š**â€ï¼Œæ˜å¤©æˆ‘ä»¬è¦æŠŠè¿™äº›ç§¯æœ¨æ­æˆé«˜æ¥¼ï¼š**Encoder Layer å’Œ Residual Connection**ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
