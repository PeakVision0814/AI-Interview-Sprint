{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edeffa4a",
   "metadata": {},
   "source": [
    "# S2W5D5: ç¼–ç å™¨æ•´ä½“ - æ™ºæ…§çš„æ‘©å¤©å¤§æ¥¼\n",
    "\n",
    "## 1 ç†è®ºï¼šä»€ä¹ˆæ˜¯ Context Vectorï¼Ÿ\n",
    "\n",
    "åœ¨å†™ä»£ç ä¹‹å‰ï¼Œå¿…é¡»å…ˆç†è§£ï¼šæˆ‘ä»¬è´¹è¿™ä¹ˆå¤§åŠ²ï¼Œè¾“å…¥ä¸€å †æ•°å­—ï¼Œæœ€åè¾“å‡ºçš„è¿™ä¸€å †æ•°å­—åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "- **è¾“å…¥**ï¼šä¸€ä¸²å­¤ç«‹çš„IDï¼š\n",
    "    - ä¾‹å¦‚ï¼š`[â€œè‹¹æœâ€, â€œæ‰â€, â€œåœ¨â€, â€œåœ°ä¸Šâ€]`ã€‚\n",
    "    - è¿™æ—¶å€™çš„â€œè‹¹æœâ€åªæ˜¯å­—å…¸é‡Œçš„ä¸€ä¸ªè¯ï¼Œè·Ÿâ€œç‰›é¡¿â€æˆ–è€…â€œæ‰‹æœºâ€æ²¡å…³ç³»ã€‚\n",
    "- **ç»è¿‡Nå±‚Enocderä¹‹å**ï¼š\n",
    "    - æ¯ä¸€å±‚çš„Attentionéƒ½åœ¨è®©è¯ä¸è¯å‘ç”Ÿâ€œåŒ–å­¦ååº”â€ã€‚\n",
    "    - ç¬¬1å±‚ï¼šçŸ¥é“â€œè‹¹æœâ€å’Œâ€œæ‰â€æœ‰å…³ç³»ã€‚\n",
    "    - ç¬¬6å±‚ï¼šçŸ¥é“è¿™é‡Œçš„â€œè‹¹æœâ€æ˜¯æŒ‡æ°´æœï¼ˆå› ä¸ºå®ƒæ‰åœ¨åœ°ä¸Šï¼‰ï¼Œè€Œä¸æ˜¯æ‰‹æœºã€‚\n",
    "- **è¾“å‡ºï¼ˆContext Vectorsï¼‰**ï¼šä¸€ä¸²**èåˆäº†ä¸Šä¸‹æ–‡**çš„å‘é‡ã€‚\n",
    "    - æ­¤æ—¶çš„$Vector_{\\text{è‹¹æœ}}$å·²ç»ä¸å†æ˜¯å•çº¯çš„â€œè‹¹æœâ€äº†ï¼Œå®ƒåŒ…å«äº†â€œæ‰è½â€ã€â€œç‰©ç†å®ä½“â€ç­‰ä¸°å¯Œçš„ä¿¡æ¯ã€‚\n",
    "    - **å…·èº«è§†è§’**: æœºå™¨äººæ‹¿åˆ°è¿™ä¸ª Vectorï¼Œå°±çŸ¥é“è¯¥å»**æŠ“å–**ï¼ˆå¯¹å¾…ç‰©ä½“ï¼‰ï¼Œè€Œä¸æ˜¯å»**å……ç”µ**ï¼ˆå¯¹å¾…ç”µå­äº§å“ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539118be",
   "metadata": {},
   "source": [
    "## 2 ä»£ç å®ç°ï¼šå †å è‰ºæœ¯\n",
    "\n",
    "æˆ‘ä»¬å°†å®ç° `TransformerEncoder` ç±»ã€‚\n",
    "\n",
    "**æ ¸å¿ƒç»„ä»¶**:\n",
    "\n",
    "1. **Embeddings**: æŠŠ ID å˜æˆå‘é‡ã€‚\n",
    "2. **Positional Encoding**: åŠ ä¸Šä½ç½®ä¿¡æ¯ã€‚\n",
    "3. **Layers Stack**: ä½¿ç”¨ `nn.ModuleList` å †å  $N$ ä¸ª `EncoderLayer`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006bd45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.models.transformer_components import PositionalEncoding, EncoderLayer \n",
    "\n",
    "def get_clones(module, N):\n",
    "    \"\"\"\n",
    "    ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼šå…‹éš† N ä¸ªç›¸åŒçš„å±‚\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, N=6, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: è¯è¡¨å¤§å° (æ¯”å¦‚ 30000)\n",
    "            d_model: å‘é‡ç»´åº¦ (512)\n",
    "            n_heads: å¤šå¤´æ•° (8)\n",
    "            d_ff: FFNéšè—å±‚ç»´åº¦ (2048)\n",
    "            N: å †å å±‚æ•° (é€šå¸¸æ˜¯ 6 æˆ– 12)\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.N = N\n",
    "        \n",
    "        # 1. è¯åµŒå…¥å±‚ (Word Embedding)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 2. ä½ç½®ç¼–ç  (Positional Encoding)\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "        \n",
    "        # 3. å †å  N å±‚ EncoderLayer\n",
    "        # å…ˆå®ä¾‹åŒ–ä¸€ä¸ªæ ‡å‡†å±‚\n",
    "        base_layer = EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "        # ç„¶åå…‹éš† N ä»½\n",
    "        self.layers = get_clones(base_layer, N)\n",
    "        \n",
    "        # 4. æœ€ç»ˆçš„è§„èŒƒåŒ–å±‚ (å¯é€‰ï¼Œä½†æ¨è)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        \"\"\"\n",
    "        src: [Batch, Seq_Len] (æ³¨æ„ï¼šè¿™é‡Œè¾“å…¥çš„æ˜¯æ•´æ•°ç´¢å¼•ï¼Œä¸æ˜¯å‘é‡äº†)\n",
    "        mask: [Batch, 1, 1, Seq_Len]\n",
    "        \"\"\"\n",
    "        # 1. Embedding\n",
    "        # [Batch, Seq] -> [Batch, Seq, Dim]\n",
    "        x = self.embedding(src)\n",
    "        \n",
    "        # 2. Scale Embedding (è¿™æ˜¯ä¸€ä¸ª trick)\n",
    "        # è®ºæ–‡ä¸­æåˆ° embedding éœ€è¦ä¹˜ä»¥ sqrt(d_model)ï¼Œè®©æ•°å€¼å˜å¤§ä¸€ç‚¹ï¼Œ\n",
    "        # ä»¥ä¾¿å’Œ Positional Encoding (åœ¨ -1 åˆ° 1 ä¹‹é—´) ç›¸åŠ æ—¶ï¼Œ\n",
    "        # åŸå§‹è¯­ä¹‰ä¿¡æ¯ä¸ä¼šè¢«ä½ç½®ä¿¡æ¯æ·¹æ²¡ã€‚\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        \n",
    "        # 3. Add Position Encoding\n",
    "        x = self.pe(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 4. Pass through N layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        # 5. Final Norm\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136c1983",
   "metadata": {},
   "source": [
    "## 3 è”è°ƒæµ‹è¯•\n",
    "\n",
    "è¦æŠŠæ•´æ•°è¾“å…¥è¿›å»ï¼Œçœ‹çœ‹èƒ½ä¸èƒ½åå‡ºå®Œç¾çš„ç‰¹å¾å‘é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b765f5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (Indices): torch.Size([32, 10])\n",
      "Output shape (Vectors): torch.Size([32, 10, 512])\n",
      "âœ… Transformer Encoder æ„å»ºæˆåŠŸï¼\n",
      "æ¨¡å‹æ€»å‚æ•°é‡: 19.43 Million (M)\n"
     ]
    }
   ],
   "source": [
    "# 1. é…ç½®å‚æ•° (å‚è€ƒ BERT-Base)\n",
    "vocab_size = 1000 # å‡è®¾è¯è¡¨åªæœ‰ 1000 ä¸ªè¯\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "N = 6           # å †å  6 å±‚\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "\n",
    "# 2. å®ä¾‹åŒ–æ¨¡å‹\n",
    "model = TransformerEncoder(vocab_size, d_model, n_heads, d_ff, N)\n",
    "\n",
    "# 3. æ„é€ è™šæ‹Ÿè¾“å…¥\n",
    "# æ³¨æ„ï¼šè¾“å…¥å¿…é¡»æ˜¯ Long (æ•´æ•°) ç±»å‹ï¼ŒèŒƒå›´åœ¨ [0, vocab_size) ä¹‹é—´\n",
    "src = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# 4. å‰å‘ä¼ æ’­\n",
    "output = model(src)\n",
    "\n",
    "# 5. éªŒè¯\n",
    "print(\"Input shape (Indices):\", src.shape) # [32, 10]\n",
    "print(\"Output shape (Vectors):\", output.shape) # [32, 10, 512]\n",
    "\n",
    "assert output.shape == (batch_size, seq_len, d_model)\n",
    "print(\"âœ… Transformer Encoder æ„å»ºæˆåŠŸï¼\")\n",
    "\n",
    "# çœ‹çœ‹è¿™æ ‹æ¥¼æœ‰å¤šé‡ (å‚æ•°é‡)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"æ¨¡å‹æ€»å‚æ•°é‡: {total_params / 1e6:.2f} Million (M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e09c9a",
   "metadata": {},
   "source": [
    "## 4 æ·±åº¦è§£æ & é¢è¯•å…«è‚¡\n",
    "\n",
    "### Q1: ä¸ºä»€ä¹ˆç”¨ `nn.ModuleList` è€Œä¸æ˜¯æ™®é€šçš„ Python `list`ï¼Ÿ(Python é¢è¯•é«˜é¢‘)\n",
    "\n",
    "  * **é”™è¯¯å†™æ³•**: `self.layers = [EncoderLayer(...) for _ in range(N)]`\n",
    "  * **åæœ**: å¦‚æœä½ ç”¨æ™®é€š listï¼ŒPyTorch çš„ `model.parameters()` **æ‰«æä¸åˆ°**è¿™äº›å±‚é‡Œçš„æƒé‡ã€‚è¿™æ„å‘³ç€ä½ è®­ç»ƒæ—¶ï¼Œè¿™äº›å±‚çš„å‚æ•°**ä¸ä¼šæ›´æ–°**ï¼\n",
    "  * **æ­£ç¡®å†™æ³•**: `nn.ModuleList` æ˜¯ä¸“é—¨ä¸º PyTorch è®¾è®¡çš„åˆ—è¡¨ï¼Œå®ƒä¼šè‡ªåŠ¨æŠŠé‡Œé¢çš„ Layer æ³¨å†Œåˆ°æ¨¡å‹é‡Œï¼Œå‘Šè¯‰ä¼˜åŒ–å™¨ï¼šâ€œè¿™ä¹Ÿæ˜¯æˆ‘è¦è®­ç»ƒçš„å­©å­ã€‚â€\n",
    "\n",
    "### Q2: Embedding ä¸ºä»€ä¹ˆè¦ä¹˜ä»¥ $\\sqrt{d_{model}}$ï¼Ÿ\n",
    "\n",
    "  * **ç›´è§‰**:\n",
    "      * Positional Encoding çš„å€¼åœ¨ $[-1, 1]$ ä¹‹é—´ã€‚\n",
    "      * Embedding åˆå§‹åŒ–åçš„å€¼å¾€å¾€å¾ˆå°ï¼ˆæ–¹å·®ä¸º 1ï¼‰ã€‚\n",
    "      * å¦‚æœç›´æ¥ç›¸åŠ ï¼Œ**ä½ç½®ä¿¡æ¯**å¯èƒ½ä¼šå¤ªå¼ºï¼Œå¹²æ‰°äº†**è¯­ä¹‰ä¿¡æ¯**ã€‚\n",
    "      * ä¹˜ä»¥ $\\sqrt{512} \\approx 22.6$ï¼Œç›¸å½“äºæŠŠè¯­ä¹‰ä¿¡å·æ”¾å¤§ï¼Œè®©å®ƒå æ®ä¸»å¯¼åœ°ä½ï¼Œä½ç½®ä¿¡æ¯ä½œä¸ºè¾…åŠ©ã€‚\n",
    "\n",
    "### Q3: è¿™å°±æ˜¯ BERT å—ï¼Ÿ\n",
    "\n",
    "  * **æ˜¯çš„ï¼**\n",
    "  * **BERT-Base** = 12 å±‚ Encoder, $d_{model}=768$ã€‚\n",
    "  * **BERT-Large** = 24 å±‚ Encoder, $d_{model}=1024$ã€‚\n",
    "  * ä½ åˆšåˆšæ‰‹å†™çš„ä»£ç ï¼Œå°±æ˜¯ NLP é¢†åŸŸè¿‡å» 5 å¹´æœ€å¼ºéœ¸ä¸»çš„æ ¸å¿ƒæ¶æ„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a20665",
   "metadata": {},
   "source": [
    "## ğŸš€ ä»Šæ—¥å¤ç›˜ä¸ä¸‹ä¸€æ­¥\n",
    "\n",
    "æ­å–œä½ ï¼åˆ°ä»Šå¤©ä¸ºæ­¢ï¼Œä½ å·²ç»**äº²æ‰‹é€ å‡ºäº†**ï¼š\n",
    "\n",
    "1.  **ä½ç½®ç¼–ç **: è®©æ¨¡å‹æœ‰æ—¶é—´è§‚å¿µã€‚\n",
    "2.  **å¤šå¤´æ³¨æ„åŠ›**: è®©æ¨¡å‹æœ‰å…¨å±€è§†é‡ã€‚\n",
    "3.  **Encoder Layer**: è®©æ¨¡å‹æœ‰æ¨ç†èƒ½åŠ›ã€‚\n",
    "4.  **Encoder Stack**: è®©æ¨¡å‹æœ‰æ·±åº¦æ™ºæ…§ã€‚\n",
    "\n",
    "**ä½ ç°åœ¨çš„å·¥ç¨‹æ–‡ä»¶å¤¹é‡Œï¼Œå·²ç»èººç€ä¸€ä¸ªå¾®å‹çš„ BERT æ¨¡å‹äº†ã€‚**\n",
    "\n",
    "### ğŸ“… æ˜å¤©çš„è®¡åˆ’ (S2W5D6)\n",
    "\n",
    "æ˜å¤©æˆ‘ä»¬ç¨å¾®è½»æ¾ä¸€ç‚¹ï¼Œè¿›è¡Œ **[æ”¯çº¿ä»»åŠ¡] ViT (Vision Transformer)** çš„å­¦ä¹ ã€‚\n",
    "ä½ ä¼šå‘ç°ï¼š**æŠŠä½ ä»Šå¤©å†™çš„ä»£ç ï¼Œè¾“å…¥æ¥å£ç¨å¾®æ”¹ä¸€ä¸‹ï¼ˆæŠŠå›¾ç‰‡åˆ‡ç‰‡ï¼‰ï¼Œå®ƒç«‹åˆ»å°±å˜æˆäº†ç‰¹æ–¯æ‹‰æœºå™¨äººçš„è§†è§‰å¤§è„‘ã€‚**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
