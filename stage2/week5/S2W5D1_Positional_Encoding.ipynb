{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d066e55e",
   "metadata": {},
   "source": [
    "# S2W5D1: è¾“å…¥å±‚ - èµ‹äºˆæ¨¡å‹â€œè¯­ä¹‰â€ä¸â€œä½ç½®â€æ„Ÿ\n",
    "\n",
    "åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œè®¡ç®—æœºåªèƒ½å¤„ç†æ•°å­—ã€‚Transformerçš„ç¬¬ä¸€æ­¥ï¼Œå°±æ˜¯è¦æŠŠäººç±»ç†è§£çš„â€œ**å­—ï¼ˆTokenï¼‰**â€è½¬æ¢æˆè®¡ç®—æœºç†è§£çš„â€œ**å‘é‡ï¼ˆVectorï¼‰**â€ï¼Œå¹¶ä¸”å‘Šè¯‰å®ƒâ€œ**é¡ºåºï¼ˆOrderï¼‰**â€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481fc9a",
   "metadata": {},
   "source": [
    "## 1 è¯åµŒå…¥ï¼ˆWord Embeddingï¼‰ï¼šè¯­ä¹‰çš„å®¹å™¨\n",
    "\n",
    "**Embedding å±‚**çš„åŸç†ï¼š\n",
    " - **åŸç†**ï¼šä¸€ä¸ªå·¨å¤§çš„æŸ¥æ‰¾è¡¨ï¼ˆLookup Tableï¼‰ã€‚æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè¯ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å¾ç»´åº¦ã€‚\n",
    " - **ä»£ç å¯¹åº”**ï¼š`nn.Embedding(vocab_size, d_model)`\n",
    "    - `vocab_size`ï¼šè¯è¡¨å¤§å°ï¼ˆæ¯”å¦‚30000ä¸ªå¸¸ç”¨è¯ï¼‰ã€‚\n",
    "    - `d_model`ï¼šåµŒå…¥ç»´åº¦ï¼ˆTransformerè®ºæ–‡ä¸­æ˜¯514ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd091389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([2, 3])\n",
      "Output Shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# å‡è®¾è¯è¡¨åªæœ‰ 10 ä¸ªè¯ï¼Œæ¯ä¸ªè¯ç”¨ 4 ç»´å‘é‡è¡¨ç¤º\n",
    "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
    "\n",
    "# è¾“å…¥ä¸€ä¸ªbatchï¼ŒåŒ…å«2ä¸ªå¥å­ï¼Œæ¯ä¸ªå¥å­3ä¸ªè¯ï¼ˆç´¢å¼•ï¼‰\n",
    "input_ids = torch.tensor([[1, 2, 3], [4, 5, 6]]) #  Shape: [2, 3]\n",
    "\n",
    "# æŸ¥è¡¨\n",
    "output = embedding_layer(input_ids)\n",
    "print(\"Input Shape:\", input_ids.shape) # [Batch=2, Seq_Len=3]\n",
    "print(\"Output Shape:\", output.shape)   # [Batch=2, Seq_Len=3, Dim=4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b7a6d5",
   "metadata": {},
   "source": [
    "## 2 ä½ç½®ç¼–ç  (Positional Encoding): æ³¨å…¥çµé­‚\n",
    "\n",
    "è¿™æ˜¯Transformeræœ€å¤©æ‰çš„è®¾è®¡ä¹‹ä¸€ã€‚\n",
    "\n",
    "### 2.1 ä½œç”¨\n",
    "\n",
    "RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰æ˜¯æŒ‰é¡ºåºè¯»å¥å­çš„ï¼ˆå…ˆè¯»â€œæˆ‘â€ï¼Œå†è¯»â€œçˆ±â€ï¼Œåœ¨è¯»â€œä½ â€ï¼‰ï¼Œæ‰€ä»¥å®ƒå¤©ç„¶çŸ¥é“ä½ç½®ä¿¡æ¯ã€‚ä½†æ˜¯Transformerçš„Attentionæœºåˆ¶æ˜¯å¹¶è¡Œçš„ï¼Œå®ƒä¸€çœ¼çœ‹å®Œæ•´ä¸ªå¥å­ã€‚å¦‚æœæ²¡æœ‰ä»»ä½•ä½ç½®ä¿¡æ¯ï¼Œâ€œæˆ‘çˆ±ä½ â€å’Œâ€œä½ çˆ±æˆ‘â€åœ¨Attentionçœ‹æ¥æ˜¯å®Œå…¨ä¸€æ ·çš„ï¼ˆéƒ½æ˜¯è¿™ä¸‰ä¸ªå­—çš„ç»„åˆï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦äººä¸ºçš„æŠŠâ€œ**ä½ç½®ä¿¡æ¯**â€æ·»åŠ åˆ°Embeddingé‡Œé¢å»ã€‚\n",
    "\n",
    "### 2.2 å¦‚ä½•æ·»åŠ \n",
    "\n",
    "TransformeråŸè®ºæ–‡ä½¿ç”¨äº†ä¸€ç»„**æ­£å¼¦Sin**å’Œ**ä½™å¼¦Cos**å‡½æ•°æ¥ç”Ÿæˆä½ç½®å‘é‡ã€‚å¯¹äºå¥å­ä¸­ç¬¬$pos$ä¸ªä½ç½®çš„è¯ï¼Œå®ƒçš„ç¬¬$2i$å’Œ$2i+1$ä¸ªç»´åº¦çš„ç¼–ç å…¬å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
    "\n",
    "    - $pos$: è¯åœ¨å¥å­ä¸­çš„ä½ç½® (0, 1, 2, ...)ã€‚\n",
    "    - $i$: å‘é‡ç»´åº¦çš„ç´¢å¼•ã€‚\n",
    "    - $d_{model}$: æ¨¡å‹çš„æ€»ç»´åº¦ (512)ã€‚\n",
    "\n",
    "**ç›´è§‚ç†è§£**ï¼š\n",
    "æƒ³è±¡ä¸€ä¸‹æ—¶é’Ÿã€‚\n",
    "\n",
    "  * ç§’é’ˆè½¬å¾—å¿« (é«˜é¢‘)ï¼šä»£è¡¨ç»´åº¦çš„ä½ä½ï¼Œå˜åŒ–å‰§çƒˆã€‚\n",
    "  * åˆ†é’ˆè½¬å¾—æ…¢ (ä½é¢‘)ï¼šä»£è¡¨ç»´åº¦çš„é«˜ä½ï¼Œå˜åŒ–ç¼“æ…¢ã€‚\n",
    "  * é€šè¿‡ä¸åŒé¢‘ç‡çš„ Sin/Cos ç»„åˆï¼Œæ¯ä¸ªä½ç½® $pos$ éƒ½èƒ½è·å¾—ä¸€ä¸ª**ç‹¬ä¸€æ— äºŒ**çš„çº¹ç†ç‰¹å¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d29416",
   "metadata": {},
   "source": [
    "## 3 æ ¸å¿ƒä»£ç å®ç°ï¼ˆHand-Written Implementationï¼‰\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä½ç½®ç¼–ç çš„æ ¸å¿ƒä»£ç å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cae02b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m input_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, max_len, d_model) \u001b[38;5;66;03m# Batch=32\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# å®ä¾‹åŒ–\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m pe_layer \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# å‰å‘ä¼ æ’­\u001b[39;00m\n\u001b[1;32m     58\u001b[0m output \u001b[38;5;241m=\u001b[39m pe_layer(input_embedding)\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mPositionalEncoding.__init__\u001b[0;34m(self, d_model, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m        d_model: æ¨¡å‹çš„ç»´åº¦ (é€šå¸¸æ˜¯ 512)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m        max_len: é¢„è®¾çš„æœ€å¤§åºåˆ—é•¿åº¦ (é˜²æ­¢è¾“å…¥å¥å­å¤ªé•¿)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPositionalEncoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# 1. åˆ›å»ºä¸€ä¸ªçŸ©é˜µ peï¼Œå¤§å°æ˜¯ [max_len, d_model]ï¼Œåˆå§‹åŒ–ä¸º 0\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     pe \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(max_len, d_model)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: æ¨¡å‹çš„ç»´åº¦ (é€šå¸¸æ˜¯ 512)\n",
    "            max_len: é¢„è®¾çš„æœ€å¤§åºåˆ—é•¿åº¦ (é˜²æ­¢è¾“å…¥å¥å­å¤ªé•¿)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # 1. åˆ›å»ºä¸€ä¸ªçŸ©é˜µ peï¼Œå¤§å°æ˜¯ [max_len, d_model]ï¼Œåˆå§‹åŒ–ä¸º 0\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # 2. ç”Ÿæˆä½ç½®ç´¢å¼•pos: [0, 1, 2, ..., max_len - 1] -> Shape: [max_len, 1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # 3. è®¡ç®—åˆ†æ¯ä¸­çš„ div_term: 10000^(2i/d_model)\n",
    "        # è¿™é‡Œçš„æ•°å­¦å˜æ¢åˆ©ç”¨äº† log: exp(log(x)) = x\n",
    "        # è¿™æ˜¯ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œé˜²æ­¢åˆ†æ¯æº¢å‡º\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # 4. å¡«å……çŸ©é˜µ\n",
    "        # å¶æ•°ç»´åº¦ç”¨ sin\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # å¥‡æ•°ç»´åº¦ç”¨ cos\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # 5. å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œä½¿å…¶é€‚é… Batch: [max_len, d_model] -> [1, max_len, d_model]\n",
    "        # è¿™æ ·åœ¨åŠ æ³•æ—¶ï¼Œå¯ä»¥åˆ©ç”¨å¹¿æ’­æœºåˆ¶ (Broadcasting) åº”ç”¨åˆ°æ‰€æœ‰ Batch ä¸Š\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # 6. æ³¨å†Œä¸º buffer\n",
    "        # è¿™æ˜¯ä¸€ä¸ªå…³é”®ç‚¹ï¼\n",
    "        # register_buffer å‘Šè¯‰ PyTorch: \"è¿™ä¸ªå˜é‡ä¸æ˜¯æ¨¡å‹å‚æ•°(ä¸éœ€è¦æ¢¯åº¦æ›´æ–°)ï¼Œ\n",
    "        # ä½†æ˜¯å®ƒæ˜¯æ¨¡å‹çŠ¶æ€çš„ä¸€éƒ¨åˆ†ï¼Œä¿å­˜æ¨¡å‹(save_state_dict)æ—¶è¦æŠŠå®ƒå¸¦ä¸Šã€‚\"\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # x çš„é•¿åº¦å¯èƒ½å°äº max_lenï¼Œæ‰€ä»¥æˆ‘ä»¬è¦åˆ‡ç‰‡\n",
    "        # x + pe[:x.size(1)] åˆ©ç”¨äº†å¹¿æ’­æœºåˆ¶\n",
    "        # [batch, seq, d] + [1, seq, d] -> [batch, seq, d]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "    \n",
    "# --- æµ‹è¯•ä»£ç  ---\n",
    "# æ¨¡æ‹Ÿå‚æ•°\n",
    "d_model = 512\n",
    "max_len = 100\n",
    "input_embedding = torch.randn(32, max_len, d_model) # Batch=32\n",
    "\n",
    "# å®ä¾‹åŒ–\n",
    "pe_layer = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "output = pe_layer(input_embedding)\n",
    "\n",
    "print(\"Input shape:\", input_embedding.shape)\n",
    "print(\"PE shape (internal):\", pe_layer.pe.shape)\n",
    "print(\"Output shape:\", output.shape) # åº”è¯¥ä¿æŒä¸å˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88984a",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# --- æµ‹è¯•ä»£ç  ---\n",
    "# æ¨¡æ‹Ÿå‚æ•°\n",
    "d_model = 512\n",
    "max_len = 100\n",
    "input_embedding = torch.randn(32, max_len, d_model) # Batch=32\n",
    "\n",
    "# å®ä¾‹åŒ–\n",
    "pe_layer = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "\n",
    "# å‰å‘ä¼ æ’­\n",
    "output = pe_layer(input_embedding)\n",
    "\n",
    "print(\"Input shape:\", input_embedding.shape)\n",
    "print(\"PE shape (internal):\", pe_layer.pe.shape)\n",
    "print(\"Output shape:\", output.shape) # åº”è¯¥ä¿æŒä¸å˜\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## 4\\. å¯è§†åŒ–éªŒè¯ (Debug like a Pro)\n",
    "\n",
    "ä¸ºäº†è®©ä½ â€œçœ‹è§â€ä½ç½®ç¼–ç ï¼Œæˆ‘ä»¬ç”»ä¸ªå›¾ã€‚è¿™æ˜¯ç†è§£â€œé¢‘ç‡å˜åŒ–â€æœ€å¥½çš„æ–¹å¼ã€‚\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "# å–å‡º pe çš„ç¬¬ä¸€ä¸ª batch (å› ä¸ºå®ƒå…¨æ˜¯å¤åˆ¶çš„)ï¼Œæˆªå–å‰ 100 ä¸ªä½ç½®ï¼Œå‰ 50 ä¸ªç»´åº¦\n",
    "pe_matrix = pe_layer.pe[0, :100, :50].numpy()\n",
    "\n",
    "plt.imshow(pe_matrix, cmap='viridis')\n",
    "plt.xlabel('Embedding Dimension (Depth)')\n",
    "plt.ylabel('Token Position (Time Step)')\n",
    "plt.colorbar()\n",
    "plt.title(\"Visualizing Positional Encoding\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**è§‚å¯Ÿ**: ä½ åº”è¯¥èƒ½çœ‹åˆ°å·¦è¾¹ï¼ˆä½ç»´åº¦ï¼‰çº¹ç†æ³¢åŠ¨å¾ˆå¿«ï¼ˆé«˜é¢‘ï¼‰ï¼Œå³è¾¹ï¼ˆé«˜ç»´åº¦ï¼‰é¢œè‰²å˜åŒ–å¾ˆæ…¢ï¼ˆä½é¢‘ï¼‰ã€‚è¿™å°±æ˜¯æ­£å¼¦æ³¢åœ¨èµ·ä½œç”¨ã€‚\n",
    "\n",
    "-----\n",
    "\n",
    "## 5\\. é¢è¯•å…«è‚¡æ–‡ä¸å…·èº«è¿ç§» (Interview & Robotics)\n",
    "\n",
    "### ğŸ“¢ é¢è¯•å®˜ä¼šé—®ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "1.  **Q: ä¸ºä»€ä¹ˆè¦ç›¸åŠ  (`x + pe`) è€Œä¸æ˜¯æ‹¼æ¥ (`concat(x, pe)`)ï¼Ÿ**\n",
    "\n",
    "      * **A**:\n",
    "          * **ç»´åº¦æ•ˆç‡**: æ‹¼æ¥ä¼šå¢åŠ ç»´åº¦ï¼Œå¯¼è‡´åç»­å‚æ•°é‡å¢åŠ ã€‚\n",
    "          * **æ•°å­¦æ€§è´¨**: ç›¸åŠ ä¿ç•™äº† Embedding çš„åˆ†å¸ƒç‰¹å¾ã€‚è€Œä¸”æ•°å­¦ä¸Šå¯ä»¥è¯æ˜ï¼Œç›¸åŠ åŒ…å«çš„ä¿¡æ¯é‡å’Œæ‹¼æ¥æ˜¯ç­‰ä»·çš„ï¼ˆåªæ˜¯æ··åˆåœ¨äº†ä¸€èµ·ï¼Œç½‘ç»œå¯ä»¥å­¦ä¼šæŠŠå®ƒä»¬æ‹†å¼€ï¼‰ã€‚\n",
    "\n",
    "2.  **Q: ä¸ºä»€ä¹ˆè¦ç”¨ Sin/Cos è¿™ç§æ­£å¼¦å‡½æ•°ï¼Ÿä¸ºä»€ä¹ˆä¸ç›´æ¥è®­ç»ƒä¸€ä¸ª Embeddingï¼Ÿ**\n",
    "\n",
    "      * **A (æ ‡å‡†ç­”æ¡ˆ)**:\n",
    "          * **å¤–æ¨æ€§ (Extrapolation)**: å¦‚æœè®­ç»ƒæ—¶æœ€å¤§é•¿åº¦æ˜¯ 100ï¼Œæµ‹è¯•æ—¶é‡åˆ°äº†é•¿åº¦ 120 çš„å¥å­ï¼ŒSin/Cos å‡½æ•°å¯ä»¥ç›´æ¥è®¡ç®—å‡ºç¬¬ 120 ä¸ªä½ç½®çš„ç¼–ç ã€‚è€Œå¦‚æœæ˜¯è®­ç»ƒå‡ºæ¥çš„ Embedding (`nn.Embedding`)ï¼Œç¬¬ 120 ä¸ªä½ç½®å°±æ˜¯æœªçŸ¥çš„ï¼ˆè¶Šç•Œäº†ï¼‰ã€‚\n",
    "          * **ç›¸å¯¹ä½ç½®ä¿¡æ¯**: æ­£å¼¦å‡½æ•°æœ‰ä¸ªæ€§è´¨ï¼š$PE(pos+k)$ å¯ä»¥è¡¨ç¤ºä¸º $PE(pos)$ çš„çº¿æ€§å˜æ¢ã€‚è¿™æ„å‘³ç€æ¨¡å‹æ›´å®¹æ˜“å­¦åˆ°â€œç›¸å¯¹è·ç¦»â€çš„å…³ç³»ï¼ˆæ¯”å¦‚å•è¯ A åœ¨å•è¯ B åé¢ç¬¬ 3 ä¸ªä½ç½®ï¼‰ã€‚\n",
    "\n",
    "3.  **Q: `register_buffer` æ˜¯ä»€ä¹ˆï¼Ÿ**\n",
    "\n",
    "      * **A**: å®ƒæ˜¯ `nn.Module` çš„ä¸€ä¸ªæ–¹æ³•ï¼Œç”¨äºæ³¨å†Œé‚£äº›â€œä¸æ˜¯å‚æ•°ï¼ˆä¸éœ€è¦æ¢¯åº¦ä¸‹é™ï¼‰â€ä½†â€œéœ€è¦éšæ¨¡å‹ä¿å­˜å’ŒåŠ è½½â€çš„å¼ é‡ã€‚PE çŸ©é˜µå°±æ˜¯å…¸å‹çš„ bufferã€‚\n",
    "\n",
    "### ğŸ¤– å…·èº«æ™ºèƒ½è§†è§’ (Embodied AI)\n",
    "\n",
    "ä½œä¸ºæœºå™¨äººé¡¹ç›®è´Ÿè´£äººï¼Œä½ éœ€è¦è¿™æ ·ç†è§£ PEï¼š\n",
    "\n",
    "  * **è½¨è¿¹è§„åˆ’**: åœ¨ Transformer æ§åˆ¶æœºå™¨è‡‚æ—¶ (ä¾‹å¦‚ Action Chunking)ï¼Œä½ è¾“å…¥çš„ä¸æ˜¯å¥å­ï¼Œè€Œæ˜¯è¿‡å» 10 å¸§çš„**å…³èŠ‚çŠ¶æ€**ã€‚å¦‚æœä¸åŠ  PEï¼Œæœºå™¨äººå°±ä¸çŸ¥é“å“ªä¸€å¸§æ˜¯â€œå‰ä¸€ç§’â€ï¼Œå“ªä¸€å¸§æ˜¯â€œç°åœ¨â€ã€‚è¿™é‡Œçš„ PE ä»£è¡¨äº†**æ—¶é—´ (Time)**ã€‚\n",
    "  * **ViT (è§†è§‰)**: å½“æœºå™¨äººçœ‹å›¾æ—¶ï¼Œå›¾ç‰‡è¢«åˆ‡æˆ 16x16 çš„å—ã€‚å¦‚æœä¸åŠ  PEï¼Œæœºå™¨äººå°±ä¸çŸ¥é“å“ªä¸ªå—æ˜¯â€œå·¦ä¸Šè§’â€ï¼Œå“ªä¸ªæ˜¯â€œå³ä¸‹è§’â€ã€‚è¿™é‡Œçš„ PE ä»£è¡¨äº†**ç©ºé—´ (Space)**ã€‚\n",
    "\n",
    "-----\n",
    "\n",
    "## ğŸš€ ä»Šæ—¥ä»»åŠ¡ (Action Items)\n",
    "\n",
    "1.  **ä»£ç å¤ç°**: ä¸è¦å¤åˆ¶ç²˜è´´ï¼Œè¯·æ‰‹åŠ¨æ•²ä¸€é `PositionalEncoding` ç±»ï¼Œå°¤å…¶æ˜¯è®¡ç®— `div_term` çš„é‚£ä¸€è¡Œã€‚\n",
    "2.  **è¿è¡Œå¯è§†åŒ–**: è·‘é€šçƒ­åŠ›å›¾ï¼Œç›¯ç€å®ƒçœ‹ 10 ç§’ï¼Œç†è§£è¿™ç§â€œé¢‘ç‡çº¹ç†â€ã€‚\n",
    "3.  **æ€è€ƒ**: å¦‚æœæˆ‘è¦åš**2D å›¾åƒ**çš„ä½ç½®ç¼–ç ï¼ˆæœ‰ x è½´å’Œ y è½´ï¼‰ï¼Œè¿™å¥—å…¬å¼è¯¥æ€ä¹ˆæ”¹ï¼Ÿï¼ˆæç¤ºï¼šåˆ†åˆ«ä¸º x å’Œ y ç”Ÿæˆç¼–ç ï¼Œç„¶åæ‹¼æ¥æˆ–ç›¸åŠ ï¼‰ã€‚\n",
    "\n",
    "å†™å®Œä»£ç åï¼Œè¯·å‘Šè¯‰æˆ‘â€œ**Day 1 å®Œæˆ**â€ï¼Œå¹¶é™„ä¸Šä½ å¯¹å¯è§†åŒ–å›¾çš„è§‚å¯Ÿï¼Œæˆ–è€…ä»»ä½•ç–‘é—®ã€‚æˆ‘ä»¬å°†è¿›å…¥ Day 2 çš„æ ¸å¿ƒæˆ˜å½¹â€”â€”æ‰‹æ’• Attentionï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
