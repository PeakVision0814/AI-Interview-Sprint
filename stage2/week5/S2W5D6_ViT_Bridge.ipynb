{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a817f52a",
   "metadata": {},
   "source": [
    "# S2W5D6: ViT â€”â€” ç»™ Transformer è£…ä¸Šçœ¼ç›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abcabd",
   "metadata": {},
   "source": [
    "## 1 æ ¸å¿ƒç†è®ºï¼šPatchifyï¼ˆåˆ‡å—åŒ–ï¼‰\n",
    "\n",
    "Transformer çš„è¾“å…¥å¿…é¡»æ˜¯ **åºåˆ— (Sequence)**ï¼Œä¹Ÿå°±æ˜¯ä¸€æ’å‘é‡ `[vector_1, vector_2, ...]`ã€‚ä½†å›¾ç‰‡æ˜¯ä¸€ä¸ª**æ–¹å—ï¼ˆGridï¼‰**ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªåƒç´ çŸ©é˜µ`[H, W, C]`ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–¹å—å˜æˆåºåˆ—ã€‚\n",
    "\n",
    "### 1.1 ä¼ ç»ŸCNNçš„åšæ³•\n",
    "\n",
    "ä¸€å±‚å±‚å·ç§¯ï¼ŒæŠŠå›¾ç‰‡å˜å¾—è¶Šæ¥è¶Šå°ï¼Œè¶Šæ¥è¶Šåšã€‚\n",
    "\n",
    "### 1.2 ViTçš„åšæ³•ï¼ˆæš´åŠ›ç¾å­¦ï¼‰\n",
    "\n",
    "åƒåˆ‡è›‹ç³•ä¸€æ ·ï¼ŒæŠŠä¸€å¼ $224 \\times 224$çš„å¤§å›¾ï¼Œåˆ‡æˆæ— æ•°ä¸ª$16 \\times 16$çš„**å°æ–¹å— (Patches)**ã€‚\n",
    "\n",
    "- **æ¯ä¸€ä¸ª Patchï¼Œå°±ç­‰äº NLP é‡Œçš„ä¸€ä¸ªå•è¯ (Token)ã€‚**\n",
    "- **è®¡ç®—**ï¼š\n",
    "    - å›¾ç‰‡å¤§å°: $224 \\times 224$\n",
    "    - Patch å¤§å°: $16 \\times 16$\n",
    "    - æ¨ªç€èƒ½åˆ‡: $224 / 16 = 14$ ä¸ª\n",
    "    - ç«–ç€èƒ½åˆ‡: $224 / 16 = 14$ ä¸ª\n",
    "    - **Token æ€»æ•°**: $14 \\times 14 = 196$ ä¸ªã€‚\n",
    "\n",
    "è¿™å°±æ˜¯ ViT çš„å…¨éƒ¨ç§˜å¯†ï¼š**Image $\\to$ Patches $\\to$ Linear Projection $\\to$ Transformer Encoder**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2a741",
   "metadata": {},
   "source": [
    "## 2 å·¥ç¨‹å®è·µï¼šç”¨`Cconv2d`å®ç°Patch Embedding\n",
    "\n",
    "è™½ç„¶é€»è¾‘ä¸Šæ˜¯â€œåˆ‡å—å†æ‹‰ç›´â€ï¼Œä½†åœ¨ä»£ç å®ç°æ—¶ï¼Œæˆ‘ä»¬**ç»å¯¹ä¸ä¼š**å†™ `for` å¾ªç¯å»åˆ‡å›¾ï¼ˆå¤ªæ…¢äº†ï¼‰ã€‚æœ€ä¼˜é›…çš„æ–¹æ³•æ˜¯ä½¿ç”¨ **å·ç§¯ (Convolution)**ã€‚\n",
    "\n",
    "**åŸç†**ï¼šå¦‚æœä½ è®¾ç½®ä¸€ä¸ªå·ç§¯æ ¸\n",
    "- **Kernel Size** = Patch Size (æ¯”å¦‚ 16)\n",
    "- **Stride (æ­¥é•¿)** = Patch Size (æ¯”å¦‚ 16)\n",
    "\n",
    "é‚£ä¹ˆè¿™ä¸ªå·ç§¯æ ¸æ¯èµ°ä¸€æ­¥ï¼Œå°±æ­£å¥½è¦†ç›–å¹¶å¤„ç†äº†ä¸€ä¸ªå®Œæ•´çš„ Patchï¼Œè€Œä¸”**äº’ä¸é‡å **ã€‚å·ç§¯çš„è¾“å‡ºç»“æœï¼Œç›´æ¥å°±æ˜¯è¿™ä¸ª Patch çš„ Embedding å‘é‡ï¼\n",
    "\n",
    "**ä»£ç å®ç°ï¼š**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798cb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_sizeï¼šå›¾ç‰‡åˆ†è¾¨ç‡ï¼ˆ224ï¼‰\n",
    "            patch_sizeï¼šæ¯ä¸ªå°æ–¹å—çš„å¤§å°ï¼ˆ16ï¼‰\n",
    "            in_chansï¼šè¾“å…¥é€šé“æ•°ï¼ˆRGBå›¾ç‰‡æ˜¯3ï¼‰\n",
    "            embed_dimï¼šè½¬æ¢åçš„ç‰¹å¾ç»´åº¦ï¼ˆ768, å’ŒBERT-Baseä¸€æ ·ï¼‰\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_size=img_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # è®¡ç®— Patch çš„æ€»æ•°é‡\n",
    "        # (224 // 16) * (224 // 16) = 14 * 14 = 196\n",
    "        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
    "\n",
    "        # --- æ ¸å¿ƒé­”æ³•: ä½¿ç”¨ Conv2d åšåˆ‡ç‰‡ ---\n",
    "        # kernel_size=16, stride=16\n",
    "        # è¿™æ„å‘³ç€å·ç§¯æ ¸ä¸€æ¬¡çœ‹ 16x16 çš„åŒºåŸŸï¼Œç„¶åè·³è¿‡ 16 ä¸ªåƒç´ çœ‹ä¸‹ä¸€ä¸ª\n",
    "        # è¾“å‡ºé€šé“ embed_dim å°±æ˜¯è¦æŠŠè¿™ä¸ª Patch å‹ç¼©æˆçš„å‘é‡é•¿åº¦\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [Batch, Channels, Height, Width] (ä¾‹å¦‚ï¼š[32, 3, 224, 224])\n",
    "        \"\"\"\n",
    "        # 1. å·ç§¯æŠ•å½±\n",
    "        # input: [B, 3, 224, 224]\n",
    "        # output: [B, 768, 14, 14]\n",
    "        x = self.proj(x)\n",
    "\n",
    "        # 2. å±•å¹³ (Flatten)\n",
    "        # æˆ‘ä»¬éœ€è¦åºåˆ—ï¼Œä¸éœ€è¦ grid\n",
    "        # ä»ç»´åº¦ 2 (H) å¼€å§‹å±•å¹³\n",
    "        # output: [B, 768, 196] (196 = 14*14)\n",
    "        x = x.flatten(2)\n",
    "\n",
    "        # 3. ç»´åº¦äº¤æ¢ï¼ˆTransposeï¼‰\n",
    "        # Transformer éœ€è¦çš„è¾“å…¥æ˜¯ [Batch, Seq_Len, Dim]\n",
    "        # ç›®å‰æ˜¯ [B, Dim, Seq_Len]ï¼Œæ‰€ä»¥è¦äº¤æ¢ Dim å’Œ Seq_Len\n",
    "        # output: [B, 196, 768]\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c060f",
   "metadata": {},
   "source": [
    "## 3 è”è°ƒæµ‹è¯•ï¼šæ¥ä¸Šæ˜¨å¤©çš„Transformer\n",
    "\n",
    "ç°åœ¨ï¼Œæˆ‘ä»¬è¦æŠŠè¿™ä¸ª **PatchEmbedding** å’Œæ˜¨å¤©å†™çš„ **TransformerEncoder** è¿èµ·æ¥ï¼Œç»„æˆä¸€ä¸ªå®Œæ•´çš„ **ViT** åŸå‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f837d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Embedding Output shape: torch.Size([2, 196, 512])\n",
      "è¿™ä¸ª shape torch.Size([2, 196, 512]) å®Œå…¨ç¬¦åˆ Transformer çš„è¾“å…¥è¦æ±‚ï¼\n",
      "å®ƒç°åœ¨å’Œ NLP é‡Œçš„ [Batch, Seq_Len, Dim] æ²¡æœ‰ä»»ä½•åŒºåˆ«ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.models.transformer_components import TransformerEncoder\n",
    "\n",
    "# 1. æ¨¡æ‹Ÿä¸€å¼ å›¾ç‰‡ batch\n",
    "# [Batch=2, Channel=3, Height=224, Width=224]\n",
    "dummy_img = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "# 2. å®ä¾‹åŒ– PatchEmbedding\n",
    "patch_embed = PatchEmbedding(img_size=224, patch_size=16, in_chans=3, embed_dim=512)\n",
    "# æ³¨æ„ï¼šè¿™é‡Œ embed_dim è®¾ä¸º 512ï¼Œä¸ºäº†é…åˆæ˜¨å¤©å†™çš„ Encoder é»˜è®¤å‚æ•°\n",
    "\n",
    "# 3. è¿è¡Œ Patch Embedding\n",
    "x = patch_embed(dummy_img)\n",
    "print(\"Patch Embedding Output shape:\", x.shape)\n",
    "# é¢„æœŸ: [2, 196, 512]\n",
    "# è§£é‡Š: 2å¼ å›¾ï¼Œæ¯å¼ å›¾åˆ‡æˆ 196 ä¸ªå—ï¼Œæ¯ä¸ªå—æ˜¯ 512 ç»´å‘é‡\n",
    "\n",
    "# 4. æ¥ä¸Š Transformer Encoder (æ˜¨å¤©å†™çš„)\n",
    "# å‡è®¾æ˜¨å¤©å†™çš„ Encoder éœ€è¦ vocab_size å‚æ•°ï¼Œä½†åœ¨ ViT é‡Œå…¶å®ä¸éœ€è¦æŸ¥è¡¨äº†ï¼Œ\n",
    "# ä¸è¿‡ä¸ºäº†å¤ç”¨ä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥æš‚æ—¶å¿½ç•¥ embedding å±‚ï¼Œç›´æ¥ä¼ ç»™ encoder layers\n",
    "# æˆ–è€…æ›´ç®€å•ï¼šæˆ‘ä»¬ä»…ä»…æŠŠ x ä¼ ç»™ encoder å†…éƒ¨çš„ layers (æ¨¡æ‹Ÿè¿‡ç¨‹)\n",
    "\n",
    "# ä¸ºäº†æ¼”ç¤ºä¸¥è°¨æ€§ï¼Œæˆ‘ä»¬åªçœ‹ç»´åº¦åŒ¹é…\n",
    "print(f\"è¿™ä¸ª shape {x.shape} å®Œå…¨ç¬¦åˆ Transformer çš„è¾“å…¥è¦æ±‚ï¼\")\n",
    "print(\"å®ƒç°åœ¨å’Œ NLP é‡Œçš„ [Batch, Seq_Len, Dim] æ²¡æœ‰ä»»ä½•åŒºåˆ«ã€‚\")\n",
    "\n",
    "# å¦‚æœä½ çœŸçš„æƒ³è·‘é€š ViTï¼Œåªéœ€è¦åœ¨ TransformerEncoder çš„ forward é‡Œï¼Œ\n",
    "# è·³è¿‡ self.embedding(src)ï¼Œç›´æ¥å¤„ç† dense vectors å³å¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511bdf1",
   "metadata": {},
   "source": [
    "## 4 æ·±åº¦è§£æ & å…·èº«æ™ºèƒ½è§†è§’\n",
    "\n",
    "### ğŸ¤– å…·èº«æ™ºèƒ½é¢è¯•é¢˜ (Embodied AI Interview)\n",
    "\n",
    "**Q1: ä¸ºä»€ä¹ˆç°åœ¨çš„æœºå™¨äººå¤§æ¨¡å‹ï¼ˆå¦‚ RT-2ï¼‰éƒ½ç”¨ ViT è€Œä¸ç”¨ ResNetï¼Ÿ**\n",
    "\n",
    "  * **A**:\n",
    "    1.  **ç»Ÿä¸€æ¨¡æ€ (Modality Alignment)**: æœºå™¨äººçš„è¾“å…¥æ˜¯å¤æ‚çš„ï¼ˆæ–‡æœ¬æŒ‡ä»¤ + å›¾åƒè§‚æµ‹ï¼‰ã€‚å¦‚æœç”¨ CNNï¼Œå›¾åƒå‡ºæ¥çš„æ˜¯ Feature Mapï¼Œæ–‡æœ¬å‡ºæ¥çš„æ˜¯ Tokenï¼Œä¸¤è€…å¾ˆéš¾èåˆã€‚å¦‚æœç”¨ ViTï¼Œ**å›¾åƒä¹Ÿæ˜¯ Tokenï¼Œæ–‡æœ¬ä¹Ÿæ˜¯ Token**ï¼Œå®ƒä»¬å¯ä»¥ç›´æ¥æ‹¼æ¥åœ¨ä¸€èµ·ï¼š`[æŒ‡ä»¤Token, å›¾ç‰‡Token, å›¾ç‰‡Token...]`ï¼Œç„¶åæ‰”è¿›åŒä¸€ä¸ª Transformer å°±èƒ½å­¦ä¼šâ€œçœ‹åˆ°å›¾ç‰‡æ‰§è¡ŒæŒ‡ä»¤â€ã€‚\n",
    "    2.  **å…¨å±€æ³¨æ„åŠ›**: CNN å…³æ³¨å±€éƒ¨ï¼ˆå·ç§¯æ ¸ï¼‰ï¼ŒViT ä¸€ä¸Šæ¥å°±æ˜¯ Global Attentionã€‚å¯¹äºæœºå™¨äººæ¥è¯´ï¼Œâ€œæ¡Œå­è¾¹ç¼˜çš„æ¯å­â€å’Œâ€œæˆ‘çš„æ‰‹â€è·ç¦»å¯èƒ½å¾ˆè¿œï¼ŒViT èƒ½æ›´å¥½åœ°æ•æ‰è¿™ç§è¿œè·ç¦»çš„ç©ºé—´å…³ç³»ã€‚\n",
    "\n",
    "**Q2: Patch Size å¯¹æ€§èƒ½æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ**\n",
    "\n",
    "  * **A**:\n",
    "      * **Patch è¶Šå° (å¦‚ 8x8)**: Token æ•°é‡å˜å¤šï¼ˆ$196 \\to 784$ï¼‰ï¼Œå›¾åƒçœ‹å¾—æ›´ç»†ï¼Œä½†è®¡ç®—é‡æ¿€å¢ï¼ˆAttention æ˜¯åºåˆ—é•¿åº¦çš„å¹³æ–¹å¤æ‚åº¦ $O(N^2)$ï¼‰ã€‚\n",
    "      * **Patch è¶Šå¤§ (å¦‚ 32x32)**: Token å˜å°‘ï¼Œè®¡ç®—å¿«ï¼Œä½†ä¸¢å¤±ç»†èŠ‚ï¼Œæœºå™¨äººå¯èƒ½çœ‹ä¸æ¸…å°èºä¸é’‰ã€‚\n",
    "      * **16x16**: æ˜¯ç›®å‰å·¥ä¸šç•Œæœ€å¸¸ç”¨çš„å¹³è¡¡ç‚¹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983238f",
   "metadata": {},
   "source": [
    "## ğŸš€ ä»Šæ—¥æ€»ç»“\n",
    "\n",
    "æ­å–œï¼ä½ åˆšåˆšæ‰“é€šäº† **NLP** å’Œ **CV** çš„ä»»ç£äºŒè„‰ã€‚\n",
    "\n",
    "  * ä½ å‘ç°ï¼š**æ‰€è°“çš„ CV æ¨¡å‹ï¼Œåªè¦åšä¸€æ­¥ `Conv2d` åˆ‡ç‰‡ï¼Œç¬é—´å°±å˜æˆäº† NLP åºåˆ—é—®é¢˜ã€‚**\n",
    "  * è¿™ä¸ºä½ åç»­å­¦ä¹  **å¤šæ¨¡æ€å¤§æ¨¡å‹** å’Œ **å…·èº« Agent** æ‰«æ¸…äº†æœ€å¤§çš„éšœç¢ã€‚\n",
    "\n",
    "**Stage 2 ä»»åŠ¡åœ†æ»¡å®Œæˆï¼** ğŸ‰\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œè¯·èŠ±ä¸€ç‚¹æ—¶é—´æ•´ç†ä½ çš„ `src` æ–‡ä»¶å¤¹ã€‚\n",
    "æ˜å¤©ï¼Œæˆ‘ä»¬å°†è¿›å…¥ **Review Day (å¤ç›˜æ—¥)**ï¼Œä¹Ÿå°±æ˜¯ **Week 6 çš„å¼€å§‹**ã€‚æˆ‘ä»¬å°†åŸºäº Hugging Face åº“ï¼Œç”¨**å·¥ä¸šç•Œ**çš„æ–¹å¼é‡å†™è¿™ä¸€åˆ‡ï¼Œå¹¶å¼€å§‹è®­ç»ƒ BERT åšæ–‡æœ¬åˆ†ç±»ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
