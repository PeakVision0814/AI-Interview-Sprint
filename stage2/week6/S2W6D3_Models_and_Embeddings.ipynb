{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# --- 1. å¼ºåˆ¶é…ç½®å›½å†…é•œåƒ (æœ€å…³é”®çš„ä¸€æ­¥) ---\n",
    "# å¿…é¡»åœ¨ import transformers ä¹‹å‰æˆ–è€…å°½é‡é å‰çš„ä½ç½®è®¾ç½®\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "# --- 2. è·¯å¾„é…ç½® ---\n",
    "# ç¡®ä¿è¿™ä¸ªè·¯å¾„å’Œä½ æŠ¥é”™ä¿¡æ¯é‡Œçš„è·¯å¾„ä¸€è‡´\n",
    "current_dir = os.getcwd()\n",
    "target_path = os.path.abspath(os.path.join(current_dir, '../../data/pretrained_models/bert-base-chinese'))\n",
    "\n",
    "print(f\"ğŸ¯ ç›®æ ‡ä¿®å¤è·¯å¾„: {target_path}\")\n",
    "\n",
    "# æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º\n",
    "if not os.path.exists(target_path):\n",
    "    os.makedirs(target_path)\n",
    "\n",
    "print(\"â³ æ­£åœ¨å°è¯•è¿æ¥é•œåƒç«™ä¸‹è½½æ¨¡å‹æƒé‡ (çº¦ 400MB)... è¯·è€å¿ƒç­‰å¾…ï¼Œä¸è¦ä¸­æ–­\")\n",
    "\n",
    "try:\n",
    "    # --- 3. æ ¸å¿ƒä¸‹è½½é€»è¾‘ ---\n",
    "    # from_pretrained ä¼šè‡ªåŠ¨ä¸‹è½½ pytorch_model.bin æˆ– model.safetensors\n",
    "    # æˆ‘ä»¬å…ˆä¸‹è½½åˆ°å†…å­˜/ç¼“å­˜\n",
    "    model = BertModel.from_pretrained('bert-base-chinese')\n",
    "    \n",
    "    # --- 4. ä¿å­˜åˆ°æŒ‡å®šç›®å½• ---\n",
    "    print(\"âœ… ä¸‹è½½å®Œæˆï¼Œæ­£åœ¨å†™å…¥ç¡¬ç›˜...\")\n",
    "    model.save_pretrained(target_path)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"ğŸ‰ ä¿®å¤æˆåŠŸï¼æ¨¡å‹æƒé‡å·²ä¿å­˜ã€‚\")\n",
    "    print(f\"ğŸ“‚ è¯·æ£€æŸ¥ç›®å½•: {target_path}\")\n",
    "    print(\"ä½ åº”è¯¥èƒ½çœ‹åˆ° 'pytorch_model.bin' æˆ– 'model.safetensors'\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nâŒ ä¸‹è½½å¤±è´¥ã€‚è¯·æ£€æŸ¥æŠ¥é”™ä¿¡æ¯ï¼š\")\n",
    "    print(e)\n",
    "    print(\"\\nğŸ’¡ å»ºè®®ï¼šå¦‚æœä¾ç„¶æŠ¥ç½‘ç»œé”™è¯¯ï¼Œè¯·æ£€æŸ¥æ˜¯å¦å¼€å¯äº†ä»£ç†è½¯ä»¶ï¼ˆæœ‰æ—¶ä»£ç†åè€Œä¼šé˜»æ–­é•œåƒç«™ï¼‰ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b1b1e1",
   "metadata": {},
   "source": [
    "# S2W6D3: é¢„è®­ç»ƒæ¨¡å‹ä¸è¯­ä¹‰å‘é‡ (Pre-trained Models & Embeddings)\n",
    "\n",
    "å¦‚æœè¯´å‰ä¸¤å¤©æˆ‘ä»¬è¿˜åœ¨å¤„ç†â€œçš®æ¯›â€ï¼ˆæŠŠæ–‡æœ¬å˜æˆæ•´æ•° IDï¼‰ï¼Œé‚£ä¹ˆä»Šå¤©æˆ‘ä»¬è¦æ­£å¼è§¦åŠ Transformer çš„**çµé­‚**ã€‚\n",
    "\n",
    "æˆ‘ä»¬è¦åŠ è½½é‚£ä¸ª 400MB çš„å¤§è„‘ï¼ˆæƒé‡ï¼‰ï¼Œè®©è®¡ç®—æœºç¬¬ä¸€æ¬¡çœŸæ­£â€œ**ç†è§£**â€è¯­ä¹‰ã€‚æˆ‘ä»¬å°†è§£å†³ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š**æœºå™¨å¦‚ä½•çŸ¥é“â€œæ‹¿ä¸ªè‹¹æœâ€å’Œâ€œå»å–æ°´æœâ€æ˜¯ç›¸ä¼¼çš„æŒ‡ä»¤ï¼Ÿ**\n",
    "\n",
    "**ğŸ¯ æ ¸å¿ƒç›®æ ‡**:\n",
    "\n",
    "1.  **åŠ è½½æƒé‡**: å­¦ä¼šä½¿ç”¨ `BertModel` åŠ è½½é¢„è®­ç»ƒå¥½çš„å‚æ•°ã€‚\n",
    "2.  **ç†è§£è¾“å‡º**: å½»åº•åˆ†æ¸… `last_hidden_state` å’Œ `pooler_output` çš„åŒºåˆ«ï¼ˆé¢è¯•å¿…è€ƒï¼‰ã€‚\n",
    "3.  **è¯­ä¹‰ç›¸ä¼¼åº¦**: åˆ©ç”¨ Embedding å‘é‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå®ç°å…·èº«æ™ºèƒ½ä¸­çš„**æŒ‡ä»¤æ¨¡ç³ŠåŒ¹é…**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd96db",
   "metadata": {},
   "source": [
    "## 1 ğŸ› ï¸ ç¯å¢ƒç¡®è®¤ (Asset Check)\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œç¡®ä¿ä½ çš„ `data/` ç›®å½•ä¸‹å·²ç»æœ‰æ¨¡å‹æ–‡ä»¶ã€‚ä¸ºäº†ä¿è¯ä»£ç èƒ½è·‘ï¼Œæˆ‘ä»¬å…ˆæ‰§è¡Œä¸€æ®µ\\*\\*â€œèµ„äº§åŠ è½½â€\\*\\*ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8086ca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹èµ„äº§å°±ç»ª: /home/goodminton/study/AI-Interview-Sprint/data/pretrained_models/bert-base-chinese\n",
      "ğŸš€ BERT æ¨¡å‹åŠ è½½æˆåŠŸï¼å¤§è„‘å·²ä¸Šçº¿ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1. å®šä½èµ„äº§è·¯å¾„ (æŒ‡å‘ data/pretrained_models/bert-base-chinese)\n",
    "current_dir = os.getcwd()\n",
    "model_path = os.path.abspath(os.path.join(current_dir, '../../data/pretrained_models/bert-base-chinese'))\n",
    "\n",
    "# 2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨\n",
    "if not os.path.exists(os.path.join(model_path, 'pytorch_model.bin')) and \\\n",
    "   not os.path.exists(os.path.join(model_path, 'model.safetensors')):\n",
    "    print(f\"âš ï¸ è­¦å‘Š: åœ¨ {model_path} ä¸‹æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚\")\n",
    "    print(\"è¯·å…ˆè¿è¡Œ Day 2 çš„ä¸‹è½½è„šæœ¬ï¼Œæˆ–è€…æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚\")\n",
    "else:\n",
    "    print(f\"âœ… æ¨¡å‹èµ„äº§å°±ç»ª: {model_path}\")\n",
    "\n",
    "# 3. åŠ è½½ Tokenizer å’Œ Model\n",
    "# æ³¨æ„: è¿™é‡Œå…¨éƒ¨ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼Œæ–­ç½‘å¯ç”¨\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertModel.from_pretrained(model_path)\n",
    "model.eval() # å¿…åŠ ï¼åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n",
    "\n",
    "print(\"ğŸš€ BERT æ¨¡å‹åŠ è½½æˆåŠŸï¼å¤§è„‘å·²ä¸Šçº¿ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a285b77",
   "metadata": {},
   "source": [
    "## 2 ğŸ§  æ ¸å¿ƒç†è®ºï¼šBERT çš„ä¸¤ç§è¾“å‡º (The Dual Outputs)\n",
    "\n",
    "å½“ä½ æŠŠ`input_ids`å–‚ç»™BERTåï¼Œå®ƒä¼šåå‡ºä¸¤ä¸ªä¸»è¦çš„ä¸œè¥¿ã€‚è¿™æ˜¯**æœ€å®¹æ˜“æ··æ·†**çš„æ¦‚å¿µï¼Œä¹Ÿæ˜¯**é¢è¯•é«˜é¢‘å‘ç‚¹**ã€‚\n",
    "\n",
    "### 2.1 `last_hidden_state`ï¼ˆåºåˆ—è¾“å‡ºï¼‰\n",
    "\n",
    "- **å½¢çŠ¶**ï¼š`[Batch, Seq_Len, 768]`\n",
    "- **å«ä¹‰**ï¼š**æ¯ä¸ªToken**çš„è¯­ä¹‰å‘é‡ã€‚\n",
    "- **ç”¨é€”**ï¼š\n",
    "    - **NERï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰**ï¼šéœ€è¦åˆ¤æ–­æ¯ä¸ªå­—æ˜¯ä¸æ˜¯äººæ–‡/åœ°æ–‡ã€‚\n",
    "    - **QAï¼ˆé—®ç­”ï¼‰**ï¼šéœ€è¦ä»æ–‡ç« ä¸­æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚\n",
    "\n",
    "### 2.2 `pooler_output`ï¼ˆæ± åŒ–è¾“å‡ºï¼‰\n",
    "\n",
    "- **å½¢çŠ¶**ï¼š`[Batch, 768]`\n",
    "- **å«ä¹‰**ï¼šä»£è¡¨ **æ•´ä¸ªå¥å­** çš„è¯­ä¹‰å‘é‡ã€‚\n",
    "- **åŸç†**: å®ƒæå–äº† `[CLS]` ä½ç½®çš„å‘é‡ï¼Œå¹¶ç»è¿‡äº†ä¸€ä¸ªçº¿æ€§å±‚ (`Linear` + `Tanh`) è¿›ä¸€æ­¥å¤„ç†ã€‚\n",
    "- **ç”¨é€”**ï¼š**æ–‡æœ¬åˆ†ç±»**ã€**ç›¸ä¼¼åº¦è®¡ç®—**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93dd7eb",
   "metadata": {},
   "source": [
    "## 3 ğŸ’» ä»£ç å®æˆ˜ï¼šæå–å‘é‡ (Feature Extraction)\n",
    "\n",
    "æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸¤ä¸ªè¾“å‡ºåˆ°åº•é•¿ä»€ä¹ˆæ ·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0dc3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥æ–‡æœ¬ï¼šå…·èº«æ™ºèƒ½æ˜¯æœªæ¥çš„æ–¹å‘\n",
      "------------------------------\n",
      "1. last_hidden_state Shape:torch.Size([1, 12, 768])\n",
      "2. pooler_output Shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# 1. å‡†å¤‡æ•°æ®\n",
    "text = \"å…·èº«æ™ºèƒ½æ˜¯æœªæ¥çš„æ–¹å‘\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 2. å‰å‘ä¼ æ’­ (No Gradients needed for inference)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 3. è·å–ä¸¤ç§è¾“å‡º\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output\n",
    "\n",
    "print(f\"è¾“å…¥æ–‡æœ¬: {text}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"1. last_hidden_state Shape: {last_hidden_state.shape}\")\n",
    "# é¢„æœŸ: [1, 12, 768] (1å¥è¯, 12ä¸ªtoken, 768ç»´)\n",
    "\n",
    "print(f\"2. pooler_output Shape:     {pooler_output.shape}\")\n",
    "# é¢„æœŸ: [1, 768] (1å¥è¯, 768ç»´)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e10768",
   "metadata": {},
   "source": [
    "## 4 ğŸ¤– å…·èº«å®æˆ˜ï¼šæŒ‡ä»¤ç›¸ä¼¼åº¦åŒ¹é… (Semantic Similarity)\n",
    "\n",
    "è¿™æ˜¯ä½ ä½œä¸ºæœºå™¨äººè´Ÿè´£äººçš„**æ ¸å¿ƒæŠ€èƒ½**ã€‚\n",
    "ç”¨æˆ·è¯´è¯æ˜¯å¾ˆéšæ„çš„ï¼Œä½†æœºå™¨äººçš„ API æ˜¯å›ºå®šçš„ã€‚æˆ‘ä»¬éœ€è¦æŠŠç”¨æˆ·çš„â€œè‡ªç„¶è¯­è¨€â€æ˜ å°„åˆ°â€œæ ‡å‡†æŒ‡ä»¤â€ã€‚\n",
    "\n",
    "  * **ç”¨æˆ·è¯´**: \"æŠŠé‚£ä¸ªæ°´æ¯æ‹¿ç»™æˆ‘\" / \"å–ä¸€ä¸‹æ¯å­\" / \"Grab the cup\"\n",
    "  * **æœºå™¨äºº API**: `pick_up_object(target='cup')`\n",
    "\n",
    "**åŸç†**: è®¡ç®—ä¸¤ä¸ªå¥å­å‘é‡çš„ **ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11fa9180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸºå‡†æŒ‡ä»¤: æŠ“å–é¢å‰çš„ç‰©ä½“\n",
      "------------------------------\n",
      "æŒ‡ä»¤1: 'å¸®æˆ‘æŠŠä¸œè¥¿æ‹¿èµ·æ¥' | ç›¸ä¼¼åº¦: 0.9101\n",
      "æŒ‡ä»¤2: 'å‘å·¦æ—‹è½¬90åº¦' | ç›¸ä¼¼åº¦: 0.8965\n",
      ">> åˆ¤å®š: æ‰§è¡ŒæŠ“å–åŠ¨ä½œ\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"è¾…åŠ©å‡½æ•°ï¼šè¾“å…¥æ–‡æœ¬ï¼Œè¿”å›å¥å‘é‡ (pooler_output)\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.pooler_output\n",
    "\n",
    "# --- æ¨¡æ‹Ÿåœºæ™¯ ---\n",
    "standard_cmd = \"æŠ“å–é¢å‰çš„ç‰©ä½“\"  # æ ‡å‡†æŒ‡ä»¤\n",
    "user_cmd_1 = \"å¸®æˆ‘æŠŠä¸œè¥¿æ‹¿èµ·æ¥\"  # æ„æ€ç›¸è¿‘\n",
    "user_cmd_2 = \"å‘å·¦æ—‹è½¬90åº¦\"      # æ„æ€å®Œå…¨ä¸åŒ\n",
    "\n",
    "# 1. æå–å‘é‡\n",
    "vec_std = get_embedding(standard_cmd)\n",
    "vec_1 = get_embedding(user_cmd_1)\n",
    "vec_2 = get_embedding(user_cmd_2)\n",
    "\n",
    "# 2. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "# Cosine Sim = (A . B) / (|A| * |B|)\n",
    "score_1 = F.cosine_similarity(vec_std, vec_1)\n",
    "score_2 = F.cosine_similarity(vec_std, vec_2)\n",
    "\n",
    "print(f\"åŸºå‡†æŒ‡ä»¤: {standard_cmd}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"æŒ‡ä»¤1: '{user_cmd_1}' | ç›¸ä¼¼åº¦: {score_1.item():.4f}\")\n",
    "print(f\"æŒ‡ä»¤2: '{user_cmd_2}' | ç›¸ä¼¼åº¦: {score_2.item():.4f}\")\n",
    "\n",
    "# --- åˆ¤å®šé€»è¾‘ ---\n",
    "threshold = 0.85 # é˜ˆå€¼\n",
    "if score_1 > threshold:\n",
    "    print(\">> åˆ¤å®š: æ‰§è¡ŒæŠ“å–åŠ¨ä½œ\")\n",
    "else:\n",
    "    print(\">> åˆ¤å®š: æŒ‡ä»¤ä¸åŒ¹é…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79886c5f",
   "metadata": {},
   "source": [
    "**å®éªŒé¢„æœŸ**: `score_1` åº”è¯¥æ˜¾è‘—é«˜äº `score_2`ã€‚ä½ ä¼šå‘ç° BERT çœŸçš„å¾ˆæ‡‚ä¸­æ–‡ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873edb6",
   "metadata": {},
   "source": [
    "## 5 ğŸ’¥ é¢è¯•æ·±æŒ–ï¼šCLS è¿˜æ˜¯ Mean Pooling?\n",
    "\n",
    "é¢è¯•å®˜å¯èƒ½ä¼šé—®ï¼š**â€œç›´æ¥ç”¨BERT çš„ `pooler_output` åšç›¸ä¼¼åº¦è®¡ç®—æ˜¯æœ€å¥½çš„å—ï¼Ÿâ€**\n",
    "\n",
    "  * **å›ç­”**: å…¶å®ä¸æ˜¯ã€‚\n",
    "  * **åŸå› **: åŸç”ŸBERTçš„`pooler_output`æ˜¯ä¸ºäº†NSP (Next Sentence Prediction)ä»»åŠ¡è®­ç»ƒçš„ï¼Œå¹¶æœªé’ˆå¯¹â€œè¯­ä¹‰ç›¸ä¼¼åº¦â€åšä¼˜åŒ–ã€‚\n",
    "  * **æ›´ä¼˜è§£**: **Mean Pooling (å¹³å‡æ± åŒ–)**ã€‚å³æŠŠ `last_hidden_state` ä¸­æ‰€æœ‰ Token çš„å‘é‡å–å¹³å‡å€¼ã€‚è¿™é€šå¸¸æ¯”ç›´æ¥ç”¨ `[CLS]` æ•ˆæœæ›´å¥½ã€‚\n",
    "  * **SOTA è§£æ³•**: ä½¿ç”¨ä¸“é—¨è®­ç»ƒè¿‡çš„ **Sentence-BERT (SBERT)** æ¨¡å‹ã€‚\n",
    "\n",
    "*(æ³¨ï¼šä»Šå¤©çš„ä»£ç ç”¨ pooler\\_output æ¼”ç¤ºæ˜¯å¯ä»¥çš„ï¼Œä½†åœ¨ Stage 3 åš RAG æ—¶æˆ‘ä»¬ä¼šæ¢æ›´é«˜çº§çš„æ–¹æ³•)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bf8f1f",
   "metadata": {},
   "source": [
    "\n",
    "## 6\\. âš”ï¸ æ¯æ—¥ç®—æ³•é¢˜ (LeetCode Daily)\n",
    "\n",
    "æ—¢ç„¶ä»Šå¤©è®²çš„æ˜¯**å­—ç¬¦ä¸²åŒ¹é…**çš„å‡çº§ç‰ˆï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰ï¼Œé‚£æˆ‘ä»¬æ¥å›é¡¾ä¸€ä¸‹ç»å…¸çš„**å­—ç¬¦ä¸²æŸ¥æ‰¾**ç®—æ³•ã€‚\n",
    "\n",
    "### ğŸ¯ é¢˜ç›®: [LeetCode 28](../../LeetCode%20practice/1-50.ipynb). æ‰¾å‡ºå­—ç¬¦ä¸²ä¸­ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹çš„ä¸‹æ ‡ (Find the Index of the First Occurrence in a String)\n",
    "\n",
    "  * **éš¾åº¦**: Easy (ä½†è¦å†™å¥½ä¸å®¹æ˜“)\n",
    "  * **æè¿°**: å®ç° `strStr()` å‡½æ•°ã€‚åœ¨ `haystack` ä¸­æ‰¾å‡º `needle` ç¬¬ä¸€ä¸ªå‡ºç°çš„ä½ç½®ã€‚\n",
    "  * **å…³è”**:\n",
    "      * **ä¼ ç»Ÿæœç´¢**: å¿…é¡»å­—é¢ä¸Šå®Œå…¨åŒ¹é… (Exact Match)ã€‚è¿™é“é¢˜å°±æ˜¯ä¼ ç»Ÿæœç´¢çš„æè‡´ã€‚\n",
    "      * **BERT æœç´¢**: è¯­ä¹‰åŒ¹é… (Semantic Match)ã€‚ä½ å¯ä»¥å¯¹æ¯”ä¸€ä¸‹è¿™ä¸¤ç§æœç´¢æ–¹å¼çš„å·®å¼‚ã€‚\n",
    "  * **è¦æ±‚**: å“ªæ€•å¯ä»¥ç”¨ python çš„ `.find()` æˆ– `.index()` ç§’æ€ï¼Œä¹Ÿè¯·å°è¯•ç†è§£ **æ»‘åŠ¨çª—å£** æˆ– **KMP ç®—æ³•** çš„æ€æƒ³ï¼ˆKMP é¢è¯•ä¸å¼ºæ±‚æ‰‹å†™ï¼Œä½†è¦çŸ¥é“åŸç†ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6e8f4",
   "metadata": {},
   "source": [
    "### âœ… Day 3 ä»»åŠ¡æ¸…å•\n",
    "\n",
    "1.  **è¿è¡Œä»£ç **: æˆåŠŸåŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œä¸æŠ¥é”™ã€‚\n",
    "2.  **éªŒè¯ç›¸ä¼¼åº¦**: è¿è¡Œå…·èº«å®æˆ˜ä»£ç ï¼Œçœ‹çœ‹ BERT è®¤ä¸º \"æ‹¿ä¸œè¥¿\" å’Œ \"å‘å·¦è½¬\" çš„ç›¸ä¼¼åº¦åˆ†åˆ«æ˜¯å¤šå°‘ã€‚\n",
    "3.  **LeetCode**: å®Œæˆ LeetCode 28ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
