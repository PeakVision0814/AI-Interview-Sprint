{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c6a6bc",
   "metadata": {},
   "source": [
    "# S2W6D5: Datasets åº“å®æˆ˜ (Real-World Datasets)\n",
    "\n",
    "ä»Šå¤© **Day 5**ï¼Œæˆ‘ä»¬è¦æŠŠæˆ˜åœºè½¬ç§»åˆ° **çœŸå®ä¸–ç•Œ**ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face ç»å…¸çš„ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®é›† `ChnSentiCorp`ï¼ŒçœŸæ­£ä½“éªŒ **Apache Arrow** å¸¦æ¥çš„é£ä¸€èˆ¬çš„é€Ÿåº¦ï¼Œå¹¶æŒæ¡æ•°æ®çš„**æŒä¹…åŒ–å­˜å‚¨**ã€‚\n",
    "\n",
    "**ğŸ¯ æ ¸å¿ƒç›®æ ‡**:\n",
    "\n",
    "1.  **ç†è®ºç¥›é­…**: ç†è§£ä¸ºä»€ä¹ˆ Datasets èƒ½åœ¨ 8GB å†…å­˜çš„ç”µè„‘ä¸Šå¤„ç† 100GB æ•°æ® (Memory Mapping)ã€‚\n",
    "2.  **å®æˆ˜æ¼”ç»ƒ**: åŠ è½½çœŸå®çš„ `ChnSentiCorp` æ•°æ®é›†ï¼Œæ¸…æ´—ã€æ˜ å°„ã€ä¿å­˜ã€‚\n",
    "3.  **å·¥ç¨‹æ•ˆç‡**: å­¦ä¼š `save_to_disk`ï¼Œé¿å…æ¯æ¬¡è®­ç»ƒéƒ½é‡æ–°é¢„å¤„ç†æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc49135",
   "metadata": {},
   "source": [
    "## 1 ğŸ“š ç†è®ºï¼šApache Arrow ä¸å†…å­˜æ˜ å°„\n",
    "\n",
    "é¢è¯•å®˜å¸¸é—®ï¼š*â€œPython çš„ List å’Œ Pandas è¯»å–å¤§æ–‡ä»¶éƒ½å¾ˆæ…¢ä¸”å å†…å­˜ï¼ŒHugging Face æ˜¯æ€ä¹ˆè§£å†³çš„ï¼Ÿâ€*\n",
    "\n",
    "  * **ä¼ ç»Ÿæ–¹å¼ (Python Object)**: æ¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€æ¯ä¸€ä¸ªæ•°å­—éƒ½æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œæœ‰å·¨å¤§çš„ Overheadï¼ˆå¤´éƒ¨å¼€é”€ï¼‰ã€‚è¯»å– 1GB æ–‡æœ¬å¯èƒ½å  5GB å†…å­˜ã€‚\n",
    "  * **HF Datasets æ–¹å¼ (Apache Arrow)**:\n",
    "      * **åˆ—å¼å­˜å‚¨**: æ•°æ®åƒä¸€æ ¹æ ¹ç´§å‡‘çš„æŸ±å­æ’åˆ—ï¼Œæ²¡æœ‰é¢å¤–çš„ Python å¯¹è±¡å¼€é”€ã€‚\n",
    "      * **Memory Mapping (mmap)**:\n",
    "\n",
    "![mmap](https://encrypted-tbn1.gstatic.com/licensed-image?q=tbn:ANd9GcTBNFnVT9w-t3IYzQlCp6OlrLYmLKEC_1o98KbprMpNm9Ls8I6m7QU6NbVlzFzNcdB7mjGrPL4i5NbG_dfsAV4BzJAv4XBFby-uB9mQ4VoPnsJNWq0)\n",
    "\n",
    "æ“ä½œç³»ç»Ÿç›´æ¥æŠŠç¡¬ç›˜æ–‡ä»¶â€œæ˜ å°„â€åˆ°è™šæ‹Ÿå†…å­˜åœ°å€ã€‚**ä½ éœ€è¦è¯»å“ªä¸€è¡Œï¼ŒOS æ‰ä»ç¡¬ç›˜åŠ è½½å“ªä¸€è¡Œ**ã€‚è¿™å«â€œé›¶å†…å­˜æ‹·è´â€ (Zero-Copy)ã€‚\n",
    "\\* **ç»“è®º**: å“ªæ€•æ•°æ®é›†æœ‰ 1TBï¼Œåªè¦ä½ ä¸ä¸€æ¬¡æ€§æ‰“å°å‡ºæ¥ï¼Œå®ƒåœ¨ Python é‡Œçœ‹èµ·æ¥å°±åªå å‡  KBã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9114a",
   "metadata": {},
   "source": [
    "## 2 ğŸ’» ä»£ç å®æˆ˜ï¼šåŠ è½½çœŸå®æ•°æ®é›†\n",
    "\n",
    "æˆ‘ä»¬å…ˆé…ç½®é•œåƒï¼Œå°è¯•åŠ è½½äº‘ç«¯æ•°æ®é›†ã€‚\n",
    "\n",
    "### 2.1 åŠ è½½æ•°æ® (Load from Hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800b27a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ æ­£åœ¨ä»æœ¬åœ°åŠ è½½ Arrow æ–‡ä»¶...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 9600 examples [00:00, 363149.75 examples/s]\n",
      "Generating validation split: 1200 examples [00:00, 268006.65 examples/s]\n",
      "Generating test split: 1200 examples [00:00, 246361.47 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… åŠ è½½æˆåŠŸï¼\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 9600\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 1200\n",
      "    })\n",
      "})\n",
      "\n",
      "--- æŸ¥çœ‹ç¬¬ä¸€æ¡æ•°æ® ---\n",
      "{'label': 1, 'text': 'é€‰æ‹©ç æ±ŸèŠ±å›­çš„åŸå› å°±æ˜¯æ–¹ä¾¿ï¼Œæœ‰ç”µåŠ¨æ‰¶æ¢¯ç›´æ¥åˆ°è¾¾æµ·è¾¹ï¼Œå‘¨å›´é¤é¦†ã€é£Ÿå»Šã€å•†åœºã€è¶…å¸‚ã€æ‘Šä½ä¸€åº”ä¿±å…¨ã€‚é…’åº—è£…ä¿®ä¸€èˆ¬ï¼Œä½†è¿˜ç®—æ•´æ´ã€‚ æ³³æ± åœ¨å¤§å ‚çš„å±‹é¡¶ï¼Œå› æ­¤å¾ˆå°ï¼Œä¸è¿‡å¥³å„¿å€’æ˜¯å–œæ¬¢ã€‚ åŒ…çš„æ—©é¤æ˜¯è¥¿å¼çš„ï¼Œè¿˜ç®—ä¸°å¯Œã€‚ æœåŠ¡å—ï¼Œä¸€èˆ¬'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. æŒ‡å‘ä½ å­˜æ”¾æ–‡ä»¶çš„ç›®å½•\n",
    "# å‡è®¾ä½ æ”¾åœ¨äº†è¿™é‡Œ\n",
    "data_dir = \"../../data/raw_data/ChnSentiCorp\"\n",
    "\n",
    "# 2. å®šä¹‰æ–‡ä»¶è·¯å¾„å­—å…¸\n",
    "data_files = {\n",
    "    \"train\": os.path.join(data_dir, \"chn_senti_corp-train.arrow\"),\n",
    "    \"validation\": os.path.join(data_dir, \"chn_senti_corp-validation.arrow\"),\n",
    "    \"test\": os.path.join(data_dir, \"chn_senti_corp-test.arrow\")\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‚ æ­£åœ¨ä»æœ¬åœ°åŠ è½½ Arrow æ–‡ä»¶...\")\n",
    "\n",
    "try:\n",
    "    # 3. æŒ‡å®šåŠ è½½å™¨ä¸º \"arrow\"\n",
    "    # è¿™ä¼šç›´æ¥è¯»å–äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œé€Ÿåº¦æå¿«ï¼Œæ— éœ€è§£æè„šæœ¬\n",
    "    dataset = load_dataset(\"arrow\", data_files=data_files)\n",
    "    \n",
    "    print(\"\\nâœ… åŠ è½½æˆåŠŸï¼\")\n",
    "    print(dataset)\n",
    "    \n",
    "    print(\"\\n--- æŸ¥çœ‹ç¬¬ä¸€æ¡æ•°æ® ---\")\n",
    "    print(dataset['train'][0])\n",
    "    \n",
    "    # æç¤ºï¼šArrowæ–‡ä»¶é€šå¸¸å·²ç»åŒ…å«æ­£ç¡®çš„åˆ—åï¼Œä¸éœ€è¦æ”¹å\n",
    "    # å¦‚æœåˆ—åè¿˜æ˜¯ä¸å¯¹ï¼Œå¯ä»¥ç”¨ rename_column è‡ªè¡Œè°ƒæ•´\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ åŠ è½½å¤±è´¥: {e}\")\n",
    "    print(\"è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œä»¥åŠæ–‡ä»¶åæ˜¯å¦ä¸ä»£ç ä¸­ä¸€è‡´ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418ee5e",
   "metadata": {},
   "source": [
    "### 2.2 æ•°æ®æ¸…æ´—ä¸è¿‡æ»¤ (Filter)\n",
    "\n",
    "çœŸå®æ•°æ®å¾€å¾€æ˜¯è„çš„ã€‚æ¯”å¦‚æœ‰äº›è¯„è®ºå¤ªçŸ­ï¼ˆ\"å•Š\"ï¼‰ï¼Œæˆ–è€…å¤ªé•¿ã€‚æˆ‘ä»¬éœ€è¦æ¸…æ´—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13baed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9600/9600 [00:00<00:00, 133309.00 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:00<00:00, 98062.67 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:00<00:00, 89027.41 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ•°é‡: 9600\n",
      "è¿‡æ»¤åæ•°é‡: 9598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset = dataset.filter(lambda x: len(x['text']) > 5)\n",
    "\n",
    "print(f\"åŸå§‹æ•°é‡: {len(dataset['train'])}\")\n",
    "print(f\"è¿‡æ»¤åæ•°é‡: {len(filtered_dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967232f",
   "metadata": {},
   "source": [
    "### 2.3 é«˜æ•ˆæ˜ å°„ (Map with Tokenizer)\n",
    "\n",
    "è¿™æ˜¯ä»Šå¤©çš„é‡å¤´æˆã€‚æˆ‘ä»¬è¦æŠŠæ¸…æ´—åçš„æ–‡æœ¬æ‰¹é‡è½¬ä¸º IDã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0ea4ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9598/9598 [00:02<00:00, 3267.47 examples/s]\n",
      "Map (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:00<00:00, 2169.19 examples/s]\n",
      "Map (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:00<00:00, 2230.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- å¤„ç†åçš„åˆ—å ---\n",
      "['label', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# åŠ è½½æœ¬åœ° Tokenizer\n",
    "model_path = \"../../data/pretrained_models/bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # max_length=128: æƒ…æ„Ÿåˆ†æé€šå¸¸ä¸éœ€è¦å¤ªé•¿ï¼Œæˆªæ–­èƒ½æå‡é€Ÿåº¦\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=128 \n",
    "    )\n",
    "\n",
    "# --- æ ¸å¿ƒï¼šå¤šè¿›ç¨‹æ‰¹é‡æ˜ å°„ ---\n",
    "# batched=True: å¿…é¡»å¼€ï¼åˆ©ç”¨ Rust åº•å±‚åŠ é€Ÿ\n",
    "# num_proc=4: å¼€å¯ 4 ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç† (å¦‚æœä½ çš„ CPU æ ¸å¤Ÿå¤š)\n",
    "tokenized_dataset = filtered_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True, \n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"] # å¤„ç†å®Œåï¼ŒåŸå§‹æ–‡æœ¬å°±ä¸éœ€è¦äº†ï¼Œåˆ æ‰çœç©ºé—´\n",
    ")\n",
    "\n",
    "print(\"\\n--- å¤„ç†åçš„åˆ—å ---\")\n",
    "print(tokenized_dataset['train'].column_names)\n",
    "# åº”è¯¥åŒ…å«: input_ids, token_type_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430c95a",
   "metadata": {},
   "source": [
    "### 2.4 æ•°æ®æŒä¹…åŒ– (Save to Disk)\n",
    "\n",
    "**è¿™æ˜¯å·¥ä¸šç•Œæœ€å…³é”®çš„ä¸€æ­¥**ã€‚\n",
    "é¢„å¤„ç†ï¼ˆTokenizerï¼‰æ˜¯å¾ˆè€—æ—¶çš„ã€‚ä½ è‚¯å®šä¸æƒ³æ˜å¤©è®­ç»ƒæ¨¡å‹æ—¶ï¼Œåˆé‡æ–°è·‘ä¸€éä¸Šé¢çš„ä»£ç ã€‚\n",
    "æˆ‘ä»¬å¯ä»¥æŠŠå¤„ç†å¥½çš„ Arrow æ–‡ä»¶å­˜åˆ°ç¡¬ç›˜ä¸Šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db2d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9598/9598 [00:00<00:00, 181796.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:00<00:00, 77980.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:00<00:00, 104040.45 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°: ../../data/processed_data/chn_senti_corp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = \"../../data/processed_data/chn_senti_corp\"\n",
    "\n",
    "# ä¿å­˜\n",
    "tokenized_dataset.save_to_disk(save_path)\n",
    "print(f\"âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°: {save_path}\")\n",
    "\n",
    "# --- æ¨¡æ‹Ÿï¼šä¸‹æ¬¡è®­ç»ƒæ—¶ç›´æ¥åŠ è½½ ---\n",
    "# from datasets import load_from_disk\n",
    "# reloaded_dataset = load_from_disk(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285087df",
   "metadata": {},
   "source": [
    "## 3 ğŸ’¥ é¢è¯•æ·±æŒ– (Deep Dive)\n",
    "\n",
    "**Q: `dataset.map` å’Œ Python çš„ `for` å¾ªç¯æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿ**\n",
    "\n",
    "  * **A**:\n",
    "    1.  **å¹¶è¡Œæ€§**: `map` æ”¯æŒ `batched=True` å’Œ `num_proc`ï¼Œå¯ä»¥å¹¶è¡Œè°ƒç”¨ Tokenizer (Rust å®ç°)ï¼Œæ¯”å•çº¿ç¨‹ Python å¾ªç¯å¿« 10-100 å€ã€‚\n",
    "    2.  **å†…å­˜ç®¡ç†**: `map` å¤„ç†åçš„ç»“æœä¼šç›´æ¥å†™å…¥ç¡¬ç›˜ç¼“å­˜ (Arrow æ–‡ä»¶)ï¼Œè€Œä¸æ˜¯å †ç§¯åœ¨ RAM é‡Œï¼Œä¸ä¼šå¯¼è‡´ OOMã€‚\n",
    "\n",
    "**Q: ä¸ºä»€ä¹ˆä½ è¯´ Arrow æ ¼å¼æ˜¯â€œåˆ—å¼å­˜å‚¨â€ï¼Œè¿™æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ**\n",
    "\n",
    "  * **A**: åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦å–å‡ºä¸€æ•´ä¸ª Batch çš„ `input_ids`ï¼ˆä¸€åˆ—ï¼‰ä¼ ç»™ GPUã€‚\n",
    "      * **è¡Œå¼å­˜å‚¨ (CSV/JSON)**: å¿…é¡»è¯»å®Œæ¯ä¸€è¡Œï¼Œæ‰èƒ½æŠŠ `input_ids` æŠ å‡ºæ¥ã€‚\n",
    "      * **åˆ—å¼å­˜å‚¨ (Arrow)**: `input_ids` åœ¨ç¡¬ç›˜ä¸Šæ˜¯è¿ç»­å­˜æ”¾çš„ï¼Œè¯»å–æ—¶åªéœ€è¦ä¸€æ¬¡è¿ç»­ I/Oï¼Œé€Ÿåº¦æå¿«ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ac6b3",
   "metadata": {},
   "source": [
    "## 4 âš”ï¸ æ¯æ—¥ç®—æ³•é¢˜ (LeetCode Daily)\n",
    "\n",
    "ä»Šå¤©è¿™é“é¢˜æ˜¯é¢è¯•ä¸­ **åŒæŒ‡é’ˆ (Two Pointers)** ç®—æ³•çš„å·…å³°ä¹‹ä½œï¼Œä¹Ÿæ˜¯é¢è¯•å®˜æœ€çˆ±è€ƒçš„é¢˜ç›®ä¹‹ä¸€ã€‚\n",
    "\n",
    "### ğŸ¯ é¢˜ç›®: [LeetCode 15. ä¸‰æ•°ä¹‹å’Œ (3Sum)](../../LeetCode%20practice/1-50.ipynb)\n",
    "\n",
    "  * **éš¾åº¦**: Medium (æ¥è¿‘ Hard)\n",
    "  * **æè¿°**: ç»™ä½ ä¸€ä¸ªæ•´æ•°æ•°ç»„ `nums`ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨ä¸‰ä¸ªå…ƒç´  $a, b, c$ ä½¿å¾— $a + b + c = 0$ï¼Ÿè¯·ä½ æ‰¾å‡ºæ‰€æœ‰å’Œä¸º 0 ä¸”ä¸é‡å¤çš„ä¸‰å…ƒç»„ã€‚\n",
    "  * **æ ¸å¿ƒæ€è·¯**: **æ’åº + åŒæŒ‡é’ˆ**ã€‚\n",
    "    1.  å…ˆå¯¹æ•°ç»„è¿›è¡Œæ’åºã€‚\n",
    "    2.  å›ºå®šç¬¬ä¸€ä¸ªæ•° `nums[i]`ã€‚\n",
    "    3.  ç”¨ä¸¤ä¸ªæŒ‡é’ˆ `L` (å·¦) å’Œ `R` (å³) åœ¨ `i` åé¢çš„åŒºé—´é‡Œå¯»æ‰¾ `nums[L] + nums[R] = -nums[i]`ã€‚\n",
    "    4.  **éš¾ç‚¹**: å¦‚ä½•å»é‡ï¼Ÿï¼ˆæ¯”å¦‚ `[-1, -1, 2]`ï¼Œä¸¤ä¸ª `-1` ä¼šå¯¼è‡´é‡å¤ç»“æœï¼‰ã€‚\n",
    "\n",
    "-----\n",
    "\n",
    "### âœ… Day 5 ä»»åŠ¡æ¸…å•\n",
    "\n",
    "1.  **è¿è¡Œä»£ç **: åŠ è½½ `ChnSentiCorp`ï¼Œè·‘é€š `filter` -\\> `map` -\\> `save_to_disk` çš„å…¨æµç¨‹ã€‚\n",
    "      * *æ³¨ï¼šå¦‚æœ `ChnSentiCorp` ä¸‹è½½æ…¢ï¼Œå¯ä»¥æ¢æˆ `load_dataset(\"csv\", data_files=...)` åŠ è½½æˆ‘ä»¬æ˜¨å¤©çš„æ¨¡æ‹Ÿ CSVï¼Œæµç¨‹æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ã€‚*\n",
    "2.  **æŸ¥çœ‹ç¼“å­˜**: å» `../../data/processed_data/` ç›®å½•ä¸‹çœ‹çœ‹ç”Ÿæˆäº†ä»€ä¹ˆæ–‡ä»¶ï¼ˆåº”è¯¥ä¼šæœ‰ `dataset.arrow`ï¼‰ã€‚\n",
    "3.  **LeetCode**: æ”»å…‹ LeetCode 15ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
