{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d893d26",
   "metadata": {},
   "source": [
    "# S2W6D7:æœ¬å‘¨å¤ä¹ \n",
    "\n",
    "è€Œä¸”ï¼Œè¿™æ¬¡æˆ‘ä»¬ä¸ä¾èµ– Hugging Face çš„é«˜çº§å°è£…ï¼Œæˆ‘ä»¬è¦å†™ä¸€ä¸ª\\*\\*â€œç™½ç›’â€è„šæœ¬\\*\\*ï¼Œè¿™æ‰æ˜¯ç®—æ³•å·¥ç¨‹å¸ˆçš„åº•æ°”ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2e7f3",
   "metadata": {},
   "source": [
    "### ğŸš€ 1 å¾®å‹æ¨ç†è„šæœ¬ (The Micro-Pipeline)\n",
    "\n",
    "æˆ‘ä»¬è¦å†™ä¸€æ®µä»£ç ï¼Œæ¨¡æ‹Ÿ**æ¨¡å‹ä¸Šçº¿éƒ¨ç½²**åçš„æ ·å­ã€‚\n",
    "**ä»»åŠ¡**ï¼šè¾“å…¥ä¸¤å¥è¯ï¼Œè¾“å‡ºå®ƒä»¬çš„ç›¸ä¼¼åº¦ã€‚è¦æ±‚**å…¨é“¾è·¯æœ¬åœ°åŒ–**ï¼Œä¸è”ç½‘ã€‚\n",
    "\n",
    "è¯·ç›´æ¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œè¿™æ˜¯ä½ è¿™ä¸€å‘¨å¿ƒè¡€çš„ç»“æ™¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49173e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¡ æ­£åœ¨åŠ è½½æ¨¡å‹èµ„äº§: ../../data/pretrained_models/bert-base-chinese ...\n",
      "âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼\n",
      "\n",
      "åŒ¹é…ç»“æœ:\n",
      "'è¯·æŠŠæ¡Œå­ä¸Šçš„è‹¹æœæ‹¿ç»™æˆ‘' vs 'æŠ“å–ä½äºæ¡Œé¢çš„çº¢è‰²æ°´æœ': 0.7387\n",
      "'è¯·æŠŠæ¡Œå­ä¸Šçš„è‹¹æœæ‹¿ç»™æˆ‘' vs 'å‘å·¦æ—‹è½¬ 90 åº¦': 0.6463\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "\n",
    "class SimpleInferencePipeline:\n",
    "    def __init__(self, model_dir):\n",
    "        # 1. åŠ è½½æœ¬åœ°èµ„äº§ (Day 1 & Day 3 çš„æˆæœ)\n",
    "        # æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒï¼šè·¯å¾„æ˜¯æ­»çš„ï¼Œä¸èƒ½ä¾èµ–ç½‘ç»œ\n",
    "        print(f\"ğŸ“¡ æ­£åœ¨åŠ è½½æ¨¡å‹èµ„äº§: {model_dir} ...\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "        self.model = BertModel.from_pretrained(model_dir)\n",
    "        self.model.eval() # å¿…åŠ ï¼(Day 3 é‡ç‚¹)\n",
    "        print(\"âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼\")\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        # 2. é¢„å¤„ç† (Day 1 & Day 2 çš„æˆæœ)\n",
    "        # ç”Ÿäº§ç¯å¢ƒé€šå¸¸ä¸€æ¡æ¡å¤„ç†ï¼Œä¸éœ€è¦ Collate_fn\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,      # å•æ¡å…¶å®Padä¸Padæ— æ‰€è°“ï¼Œä½†ä¸ºäº†ä»£ç é€šç”¨æ€§ä¿ç•™\n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # 3. æ¨¡å‹æ¨ç† (Day 3 çš„æˆæœ)\n",
    "        with torch.no_grad(): # å¿…åŠ ï¼çœæ˜¾å­˜\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        # 4. å‘é‡è·å– (Day 3 çš„é¿å‘æŒ‡å—)\n",
    "        # é¿å‘ï¼šåŸç”Ÿ BERT pooler_output æ•ˆæœä¸å¥½ï¼Œè¿™é‡Œæˆ‘ä»¬ç”¨ Mean Pooling\n",
    "        # last_hidden_state: [1, Seq_Len, 768]\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        # æ‰‹å†™ Mean Pooling (æ’é™¤ Padding çš„å½±å“)\n",
    "        # è¿™æ˜¯ä¸€ä¸ªè¿›é˜¶æŠ€å·§ï¼šåªå¯¹ mask=1 çš„éƒ¨åˆ†æ±‚å¹³å‡\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def compute_similarity(self, text1, text2):\n",
    "        # 5. ç›¸ä¼¼åº¦è®¡ç®—\n",
    "        vec1 = self.get_embedding(text1)\n",
    "        vec2 = self.get_embedding(text2)\n",
    "        score = F.cosine_similarity(vec1, vec2)\n",
    "        return score.item()\n",
    "\n",
    "# --- ğŸš€ å¯åŠ¨å¼•æ“ ---\n",
    "# æŒ‡å‘ä½ çš„æœ¬åœ°è·¯å¾„\n",
    "model_path = \"../../data/pretrained_models/bert-base-chinese\"\n",
    "pipeline = SimpleInferencePipeline(model_path)\n",
    "\n",
    "# æµ‹è¯•\n",
    "t1 = \"è¯·æŠŠæ¡Œå­ä¸Šçš„è‹¹æœæ‹¿ç»™æˆ‘\"\n",
    "t2 = \"æŠ“å–ä½äºæ¡Œé¢çš„çº¢è‰²æ°´æœ\"\n",
    "t3 = \"å‘å·¦æ—‹è½¬ 90 åº¦\"\n",
    "\n",
    "score_pos = pipeline.compute_similarity(t1, t2)\n",
    "score_neg = pipeline.compute_similarity(t1, t3)\n",
    "\n",
    "print(f\"\\nåŒ¹é…ç»“æœ:\")\n",
    "print(f\"'{t1}' vs '{t2}': {score_pos:.4f}\")\n",
    "print(f\"'{t1}' vs '{t3}': {score_neg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fbc78",
   "metadata": {},
   "source": [
    "**è§‚å¯Ÿé‡ç‚¹**ï¼š\n",
    "è¿™æ®µä»£ç é‡Œï¼Œæˆ‘ä»¬ç‰¹æ„æ‰‹å†™äº†ä¸€ä¸ª `Mean Pooling` çš„é€»è¾‘ï¼ˆè€Œä¸æ˜¯ç®€å•çš„ `.mean()`ï¼‰ï¼Œè¿™è€ƒè™‘äº† Mask çš„å½±å“ï¼Œæ¯” Hugging Face é»˜è®¤çš„æ›´ä¸¥è°¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9457423",
   "metadata": {},
   "source": [
    "# ğŸ¤ 2 æ¨¡æ‹Ÿé¢è¯• (Mock Interview Self-Check)\n",
    "\n",
    "è¿™äº›é—®é¢˜æ˜¯ä½ ä¸‹å‘¨é¢è¯•å¯èƒ½ä¼šé‡åˆ°çš„â€œå¿…æ€æŠ€â€ã€‚æˆ‘ä»¬æ¥æ·±åº¦æ‹†è§£ã€‚\n",
    "\n",
    "## ğŸ”¥ Q1: `bert-base-chinese` çš„è¯è¡¨å¤§å°æ˜¯å¤šå°‘ï¼Ÿå®ƒæ˜¯æ€ä¹ˆå®šçš„ï¼Ÿ\n",
    "\n",
    "  * **æ ‡å‡†ç­”æ¡ˆ**:\n",
    "    > **21128**ã€‚\n",
    "  * **æ·±åº¦è§£æ (è¿½é—®)**:\n",
    "      * å®ƒæ˜¯åŒ…å«äº†å¸¸è§çš„**æ±‰å­—**ï¼ˆçº¦ 2 ä¸‡ä¸ªï¼‰ã€**å¸Œè…Šå­—æ¯**ã€**ä¿„è¯­å­—æ¯**ã€**å›¾å½¢ç¬¦å·**ä»¥åŠ **[CLS], [SEP], [PAD], [UNK], [MASK]** ç­‰ç‰¹æ®Š Tokenã€‚\n",
    "      * *é¢è¯•å‘ç‚¹*ï¼šå®ƒå¹¶ä¸åŒ…å«æ‰€æœ‰æ±‰å­—ã€‚ç”Ÿåƒ»å­—ä¼šè¢«æ˜ å°„ä¸º `[UNK]`ã€‚\n",
    "      * ç›¸æ¯”ä¹‹ä¸‹ï¼Œè‹±æ–‡ `bert-base-uncased` æ˜¯ **30522**ã€‚\n",
    "\n",
    "## ğŸ”¥ Q2: ä¸ºä»€ä¹ˆ BERT çš„è¾“å…¥é•¿åº¦é™åˆ¶é€šå¸¸æ˜¯ 512ï¼Ÿ\n",
    "\n",
    "è¿™ä¸ªé—®é¢˜çš„å«é‡‘é‡æé«˜ï¼Œæ¶µç›–äº†åŸç†å’Œå·¥ç¨‹ä¸¤ä¸ªç»´åº¦ã€‚\n",
    "\n",
    "  * **ç»´åº¦ 1ï¼šæ•°å­¦å¤æ‚åº¦ (O(LÂ²))**:\n",
    "\n",
    "    > Self-Attention éœ€è¦è®¡ç®—æ¯ä¸ªè¯ä¸å…¶ä»–æ‰€æœ‰è¯çš„ç›¸å…³æ€§ï¼Œç”Ÿæˆä¸€ä¸ª $L \\times L$ çš„ Attention Mapã€‚\n",
    "\n",
    "    >   * $L=512$ æ—¶ï¼ŒçŸ©é˜µå¤§å°æ˜¯ $26ä¸‡$ã€‚\n",
    "    >   * $L=1024$ æ—¶ï¼ŒçŸ©é˜µå¤§å°æ˜¯ $100ä¸‡$ (ç¿»äº†4å€)ã€‚\n",
    "    >   * $L=2048$ æ—¶ï¼ŒçŸ©é˜µå¤§å°æ˜¯ $400ä¸‡$ã€‚\n",
    "    >     **æ˜¾å­˜å ç”¨æ˜¯åºåˆ—é•¿åº¦çš„å¹³æ–¹çº§å¢é•¿**ã€‚512 æ˜¯å½“æ—¶ Google åœ¨æ€§èƒ½å’Œç®—åŠ›ä¹‹é—´æ‰¾åˆ°çš„ä¸€ä¸ªå¹³è¡¡ç‚¹ã€‚\n",
    "\n",
    "  * **ç»´åº¦ 2ï¼šä½ç½®ç¼–ç  (Positional Embedding)**:\n",
    "\n",
    "    > BERT ä½¿ç”¨çš„æ˜¯ **ç»å¯¹ä½ç½®ç¼–ç  (Absolute Positional Embedding)**ï¼Œè€Œä¸”å®ƒæ˜¯**å­¦ä¹ å‡ºæ¥çš„ (Learned)**ï¼Œä¸æ˜¯åƒæ­£å¼¦å‡½æ•°é‚£æ ·ç®—å‡ºæ¥çš„ã€‚\n",
    "    > Google é¢„è®­ç»ƒæ—¶åªåˆå§‹åŒ–äº† 512 ä¸ªä½ç½®å‘é‡ã€‚å¦‚æœä½ è¾“å…¥ç¬¬ 513 ä¸ªå­—ï¼Œæ¨¡å‹å‹æ ¹å°±æ²¡æœ‰å¯¹åº”çš„ä½ç½®å‘é‡å¯ç”¨ï¼ˆIndexErrorï¼‰ã€‚\n",
    "    > *è§£å†³åŠæ³•*ï¼šè¦ä¹ˆæˆªæ–­ï¼Œè¦ä¹ˆä½¿ç”¨æ”¯æŒé•¿æ–‡æœ¬çš„æ¨¡å‹ï¼ˆå¦‚ Longformer, RoFormerï¼‰ã€‚\n",
    "\n",
    "## ğŸ”¥ Q3: `DataCollatorWithPadding` å’Œä½ è‡ªå·±å†™çš„ `collate_fn` æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\n",
    "\n",
    "è¿™æ˜¯è€ƒå¯Ÿä½  Day 4 å’Œ Day 6 çš„å®æˆ˜ç»éªŒã€‚\n",
    "\n",
    "  * **DataCollatorWithPadding (å®˜æ–¹è½®å­)**:\n",
    "      * **ä¼˜ç‚¹**: æè‡´ä¼˜åŒ–ï¼ˆRust å®ç°ï¼‰ï¼Œé€Ÿåº¦æå¿«ï¼›è‡ªåŠ¨å¤„ç† Hugging Face æ ‡å‡†æ ¼å¼ã€‚\n",
    "      * **ç¼ºç‚¹**: â€œé»‘ç›’â€ã€‚å®ƒé»˜è®¤åªå¤„ç† `input_ids`, `token_type_ids`, `attention_mask`, `labels` è¿™å‡ ä¸ª Keyã€‚å¦‚æœä½ æœ‰ä¸€åˆ—å« `my_custom_feature`ï¼Œå®ƒå¯èƒ½ä¼šç»™ä½ ä¸¢æ‰æˆ–è€…æŠ¥é”™ã€‚\n",
    "  * **æ‰‹å†™ collate\\_fn (Day 6 æˆæœ)**:\n",
    "      * **ä¼˜ç‚¹**: **å®Œå…¨å¯æ§**ã€‚\n",
    "          * ä½ å¯ä»¥å†³å®š Padding ç”¨ `0` è¿˜æ˜¯ `-100`ã€‚\n",
    "          * ä½ å¯ä»¥å†³å®šæ˜¯å¦è¦æŠŠ `text` åŸå§‹å†…å®¹ä¹Ÿä¿ç•™åœ¨ Batch é‡Œã€‚\n",
    "          * ä½ å¯ä»¥åš**Data Augmentation (æ•°æ®å¢å¼º)**ï¼šåœ¨æ‰“åŒ…æˆ Batch ä¹‹å‰ï¼ŒéšæœºæŠŠæŸäº›å­— mask æ‰ï¼ˆBERT é¢„è®­ç»ƒé€»è¾‘ï¼‰ã€‚\n",
    "      * **ç¼ºç‚¹**: å†™èµ·æ¥éº»çƒ¦ï¼Œå¦‚æœ Python å¾ªç¯å†™å¾—ä¸å¥½ï¼Œä¼šæ‹–æ…¢ GPU ç­‰å¾…æ—¶é—´ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4940f",
   "metadata": {},
   "source": [
    "### ğŸ“ Week 6 ç»“é¡¹æ¸…å•\n",
    "\n",
    "æ­å–œé»„ç»„é•¿ï¼ä½ å·²ç»å®Œæˆäº†ä»â€œæ‰‹å†™åŸç†â€åˆ°â€œå·¥ä¸šå®æˆ˜â€çš„èœ•å˜ã€‚\n",
    "\n",
    "1.  âœ… **åŸç†**: å½»åº•ç†è§£äº† Transformer å†…éƒ¨çš„ç»´åº¦å˜åŒ– `[Batch, Seq, 768]`ã€‚\n",
    "2.  âœ… **å·¥ç¨‹**: æŒæ¡äº† Tokenizer, Model, Datasets, DataLoader çš„å…¨å¥—æµæ°´çº¿ã€‚\n",
    "3.  âœ… **é¿å‘**: çŸ¥é“äº† BERT çš„åç¼©é—®é¢˜ã€é™æ€ Padding çš„æµªè´¹é—®é¢˜ã€Hugging Face çš„å®‰å…¨åŠ è½½é—®é¢˜ã€‚\n",
    "\n",
    "**ğŸ”œ ä¸‹å‘¨é¢„å‘Š (Stage 2 Week 7): æœ€ç»ˆå†³æˆ˜ â€”â€” å¾®è°ƒ (Fine-tuning)**\n",
    "\n",
    "ä¸‹å‘¨ï¼Œæˆ‘ä»¬å°†ä¸å†æ»¡è¶³äºä½¿ç”¨â€œé¢„è®­ç»ƒâ€çš„é€šç”¨ BERTã€‚\n",
    "\n",
    "  * æˆ‘ä»¬è¦è®­ç»ƒä¸€ä¸ª **æ‡‚å…·èº«æŒ‡ä»¤** çš„ä¸“å± BERTã€‚\n",
    "  * æˆ‘ä»¬è¦ä½¿ç”¨ **Hugging Face Trainer API**ã€‚\n",
    "  * æˆ‘ä»¬è¦çœ‹ç€ **Loss æ›²çº¿** ä¸‹é™ï¼Œäº²æ‰‹æ‰“é€ ä½ çš„ç¬¬ä¸€ä¸ª NLP è½åœ°é¡¹ç›®ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
