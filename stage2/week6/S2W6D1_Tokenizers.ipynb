{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e5818b",
   "metadata": {},
   "source": [
    "# S2W6D1: è¯­è¨€çš„åŸå­åŒ– (Tokenizers Deep Dive)\n",
    "\n",
    "åœ¨ Transformer çœ‹åˆ°ä»»ä½•æ•°å­—ä¹‹å‰ï¼Œå¿…é¡»æœ‰ä¸€ä¸ªâ€œç¿»è¯‘å®˜â€å°†äººç±»çš„è‡ªç„¶è¯­è¨€ï¼ˆæ–‡æœ¬ï¼‰è½¬æ¢ä¸ºæœºå™¨èƒ½ç†è§£çš„æ•°å­—ï¼ˆToken IDsï¼‰ã€‚è¿™ä¸ªç»„ä»¶å°±æ˜¯**Tokenizer**ã€‚\n",
    "\n",
    "## 1 æ ¸å¿ƒç†è®ºï¼šä»æ–‡æœ¬åˆ° ID\n",
    "\n",
    "### 1.1 ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨ ASCII ç ï¼Ÿ\n",
    "\n",
    "å¦‚æœåœ¨ C è¯­è¨€é‡Œï¼Œ'A' æ˜¯ 65ã€‚ä½†åœ¨ AI é‡Œï¼Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥æŠŠå¥å­å˜æˆ ASCII ç æµã€‚\n",
    "\n",
    "  * **åŸå› **: è¯­ä¹‰ç¨€ç–ã€‚'a', 'p', 'p', 'l', 'e' æ‹†å¼€çœ‹æ²¡æœ‰ä»»ä½•æ„ä¹‰ï¼Œåªæœ‰ç»„åˆåœ¨ä¸€èµ·æ‰æœ‰è¯­ä¹‰ã€‚\n",
    "  * **è§£å†³**: æˆ‘ä»¬éœ€è¦ä¸€ä¸ª**è¯è¡¨ (Vocabulary)**ï¼ŒæŠŠæœ‰æ„ä¹‰çš„å•å…ƒï¼ˆå­—ã€è¯ã€å­è¯ï¼‰æ˜ å°„ä¸ºä¸€ä¸ªå”¯ä¸€çš„ IDã€‚\n",
    "\n",
    "### 1.2 åˆ†è¯ç®—æ³•ä¹‹äº‰ (Interview Point)\n",
    "\n",
    "é¢è¯•å®˜ç»å¸¸é—®ï¼šâ€œBERT å’Œ GPT çš„åˆ†è¯æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿâ€\n",
    "\n",
    "  * **WordPiece (BERT ä½¿ç”¨)**:\n",
    "\n",
    "      * **ç­–ç•¥**: æ¦‚ç‡é©±åŠ¨ã€‚å®ƒä¼šæŠŠä¸è®¤è¯†çš„é•¿è¯æ‹†è§£ä¸ºå¸¸è§çš„â€œè¯æ ¹â€ã€‚\n",
    "      * **ä¾‹å­**: \"unhappiness\" -\\> \"un\" + \"\\#\\#happi\" + \"\\#\\#ness\" (æ³¨æ„ `##` è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªè¯çš„åç¼€ï¼Œä¸èƒ½å•ç‹¬æˆè¯)ã€‚\n",
    "      * **ä¸­æ–‡ç‰¹ä¾‹**: `bert-base-chinese` åŸºæœ¬ä¸Šæ˜¯ **å­— (Character)** çº§åˆ«çš„åˆ†è¯ã€‚æ¯”å¦‚ \"æœºå™¨äºº\" -\\> \"æœº\", \"å™¨\", \"äºº\"ã€‚\n",
    "\n",
    "  * **BPE (Byte-Pair Encoding, GPT/RoBERTa ä½¿ç”¨)**:\n",
    "\n",
    "      * **ç­–ç•¥**: ç»Ÿè®¡é¢‘æ¬¡é©±åŠ¨ã€‚ä¸æ–­åˆå¹¶å‡ºç°é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ã€‚\n",
    "      * **ç›®çš„**: éƒ½æ˜¯ä¸ºäº†è§£å†³ **OOV (Out Of Vocabulary)** é—®é¢˜ã€‚å¦‚æœè¯è¡¨é‡Œæ²¡æœ‰ \"iPhone16\"ï¼Œå°±æŠŠå®ƒæ‹†æˆ \"iPhone\" + \"16\"ï¼Œä¿è¯æ¨¡å‹èƒ½è¯»æ‡‚ã€‚\n",
    "\n",
    "### 1.3 ç‰¹æ®Š Token (The Special Ones)\n",
    "\n",
    "BERT çš„è¾“å…¥ä¸ä»…ä»…æ˜¯å­—ï¼Œè¿˜å¿…é¡»åŒ…å«â€œäº¤é€šä¿¡å·ç¯â€ï¼š\n",
    "\n",
    "  * `[CLS]` (ID: 101): **Classification Token**ã€‚æ”¾åœ¨å¥é¦–ã€‚\n",
    "      * **é¢è¯•å‘**: ä¸ºä»€ä¹ˆåˆ†ç±»ä»»åŠ¡åªç”¨è¿™ä¸ª Token çš„è¾“å‡ºï¼Ÿ(å› ä¸ºåœ¨ Pre-training æ—¶ï¼Œæ¨¡å‹è¢«è®­ç»ƒä¸ºå°†æ•´ä¸ªå¥å­çš„è¯­ä¹‰èšåˆåˆ°è¿™ä¸ªä½ç½®)ã€‚\n",
    "  * `[SEP]` (ID: 102): **Separator Token**ã€‚ç”¨äºå¥å°¾ï¼Œæˆ–è€…åŒºåˆ†ä¸¤ä¸ªå¥å­ï¼ˆQAä»»åŠ¡ä¸­çš„é—®é¢˜ä¸æ–‡ç« ï¼‰ã€‚\n",
    "  * `[PAD]` (ID: 0): **Padding Token**ã€‚ç”¨äºå ä½ï¼ŒæŠŠçŸ­å¥è¡¥é½é•¿åº¦ã€‚\n",
    "  * `[UNK]` (ID: 100): **Unknown Token**ã€‚å®åœ¨ä¸è®¤è¯†çš„ç”Ÿåƒ»å­—ï¼Œå°±å˜æˆè¿™ä¸ªï¼ˆä¿¡æ¯çš„æŸå¤±ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6464975",
   "metadata": {},
   "source": [
    "## 2\\. ä»£ç å®æˆ˜ (Hands-on Code)\n",
    "\n",
    "### 2.1 åŠ è½½ Tokenizer\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨æœ€ç»å…¸çš„ä¸­æ–‡ BERT æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5145d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goodminton/anaconda3/envs/utils/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ å‘ç°æœ¬åœ°æ¨¡å‹ï¼Œç›´æ¥åŠ è½½: /home/goodminton/study/AI-Interview-Sprint/data/pretrained_models/bert-base-chinese\n",
      "éªŒè¯åŠ è½½: 21128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com' # ä¸´æ—¶å¼€å¯é•œåƒ\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# --- é…ç½®è·¯å¾„ ---\n",
    "# è¿™ä¸€æ­¥å’Œä¹‹å‰ä¸€æ ·ï¼Œå®šä½åˆ° data/pretrained_models/bert-base-chinese\n",
    "current_dir = os.getcwd()\n",
    "# è¿™é‡Œçš„ target_path å°±æ˜¯æˆ‘ä»¬ä»¥åå­˜æ”¾â€œå¹²å‡€â€æ¨¡å‹çš„åœ°æ–¹\n",
    "target_path = os.path.abspath(os.path.join(current_dir, '../../data/pretrained_models/bert-base-chinese'))\n",
    "\n",
    "# --- æ ¸å¿ƒé€»è¾‘å˜åŒ– ---\n",
    "# 1. æ£€æŸ¥ç›®å½•ä¸‹æ˜¯å¦æœ‰ vocab.txtã€‚å¦‚æœæœ‰ï¼Œè¯´æ˜å·²ç»ä¸‹è½½å¹¶ä¿å­˜å¥½äº†ï¼Œç›´æ¥åŠ è½½æœ¬åœ°ã€‚\n",
    "# 2. å¦‚æœæ²¡æœ‰ï¼Œè¯´æ˜æ˜¯ç¬¬ä¸€æ¬¡ï¼Œéœ€è¦è”ç½‘ä¸‹è½½ï¼Œç„¶åâ€œå¦å­˜ä¸ºâ€åˆ°è¿™ä¸ªç›®å½•ã€‚\n",
    "\n",
    "if not os.path.exists(os.path.join(target_path, 'vocab.txt')):\n",
    "    print(\"ğŸŒ æœ¬åœ°æœªå‘ç°æ¨¡å‹ï¼Œæ­£åœ¨ä»é•œåƒæºä¸‹è½½...\")\n",
    "    \n",
    "    # å…ˆä¸‹è½½åˆ°å†…å­˜/ä¸´æ—¶ç¼“å­˜\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    \n",
    "    # ã€å…³é”®ä¸€æ­¥ã€‘å°†å¹²å‡€çš„æ–‡ä»¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•\n",
    "    # è¿™ä¼šåœ¨ target_path ä¸‹ç”Ÿæˆ vocab.txt, tokenizer_config.json ç­‰æ ‡å‡†æ–‡ä»¶\n",
    "    tokenizer.save_pretrained(target_path)\n",
    "    print(f\"âœ… ä¸‹è½½å¹¶è½¬å­˜å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åœ¨: {target_path}\")\n",
    "else:\n",
    "    print(f\"ğŸ“‚ å‘ç°æœ¬åœ°æ¨¡å‹ï¼Œç›´æ¥åŠ è½½: {target_path}\")\n",
    "\n",
    "# --- ä»¥åæ‰€æœ‰çš„ä»£ç éƒ½ç”¨ä¸‹é¢è¿™ä¸€è¡ŒåŠ è½½ ---\n",
    "# æ³¨æ„ï¼šè¿™é‡Œä¼ è¿›å»çš„ä¸å†æ˜¯ 'bert-base-chinese' è¿™ä¸ªå­—ç¬¦ä¸²\n",
    "# è€Œæ˜¯ target_path (ç»å¯¹è·¯å¾„)\n",
    "tokenizer = BertTokenizer.from_pretrained(target_path)\n",
    "\n",
    "print(\"éªŒè¯åŠ è½½:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1e743",
   "metadata": {},
   "source": [
    "### 2.2 æ ¸å¿ƒæ“ä½œï¼šEncode\n",
    "\n",
    "æˆ‘ä»¬è¦çœ‹çœ‹ä¸€å¥è¯æ˜¯å¦‚ä½•å˜æˆæ•°å­—çš„ã€‚æˆ‘ä»¬éœ€è¦é‡ç‚¹è§‚å¯Ÿè¾“å‡ºçš„IDså¼€å¤´æ˜¯ä¸æ˜¯`101`ï¼ˆ`CLS`ï¼‰ï¼Ÿç»“å°¾æ˜¯ä¸æ˜¯`102`ï¼ˆ`[SEP]`ï¼‰ï¼Œä¸­é—´çš„æ•°å­—å°±æ˜¯çº¢è‰²æ–¹å—å¯¹åº”æ˜¯å­—å…¸ç´¢å¼•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca9ea2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: [101, 2828, 5273, 5682, 4638, 3175, 1779, 6853, 5314, 2769, 511, 119, 119, 102]\n",
      "Decoded: [CLS] æŠŠ çº¢ è‰² çš„ æ–¹ å— é€’ ç»™ æˆ‘ ã€‚.. [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"æŠŠçº¢è‰²çš„æ–¹å—é€’ç»™æˆ‘ã€‚..\"\n",
    "\n",
    "# æ–¹å¼ A: ç®€å• encode (åªè¿”å› IDs)\n",
    "# add_special_tokens=True ä¼šè‡ªåŠ¨åŠ ä¸Š [CLS] å’Œ [SEP]\n",
    "ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "print(\"IDs:\", ids)\n",
    "\n",
    "# æ–¹å¼ B: è¿˜åŸå›æ–‡æœ¬ (Decode)\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(\"Decoded:\", decoded_text)\n",
    "\n",
    "# è§‚å¯Ÿï¼š\n",
    "# 101 æ˜¯ [CLS]\n",
    "# 102 æ˜¯ [SEP]\n",
    "# 511 (çº¢) ç­‰ç­‰..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773eebee",
   "metadata": {},
   "source": [
    "### 2.3 æ ¸å¿ƒæ“ä½œï¼šTokenizer Call (å·¥ä¸šç•Œå†™æ³•)\n",
    "\n",
    "åœ¨å®é™…å·¥ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šç›´æ¥è°ƒç”¨ `tokenizer()` å¯¹è±¡ï¼Œå› ä¸ºå®ƒä¼šä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰éœ€è¦çš„ Tensorã€‚\n",
    "\n",
    "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦é‡ç‚¹è§‚å¯Ÿ`Input IDs`çš„ç¬¬ä¸€è¡Œï¼ˆçŸ­å¥ï¼‰ï¼Œåé¢æ˜¯ä¸æ˜¯è¡¥äº†å‡ ä¸ª`0`ï¼Ÿè¿™å°±æ˜¯Paddingï¼›`Attention Mask`ï¼Œå¯¹åº”è¡¥`0`çš„ä½ç½®ï¼ŒMaskæ˜¯ä¸æ˜¯ä¹Ÿæ˜¯`0`ã€‚\n",
    "\n",
    "å¦‚æœä¸è¡¥`0`ï¼Œè¿™ä¸¤å¥è¯é•¿åº¦ä¸ä¸€æ ·ï¼Œèƒ½å¡è¿›åŒä¸€ä¸ªçŸ©é˜µï¼ˆTensorï¼‰å—ï¼Ÿ\n",
    "**ç­”æ¡ˆ**ï¼šä¸èƒ½ï¼ŒTensorå¿…é¡»å½¢çŠ¶è§„æ•´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63525bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keys inside inputs: KeysView({'input_ids': tensor([[ 101, 1403, 2340, 3181, 6760, 8114, 2428,  511,  102,    0,    0],\n",
      "        [ 101, 2831, 1357, 3466, 3844, 1168, 4638, 4680, 3403,  511,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])})\n",
      "\n",
      "--- Input IDs (è¿™æ˜¯ç»™ Embedding å±‚çš„è¾“å…¥) ---\n",
      "tensor([[ 101, 1403, 2340, 3181, 6760, 8114, 2428,  511,  102,    0,    0],\n",
      "        [ 101, 2831, 1357, 3466, 3844, 1168, 4638, 4680, 3403,  511,  102]])\n",
      "\n",
      "--- Attention Mask (è¿™æ˜¯ç»™ Self-Attention çš„ Mask) ---\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡æ‹Ÿä¸€ä¸ª Batch çš„è¾“å…¥ (å…·èº«åœºæ™¯æŒ‡ä»¤)\n",
    "instructions = [\n",
    "    # [\"å‘å·¦æ—‹è½¬30åº¦ã€‚\", \"æŠ“å–ç›®æ ‡ã€‚\"],\n",
    "    # [\"ç„¶åå‰è¿›1ç±³ã€‚\", \"æ”¾åˆ°ç¯®å­é‡Œã€‚\"]\n",
    "    \"å‘å·¦æ—‹è½¬30åº¦ã€‚\",\n",
    "    \"æŠ“å–æ£€æµ‹åˆ°çš„ç›®æ ‡ã€‚\"\n",
    "]\n",
    "\n",
    "# padding=True: æŒ‰ç…§batchä¸­æœ€é•¿çš„å¥å­è¡¥é½\n",
    "# return_tensors='pt': ç›´æ¥è¿”å›PyTorch Tensor\n",
    "inputs = tokenizer(\n",
    "    instructions,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=20,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(\"\\nKeys inside inputs:\", inputs.keys())\n",
    "# è¾“å‡ºåº”è¯¥æœ‰: input_ids, token_type_ids, attention_mask\n",
    "\n",
    "print(\"\\n--- Input IDs (è¿™æ˜¯ç»™ Embedding å±‚çš„è¾“å…¥) ---\")\n",
    "print(inputs['input_ids'])\n",
    "# è§‚å¯Ÿ 0 çš„å­˜åœ¨ï¼Œé‚£å°±æ˜¯ [PAD]\n",
    "\n",
    "print(\"\\n--- Attention Mask (è¿™æ˜¯ç»™ Self-Attention çš„ Mask) ---\")\n",
    "print(inputs['attention_mask'])\n",
    "# è§‚å¯Ÿ: 1 ä»£è¡¨çœŸå®å†…å®¹ï¼Œ0 ä»£è¡¨ paddingã€‚\n",
    "# è®°ä½ Week 5 æˆ‘ä»¬æ‰‹å†™çš„ mask å—ï¼Ÿè¿™å°±æ˜¯æ¥æºï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e9a83",
   "metadata": {},
   "source": [
    "## 3 æ·±åº¦è§£æä¸é¢è¯•å‘ (Deep Dive)\n",
    "\n",
    "### 3.1 è¯è¡¨å¤§å°ä¸ºä»€ä¹ˆæ˜¯21128ï¼Ÿ\n",
    "\n",
    "- **é¢è¯•é¢˜**ï¼š`bert-base-chinese`çš„21128åŒ…å«äº†æ‰€æœ‰æ±‰å­—å—ï¼Ÿ\n",
    "- **ç­”æ¡ˆ**ï¼šæ²¡æœ‰ã€‚å®ƒåŒ…å«äº†æœ€å¸¸ç”¨çš„æ±‰å­—ã€è‹±æ–‡å­—æ¯ã€æ•°å­—å’Œæ ‡ç‚¹ã€‚å¦‚æœè¾“å…¥éå¸¸ç”Ÿåƒ»çš„å¤æ–‡ä¼šè¢«è½¬ä¸º`[UNK]`ã€‚\n",
    "- **å¯¹æ¯”**: è‹±æ–‡ BERT çš„ vocab size æ˜¯ 30522ã€‚\n",
    "\n",
    "### 3.2 Attention Maskçš„ä½œç”¨\n",
    "\n",
    "åœ¨Week 5æ‰‹å†™Attentionæ—¶ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ª`mask`å‚æ•°ï¼Œç”¨æ¥æŠŠæŸäº›ä½ç½®å¡«æˆ`-inf`ã€‚è¿™é‡Œè¾“å‡ºçš„`0`ï¼Œåœ¨ä¼ å…¥æ¨¡å‹å†…éƒ¨åï¼Œä¼šè‡ªåŠ¨å˜æˆ`-inf`ï¼Œä»è€Œè®©Softmaxåçš„æ¦‚ç‡ä¸º0ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨å‘Šè¯‰ Attention æœºåˆ¶ï¼Œâ€œä¸è¦æŠŠæ³¨æ„åŠ›æµªè´¹åœ¨å ä½ç¬¦ `[PAD]` ä¸Šâ€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2002c",
   "metadata": {},
   "source": [
    "## 4 ğŸ¤– å…·èº«æ™ºèƒ½ç‰¹ä¾›ï¼šé¢†åŸŸæ‰©å…… (Adding Tokens)\n",
    "\n",
    "ä½œä¸ºæœºå™¨äººé¡¹ç›®è´Ÿè´£äººï¼Œä½ è‚¯å®šä¼šé‡åˆ°æ ‡å‡†è¯è¡¨é‡Œæ²¡æœ‰çš„ä¸“ä¸šæœ¯è¯­ã€‚ä¾‹å¦‚æœºå™¨äººæœ‰ä¸€ä¸ªç‰¹å®šçš„åŠ¨ä½œå« **\"Servo\\_Reset\\_Z\"**ã€‚å¦‚æœä¸åšå¤„ç†ï¼ŒBERT ä¼šæŠŠå®ƒæ‹†æˆï¼š`Servo`, `_`, `Re`, `##set`, `_`, `Z`ï¼ˆç”šè‡³æ›´ç¢ï¼‰ï¼Œè¿™ä¼šç ´åè¯­ä¹‰çš„å®Œæ•´æ€§ã€‚\n",
    "\n",
    "**è§£å†³æ–¹æ¡ˆï¼š`add_tokens`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c972ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡è®¾è¿™æ˜¯æˆ‘ä»¬çš„ç‰¹æ®ŠæŒ‡ä»¤\n",
    "robot_cmd = \"æ‰§è¡ŒæŒ‡ä»¤ï¼šServo_Reset_Z åŠ¨ä½œã€‚\"\n",
    "\n",
    "print(\"åŸå§‹åˆ†è¯ç»“æœ:\", tokenizer.tokenize(robot_cmd))\n",
    "# å¯èƒ½ä¼šè¢«åˆ‡å¾—å¾ˆç¢ï¼Œæˆ–è€…æŠŠ Servo è¿™ç§è‹±æ–‡åˆ‡æˆå­—æ¯\n",
    "\n",
    "# --- æ ¸å¿ƒæ“ä½œï¼šæ·»åŠ æ–°è¯ ---\n",
    "new_tokens = [\"Servo_Reset_Z\", \"Joint_01\"]\n",
    "num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "print(f\"æ·»åŠ äº† {num_added_toks} ä¸ªæ–° Token\")\n",
    "\n",
    "# å†æ¬¡åˆ†è¯\n",
    "print(\"æ–°åˆ†è¯ç»“æœ:\", tokenizer.tokenize(robot_cmd))\n",
    "# ç°åœ¨ 'Servo_Reset_Z' åº”è¯¥ä½œä¸ºä¸€ä¸ªæ•´ä½“å­˜åœ¨äº†ï¼\n",
    "\n",
    "ids = tokenizer.encode(robot_cmd)\n",
    "print(\"æ–° IDs:\", ids)\n",
    "# æ³¨æ„ï¼šæ–° Token çš„ ID ä¼šæ’åœ¨ 21128 ä¹‹å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790d2a8",
   "metadata": {},
   "source": [
    "âš ï¸ **é«˜å±è­¦å‘Š (Critical Warning)**:\n",
    "ä½ ç°åœ¨åªæ˜¯ä¿®æ”¹äº† Tokenizerï¼ˆå­—å…¸ï¼‰ã€‚æ¨¡å‹ï¼ˆå¤§è„‘ï¼‰çš„ Embedding çŸ©é˜µå¤§å°è¿˜æ˜¯æ—§çš„ï¼ˆ21128è¡Œï¼‰ã€‚\n",
    "å¦‚æœä½ ç›´æ¥æŠŠè¿™ä¸ªæ–° ID ä¼ ç»™æ¨¡å‹ï¼Œ**æ¨¡å‹ä¼šæŠ¥é”™**ï¼ˆç´¢å¼•è¶Šç•Œï¼‰ã€‚\n",
    "\n",
    "**è§£å†³æ–¹æ³•**ï¼šåœ¨åŠ è½½æ¨¡å‹åï¼Œå¿…é¡»è°ƒç”¨ `model.resize_token_embeddings(len(tokenizer))`ã€‚è¿™æˆ‘ä»¬åœ¨ Day 3 ä¼šè¯¦ç»†è®²ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c59fd9",
   "metadata": {},
   "source": [
    "## 5 ğŸš€ ä»Šæ—¥æ€»ç»“ä¸ä»»åŠ¡\n",
    "\n",
    "### ğŸ“ æ€»ç»“\n",
    "\n",
    "1.  **Tokenizer** æ˜¯æ–‡æœ¬å’Œæ•°å­¦çš„æ¡¥æ¢ã€‚\n",
    "2.  **Input IDs** æ˜¯å­—çš„ç´¢å¼•ï¼Œ**Attention Mask** å†³å®šäº†æ¨¡å‹çœ‹å“ªé‡Œã€‚\n",
    "3.  **Special Tokens** (`[CLS]`, `[SEP]`) æ˜¯ BERT æ¶æ„çš„é”šç‚¹ã€‚\n",
    "4.  **Add Tokens** æ˜¯å‚ç›´é¢†åŸŸï¼ˆåŒ»ç–—ã€æ³•å¾‹ã€æœºå™¨äººï¼‰å¾®è°ƒçš„å¿…ç»ä¹‹è·¯ã€‚\n",
    "\n",
    "### âœ… ä»»åŠ¡ (Assignment)\n",
    "\n",
    "1.  åœ¨ Notebook ä¸­è¿è¡Œä¸Šè¿°ä»£ç ã€‚\n",
    "2.  **åšä¸ªå®éªŒ**: è¾“å…¥ä¸€ä¸ªå¾ˆé•¿çš„å¥å­ï¼ˆè¶…è¿‡ 20 å­—ï¼‰ï¼Œè§‚å¯Ÿ `truncation=True` å’Œ `max_length=20` çš„æ•ˆæœï¼Œè¢«æˆªæ–­çš„æ˜¯å‰é¢è¿˜æ˜¯åé¢ï¼Ÿ\n",
    "3.  **æ€è€ƒ**: `token_type_ids` å…¨æ˜¯ 0ï¼Œè¿™æ˜¯å¹²å˜›çš„ï¼Ÿï¼ˆæç¤ºï¼šBERT é¢„è®­ç»ƒæœ‰ä¸€ä¸ª Next Sentence Prediction ä»»åŠ¡ï¼Œå¦‚æœæ˜¯ä¸¤ä¸ªå¥å­è¾“å…¥ï¼Œç¬¬ä¸€å¥æ˜¯0ï¼Œç¬¬äºŒå¥æ˜¯1ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e9142",
   "metadata": {},
   "source": [
    "## âš”ï¸ Day 1 æ¯æ—¥ç®—æ³•é¢˜ (LeetCode Daily)\n",
    "\n",
    "ä½œä¸º AI ç®—æ³•å·¥ç¨‹å¸ˆï¼Œæˆ‘ä»¬çš„ç®—æ³•é¢˜ä¸ä»…è¦åˆ·ï¼Œè¿˜è¦åˆ·å¾—**æœ‰èƒŒæ™¯**ã€‚\n",
    "ä»Šå¤©çš„é¢˜ç›®ä¸æˆ‘ä»¬åˆšå­¦çš„ **Tokenizer** åŸç†é«˜åº¦ç›¸å…³ã€‚\n",
    "\n",
    "### ğŸ¯ é¢˜ç›®: LeetCode 139. å•è¯æ‹†åˆ† (Word Break)\n",
    "\n",
    "* **éš¾åº¦**: Medium\n",
    "* **é“¾æ¥**: [LeetCode 139](../../LeetCode%20practice/101-150.ipynb)\n",
    "* **é¢˜ç›®æè¿°**: ç»™ä½ ä¸€ä¸ªå­—ç¬¦ä¸² `s` å’Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ `wordDict` ä½œä¸ºå­—å…¸ã€‚è¯·ä½ åˆ¤æ–­ `s` æ˜¯å¦å¯ä»¥è¢«ç©ºæ ¼æ‹†åˆ†ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªåœ¨å­—å…¸ä¸­å‡ºç°çš„å•è¯ã€‚\n",
    "\n",
    "### ğŸ§  ä¸ºä»€ä¹ˆåšè¿™é“é¢˜ï¼Ÿ(The Connection)\n",
    "\n",
    "è¿™é“é¢˜æœ¬è´¨ä¸Šå°±æ˜¯ **Tokenization (åˆ†è¯)** çš„æç®€æ¨¡æ‹Ÿï¼\n",
    "\n",
    "1.  **åœºæ™¯æ˜ å°„**:\n",
    "    * `s`: ç›¸å½“äºè¾“å…¥çš„ä¸€å¥è¯ï¼ˆæ²¡æœ‰ç©ºæ ¼ï¼Œæ¯”å¦‚ä¸­æ–‡æˆ–è¢«å»æ‰ç©ºæ ¼çš„è‹±æ–‡ï¼‰ã€‚\n",
    "    * `wordDict`: ç›¸å½“äº BERT çš„ `vocab.txt` (è¯è¡¨)ã€‚\n",
    "    * **ç›®æ ‡**: åˆ¤æ–­è¿™å¥è¯èƒ½ä¸èƒ½å®Œå…¨ç”¨è¯è¡¨é‡Œçš„ Token æ‹¼å‡ºæ¥ï¼ˆä¸äº§ç”Ÿ `[UNK]`ï¼‰ã€‚\n",
    "2.  **å·¥ç¨‹å¯ç¤º**:\n",
    "    * å®é™…çš„ Tokenizerï¼ˆå¦‚ WordPieceï¼‰ä½¿ç”¨çš„æ˜¯ **è´ªå¿ƒç®—æ³• (Greedy)** æˆ– **æœ€å¤§åŒ¹é…**ã€‚\n",
    "    * ä½†è¿™é“é¢˜è¦æ±‚åˆ¤æ–­â€œæ˜¯å¦å­˜åœ¨å¯è¡Œè§£â€ï¼Œæ ‡å‡†è§£æ³•æ˜¯ **åŠ¨æ€è§„åˆ’ (DP)**ã€‚è¿™èƒ½è®­ç»ƒä½ å¯¹â€œçŠ¶æ€è½¬ç§»â€çš„æ•æ„Ÿåº¦â€”â€”è¿™åœ¨ç†è§£ Transformer Decoder ç”Ÿæˆè¿‡ç¨‹ï¼ˆBeam Searchï¼‰æ—¶ä¹Ÿä¼šç”¨åˆ°ã€‚\n",
    "\n",
    "### ğŸ’¡ å¯¼å¸ˆæç¤º (Coach's Hint)\n",
    "\n",
    "* ä¸è¦ç”¨é€’å½’å›æº¯ï¼Œä¼šè¶…æ—¶ (Time Limit Exceeded)ã€‚\n",
    "* **DP å®šä¹‰**: `dp[i]` è¡¨ç¤ºå­—ç¬¦ä¸²çš„å‰ `i` ä¸ªå­—ç¬¦ `s[0...i-1]` æ˜¯å¦èƒ½è¢«æˆåŠŸæ‹†åˆ†ã€‚\n",
    "* **è½¬ç§»æ–¹ç¨‹**: `dp[i]` ä¸º `True` çš„æ¡ä»¶æ˜¯ï¼šå­˜åœ¨ä¸€ä¸ª `j < i`ï¼Œä½¿å¾— `dp[j]` ä¸º `True` **ä¸”** `s[j:i]` åœ¨å­—å…¸ä¸­ã€‚\n",
    "    * ç¿»è¯‘æˆäººè¯ï¼š*â€œå¦‚æœå‰é¢ä¸€æ®µèƒ½æ‹¼å¥½ï¼Œä¸”å‰©ä¸‹è¿™ä¸€æˆªä¹Ÿåœ¨å­—å…¸é‡Œï¼Œé‚£æ•´æ®µå°±èƒ½æ‹¼å¥½ã€‚â€*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
