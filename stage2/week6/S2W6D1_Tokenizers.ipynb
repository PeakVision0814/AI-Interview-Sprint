{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e5818b",
   "metadata": {},
   "source": [
    "# S2W6D1: è¯­è¨€çš„åŸå­åŒ– (Tokenizers Deep Dive)\n",
    "\n",
    "åœ¨ Transformer çœ‹åˆ°ä»»ä½•æ•°å­—ä¹‹å‰ï¼Œå¿…é¡»æœ‰ä¸€ä¸ªâ€œç¿»è¯‘å®˜â€å°†äººç±»çš„è‡ªç„¶è¯­è¨€ï¼ˆæ–‡æœ¬ï¼‰è½¬æ¢ä¸ºæœºå™¨èƒ½ç†è§£çš„æ•°å­—ï¼ˆToken IDsï¼‰ã€‚è¿™ä¸ªç»„ä»¶å°±æ˜¯**Tokenizer**ã€‚\n",
    "\n",
    "## 1 æ ¸å¿ƒç†è®ºï¼šä»æ–‡æœ¬åˆ° ID\n",
    "\n",
    "### 1.1 ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨ ASCII ç ï¼Ÿ\n",
    "\n",
    "å¦‚æœåœ¨ C è¯­è¨€é‡Œï¼Œ'A' æ˜¯ 65ã€‚ä½†åœ¨ AI é‡Œï¼Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥æŠŠå¥å­å˜æˆ ASCII ç æµã€‚\n",
    "\n",
    "  * **åŸå› **: è¯­ä¹‰ç¨€ç–ã€‚'a', 'p', 'p', 'l', 'e' æ‹†å¼€çœ‹æ²¡æœ‰ä»»ä½•æ„ä¹‰ï¼Œåªæœ‰ç»„åˆåœ¨ä¸€èµ·æ‰æœ‰è¯­ä¹‰ã€‚\n",
    "  * **è§£å†³**: æˆ‘ä»¬éœ€è¦ä¸€ä¸ª**è¯è¡¨ (Vocabulary)**ï¼ŒæŠŠæœ‰æ„ä¹‰çš„å•å…ƒï¼ˆå­—ã€è¯ã€å­è¯ï¼‰æ˜ å°„ä¸ºä¸€ä¸ªå”¯ä¸€çš„ IDã€‚\n",
    "\n",
    "### 1.2 åˆ†è¯ç®—æ³•ä¹‹äº‰ (Interview Point)\n",
    "\n",
    "é¢è¯•å®˜ç»å¸¸é—®ï¼šâ€œBERT å’Œ GPT çš„åˆ†è¯æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿâ€\n",
    "\n",
    "  * **WordPiece (BERT ä½¿ç”¨)**:\n",
    "\n",
    "      * **ç­–ç•¥**: æ¦‚ç‡é©±åŠ¨ã€‚å®ƒä¼šæŠŠä¸è®¤è¯†çš„é•¿è¯æ‹†è§£ä¸ºå¸¸è§çš„â€œè¯æ ¹â€ã€‚\n",
    "      * **ä¾‹å­**: \"unhappiness\" -\\> \"un\" + \"\\#\\#happi\" + \"\\#\\#ness\" (æ³¨æ„ `##` è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªè¯çš„åç¼€ï¼Œä¸èƒ½å•ç‹¬æˆè¯)ã€‚\n",
    "      * **ä¸­æ–‡ç‰¹ä¾‹**: `bert-base-chinese` åŸºæœ¬ä¸Šæ˜¯ **å­— (Character)** çº§åˆ«çš„åˆ†è¯ã€‚æ¯”å¦‚ \"æœºå™¨äºº\" -\\> \"æœº\", \"å™¨\", \"äºº\"ã€‚\n",
    "\n",
    "  * **BPE (Byte-Pair Encoding, GPT/RoBERTa ä½¿ç”¨)**:\n",
    "\n",
    "      * **ç­–ç•¥**: ç»Ÿè®¡é¢‘æ¬¡é©±åŠ¨ã€‚ä¸æ–­åˆå¹¶å‡ºç°é¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹ã€‚\n",
    "      * **ç›®çš„**: éƒ½æ˜¯ä¸ºäº†è§£å†³ **OOV (Out Of Vocabulary)** é—®é¢˜ã€‚å¦‚æœè¯è¡¨é‡Œæ²¡æœ‰ \"iPhone16\"ï¼Œå°±æŠŠå®ƒæ‹†æˆ \"iPhone\" + \"16\"ï¼Œä¿è¯æ¨¡å‹èƒ½è¯»æ‡‚ã€‚\n",
    "\n",
    "### 1.3 ç‰¹æ®Š Token (The Special Ones)\n",
    "\n",
    "BERT çš„è¾“å…¥ä¸ä»…ä»…æ˜¯å­—ï¼Œè¿˜å¿…é¡»åŒ…å«â€œäº¤é€šä¿¡å·ç¯â€ï¼š\n",
    "\n",
    "  * `[CLS]` (ID: 101): **Classification Token**ã€‚æ”¾åœ¨å¥é¦–ã€‚\n",
    "      * **é¢è¯•å‘**: ä¸ºä»€ä¹ˆåˆ†ç±»ä»»åŠ¡åªç”¨è¿™ä¸ª Token çš„è¾“å‡ºï¼Ÿ(å› ä¸ºåœ¨ Pre-training æ—¶ï¼Œæ¨¡å‹è¢«è®­ç»ƒä¸ºå°†æ•´ä¸ªå¥å­çš„è¯­ä¹‰èšåˆåˆ°è¿™ä¸ªä½ç½®)ã€‚\n",
    "  * `[SEP]` (ID: 102): **Separator Token**ã€‚ç”¨äºå¥å°¾ï¼Œæˆ–è€…åŒºåˆ†ä¸¤ä¸ªå¥å­ï¼ˆQAä»»åŠ¡ä¸­çš„é—®é¢˜ä¸æ–‡ç« ï¼‰ã€‚\n",
    "  * `[PAD]` (ID: 0): **Padding Token**ã€‚ç”¨äºå ä½ï¼ŒæŠŠçŸ­å¥è¡¥é½é•¿åº¦ã€‚\n",
    "  * `[UNK]` (ID: 100): **Unknown Token**ã€‚å®åœ¨ä¸è®¤è¯†çš„ç”Ÿåƒ»å­—ï¼Œå°±å˜æˆè¿™ä¸ªï¼ˆä¿¡æ¯çš„æŸå¤±ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6464975",
   "metadata": {},
   "source": [
    "## 2\\. ä»£ç å®æˆ˜ (Hands-on Code)\n",
    "\n",
    "### 2.1 åŠ è½½ Tokenizer\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨æœ€ç»å…¸çš„ä¸­æ–‡ BERT æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5145d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goodminton/anaconda3/envs/utils/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ æœ¬åœ°æœªå‘ç°æ¨¡å‹ï¼Œæ­£åœ¨ä»é•œåƒæºä¸‹è½½...\n",
      "âœ… ä¸‹è½½å¹¶è½¬å­˜å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åœ¨: /home/goodminton/study/AI-Interview-Sprint/data/pretrained_models/bert-base-chinese\n",
      "éªŒè¯åŠ è½½: 21128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com' # ä¸´æ—¶å¼€å¯é•œåƒ\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# --- é…ç½®è·¯å¾„ ---\n",
    "# è¿™ä¸€æ­¥å’Œä¹‹å‰ä¸€æ ·ï¼Œå®šä½åˆ° data/pretrained_models/bert-base-chinese\n",
    "current_dir = os.getcwd()\n",
    "# è¿™é‡Œçš„ target_path å°±æ˜¯æˆ‘ä»¬ä»¥åå­˜æ”¾â€œå¹²å‡€â€æ¨¡å‹çš„åœ°æ–¹\n",
    "target_path = os.path.abspath(os.path.join(current_dir, '../../data/pretrained_models/bert-base-chinese'))\n",
    "\n",
    "# --- æ ¸å¿ƒé€»è¾‘å˜åŒ– ---\n",
    "# 1. æ£€æŸ¥ç›®å½•ä¸‹æ˜¯å¦æœ‰ vocab.txtã€‚å¦‚æœæœ‰ï¼Œè¯´æ˜å·²ç»ä¸‹è½½å¹¶ä¿å­˜å¥½äº†ï¼Œç›´æ¥åŠ è½½æœ¬åœ°ã€‚\n",
    "# 2. å¦‚æœæ²¡æœ‰ï¼Œè¯´æ˜æ˜¯ç¬¬ä¸€æ¬¡ï¼Œéœ€è¦è”ç½‘ä¸‹è½½ï¼Œç„¶åâ€œå¦å­˜ä¸ºâ€åˆ°è¿™ä¸ªç›®å½•ã€‚\n",
    "\n",
    "if not os.path.exists(os.path.join(target_path, 'vocab.txt')):\n",
    "    print(\"ğŸŒ æœ¬åœ°æœªå‘ç°æ¨¡å‹ï¼Œæ­£åœ¨ä»é•œåƒæºä¸‹è½½...\")\n",
    "    \n",
    "    # å…ˆä¸‹è½½åˆ°å†…å­˜/ä¸´æ—¶ç¼“å­˜\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    \n",
    "    # ã€å…³é”®ä¸€æ­¥ã€‘å°†å¹²å‡€çš„æ–‡ä»¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•\n",
    "    # è¿™ä¼šåœ¨ target_path ä¸‹ç”Ÿæˆ vocab.txt, tokenizer_config.json ç­‰æ ‡å‡†æ–‡ä»¶\n",
    "    tokenizer.save_pretrained(target_path)\n",
    "    print(f\"âœ… ä¸‹è½½å¹¶è½¬å­˜å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åœ¨: {target_path}\")\n",
    "else:\n",
    "    print(f\"ğŸ“‚ å‘ç°æœ¬åœ°æ¨¡å‹ï¼Œç›´æ¥åŠ è½½: {target_path}\")\n",
    "\n",
    "# --- ä»¥åæ‰€æœ‰çš„ä»£ç éƒ½ç”¨ä¸‹é¢è¿™ä¸€è¡ŒåŠ è½½ ---\n",
    "# æ³¨æ„ï¼šè¿™é‡Œä¼ è¿›å»çš„ä¸å†æ˜¯ 'bert-base-chinese' è¿™ä¸ªå­—ç¬¦ä¸²\n",
    "# è€Œæ˜¯ target_path (ç»å¯¹è·¯å¾„)\n",
    "tokenizer = BertTokenizer.from_pretrained(target_path)\n",
    "\n",
    "print(\"éªŒè¯åŠ è½½:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1e743",
   "metadata": {},
   "source": [
    "### 2.2 æ ¸å¿ƒæ“ä½œï¼šEncode\n",
    "\n",
    "æˆ‘ä»¬è¦çœ‹çœ‹ä¸€å¥è¯æ˜¯å¦‚ä½•å˜æˆæ•°å­—çš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca9ea2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: [101, 2828, 5273, 5682, 4638, 3175, 1779, 6853, 5314, 2769, 511, 119, 119, 102]\n",
      "Decoded: [CLS] æŠŠ çº¢ è‰² çš„ æ–¹ å— é€’ ç»™ æˆ‘ ã€‚.. [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"æŠŠçº¢è‰²çš„æ–¹å—é€’ç»™æˆ‘ã€‚..\"\n",
    "\n",
    "# æ–¹å¼ A: ç®€å• encode (åªè¿”å› IDs)\n",
    "# add_special_tokens=True ä¼šè‡ªåŠ¨åŠ ä¸Š [CLS] å’Œ [SEP]\n",
    "ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "print(\"IDs:\", ids)\n",
    "\n",
    "# æ–¹å¼ B: è¿˜åŸå›æ–‡æœ¬ (Decode)\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(\"Decoded:\", decoded_text)\n",
    "\n",
    "# è§‚å¯Ÿï¼š\n",
    "# 101 æ˜¯ [CLS]\n",
    "# 102 æ˜¯ [SEP]\n",
    "# 511 (çº¢) ç­‰ç­‰..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c59fd9",
   "metadata": {},
   "source": [
    "### 2.3 æ ¸å¿ƒæ“ä½œï¼šTokenizer Call (å·¥ä¸šç•Œå†™æ³•)\n",
    "\n",
    "åœ¨å®é™…å·¥ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šç›´æ¥è°ƒç”¨ `tokenizer()` å¯¹è±¡ï¼Œå› ä¸ºå®ƒä¼šä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰éœ€è¦çš„ Tensorã€‚\n",
    "\n",
    "```python\n",
    "# æ¨¡æ‹Ÿä¸€ä¸ª Batch çš„è¾“å…¥ (å…·èº«åœºæ™¯æŒ‡ä»¤)\n",
    "instructions = [\n",
    "    \"å‘å·¦æ—‹è½¬30åº¦ã€‚\",\n",
    "    \"æŠ“å–æ£€æµ‹åˆ°çš„ç›®æ ‡ã€‚\"\n",
    "]\n",
    "\n",
    "# padding=True: æŒ‰ç…§batchä¸­æœ€é•¿çš„å¥å­è¡¥é½\n",
    "# return_tensors='pt': ç›´æ¥è¿”å› PyTorch Tensor\n",
    "inputs = tokenizer(\n",
    "    instructions, \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=20, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(\"\\nKeys inside inputs:\", inputs.keys())\n",
    "# è¾“å‡ºåº”è¯¥æœ‰: input_ids, token_type_ids, attention_mask\n",
    "\n",
    "print(\"\\n--- Input IDs (è¿™æ˜¯ç»™ Embedding å±‚çš„è¾“å…¥) ---\")\n",
    "print(inputs['input_ids'])\n",
    "# è§‚å¯Ÿ 0 çš„å­˜åœ¨ï¼Œé‚£å°±æ˜¯ [PAD]\n",
    "\n",
    "print(\"\\n--- Attention Mask (è¿™æ˜¯ç»™ Self-Attention çš„ Mask) ---\")\n",
    "print(inputs['attention_mask'])\n",
    "# è§‚å¯Ÿ: 1 ä»£è¡¨çœŸå®å†…å®¹ï¼Œ0 ä»£è¡¨ paddingã€‚\n",
    "# è®°ä½ Week 5 æˆ‘ä»¬æ‰‹å†™çš„ mask å—ï¼Ÿè¿™å°±æ˜¯æ¥æºï¼\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## 3\\. æ·±åº¦è§£æä¸é¢è¯•å‘ (Deep Dive)\n",
    "\n",
    "### 3.1 è¯è¡¨å¤§å°ä¸ºä»€ä¹ˆæ˜¯ 21128ï¼Ÿ\n",
    "\n",
    "  * **é¢è¯•é¢˜**: `bert-base-chinese` çš„ 21128 åŒ…å«äº†æ‰€æœ‰æ±‰å­—å—ï¼Ÿ\n",
    "  * **ç­”æ¡ˆ**: æ²¡æœ‰ã€‚å®ƒåŒ…å«äº†æœ€å¸¸ç”¨çš„æ±‰å­—ã€è‹±æ–‡å­—æ¯ã€æ•°å­—å’Œæ ‡ç‚¹ã€‚å¦‚æœä½ è¾“å…¥éå¸¸ç”Ÿåƒ»çš„å¤æ–‡ï¼Œä¼šè¢«è½¬ä¸º `[UNK]`ã€‚\n",
    "  * **å¯¹æ¯”**: è‹±æ–‡ BERT çš„ vocab size æ˜¯ 30522ã€‚\n",
    "\n",
    "### 3.2 Attention Mask çš„ä½œç”¨\n",
    "\n",
    "è¯·ä»”ç»†çœ‹ä»£ç è¾“å‡ºçš„ `attention_mask`ã€‚\n",
    "\n",
    "  * åœ¨ Week 5 æ‰‹å†™ Attention æ—¶ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ª `mask` å‚æ•°ï¼Œç”¨æ¥æŠŠæŸäº›ä½ç½®å¡«æˆ `-inf`ã€‚\n",
    "  * è¿™é‡Œè¾“å‡ºçš„ `0`ï¼Œåœ¨ä¼ å…¥æ¨¡å‹å†…éƒ¨åï¼Œä¼šè‡ªåŠ¨å˜æˆ `-inf`ï¼Œä»è€Œè®© Softmax åçš„æ¦‚ç‡ä¸º 0ã€‚\n",
    "  * **ä¸€å¥è¯è§£é‡Š**: å‘Šè¯‰ Attention æœºåˆ¶ï¼Œâ€œä¸è¦æŠŠæ³¨æ„åŠ›æµªè´¹åœ¨å ä½ç¬¦ `[PAD]` ä¸Šâ€ã€‚\n",
    "\n",
    "-----\n",
    "\n",
    "## 4\\. ğŸ¤– å…·èº«æ™ºèƒ½ç‰¹ä¾›ï¼šé¢†åŸŸæ‰©å…… (Adding Tokens)\n",
    "\n",
    "ä½œä¸ºæœºå™¨äººé¡¹ç›®è´Ÿè´£äººï¼Œä½ è‚¯å®šä¼šé‡åˆ°æ ‡å‡†è¯è¡¨é‡Œæ²¡æœ‰çš„ä¸“ä¸šæœ¯è¯­ã€‚\n",
    "ä¾‹å¦‚ï¼šä½ çš„æœºå™¨äººæœ‰ä¸€ä¸ªç‰¹å®šçš„åŠ¨ä½œå« **\"Servo\\_Reset\\_Z\"**ã€‚\n",
    "\n",
    "å¦‚æœä¸åšå¤„ç†ï¼ŒBERT ä¼šæŠŠå®ƒæ‹†æˆï¼š`Servo`, `_`, `Re`, `##set`, `_`, `Z`ï¼ˆç”šè‡³æ›´ç¢ï¼‰ï¼Œè¿™ä¼šç ´åè¯­ä¹‰çš„å®Œæ•´æ€§ã€‚\n",
    "\n",
    "**è§£å†³æ–¹æ¡ˆï¼š`add_tokens`**\n",
    "\n",
    "```python\n",
    "# å‡è®¾è¿™æ˜¯æˆ‘ä»¬çš„ç‰¹æ®ŠæŒ‡ä»¤\n",
    "robot_cmd = \"æ‰§è¡ŒæŒ‡ä»¤ï¼šServo_Reset_Z åŠ¨ä½œã€‚\"\n",
    "\n",
    "print(\"åŸå§‹åˆ†è¯ç»“æœ:\", tokenizer.tokenize(robot_cmd))\n",
    "# å¯èƒ½ä¼šè¢«åˆ‡å¾—å¾ˆç¢ï¼Œæˆ–è€…æŠŠ Servo è¿™ç§è‹±æ–‡åˆ‡æˆå­—æ¯\n",
    "\n",
    "# --- æ ¸å¿ƒæ“ä½œï¼šæ·»åŠ æ–°è¯ ---\n",
    "new_tokens = [\"Servo_Reset_Z\", \"Joint_01\"]\n",
    "num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "print(f\"æ·»åŠ äº† {num_added_toks} ä¸ªæ–° Token\")\n",
    "\n",
    "# å†æ¬¡åˆ†è¯\n",
    "print(\"æ–°åˆ†è¯ç»“æœ:\", tokenizer.tokenize(robot_cmd))\n",
    "# ç°åœ¨ 'Servo_Reset_Z' åº”è¯¥ä½œä¸ºä¸€ä¸ªæ•´ä½“å­˜åœ¨äº†ï¼\n",
    "\n",
    "ids = tokenizer.encode(robot_cmd)\n",
    "print(\"æ–° IDs:\", ids)\n",
    "# æ³¨æ„ï¼šæ–° Token çš„ ID ä¼šæ’åœ¨ 21128 ä¹‹å\n",
    "```\n",
    "\n",
    "âš ï¸ **é«˜å±è­¦å‘Š (Critical Warning)**:\n",
    "ä½ ç°åœ¨åªæ˜¯ä¿®æ”¹äº† Tokenizerï¼ˆå­—å…¸ï¼‰ã€‚æ¨¡å‹ï¼ˆå¤§è„‘ï¼‰çš„ Embedding çŸ©é˜µå¤§å°è¿˜æ˜¯æ—§çš„ï¼ˆ21128è¡Œï¼‰ã€‚\n",
    "å¦‚æœä½ ç›´æ¥æŠŠè¿™ä¸ªæ–° ID ä¼ ç»™æ¨¡å‹ï¼Œ**æ¨¡å‹ä¼šæŠ¥é”™**ï¼ˆç´¢å¼•è¶Šç•Œï¼‰ã€‚\n",
    "**è§£å†³æ–¹æ³•**ï¼šåœ¨åŠ è½½æ¨¡å‹åï¼Œå¿…é¡»è°ƒç”¨ `model.resize_token_embeddings(len(tokenizer))`ã€‚è¿™æˆ‘ä»¬åœ¨ Day 3 ä¼šè¯¦ç»†è®²ã€‚\n",
    "\n",
    "-----\n",
    "\n",
    "## 5\\. ğŸš€ ä»Šæ—¥æ€»ç»“ä¸ä»»åŠ¡\n",
    "\n",
    "### ğŸ“ æ€»ç»“\n",
    "\n",
    "1.  **Tokenizer** æ˜¯æ–‡æœ¬å’Œæ•°å­¦çš„æ¡¥æ¢ã€‚\n",
    "2.  **Input IDs** æ˜¯å­—çš„ç´¢å¼•ï¼Œ**Attention Mask** å†³å®šäº†æ¨¡å‹çœ‹å“ªé‡Œã€‚\n",
    "3.  **Special Tokens** (`[CLS]`, `[SEP]`) æ˜¯ BERT æ¶æ„çš„é”šç‚¹ã€‚\n",
    "4.  **Add Tokens** æ˜¯å‚ç›´é¢†åŸŸï¼ˆåŒ»ç–—ã€æ³•å¾‹ã€æœºå™¨äººï¼‰å¾®è°ƒçš„å¿…ç»ä¹‹è·¯ã€‚\n",
    "\n",
    "### âœ… ä»»åŠ¡ (Assignment)\n",
    "\n",
    "1.  åœ¨ Notebook ä¸­è¿è¡Œä¸Šè¿°ä»£ç ã€‚\n",
    "2.  **åšä¸ªå®éªŒ**: è¾“å…¥ä¸€ä¸ªå¾ˆé•¿çš„å¥å­ï¼ˆè¶…è¿‡ 20 å­—ï¼‰ï¼Œè§‚å¯Ÿ `truncation=True` å’Œ `max_length=20` çš„æ•ˆæœï¼Œè¢«æˆªæ–­çš„æ˜¯å‰é¢è¿˜æ˜¯åé¢ï¼Ÿ\n",
    "3.  **æ€è€ƒ**: `token_type_ids` å…¨æ˜¯ 0ï¼Œè¿™æ˜¯å¹²å˜›çš„ï¼Ÿï¼ˆæç¤ºï¼šBERT é¢„è®­ç»ƒæœ‰ä¸€ä¸ª Next Sentence Prediction ä»»åŠ¡ï¼Œå¦‚æœæ˜¯ä¸¤ä¸ªå¥å­è¾“å…¥ï¼Œç¬¬ä¸€å¥æ˜¯0ï¼Œç¬¬äºŒå¥æ˜¯1ï¼‰ã€‚\n",
    "\n",
    "å®Œæˆå®éªŒåï¼Œè¯·å‘Šè¯‰æˆ‘ **â€œTokenizer å®éªŒå®Œæ¯•â€**ï¼Œæˆ‘ä»¬å°†åœ¨ Day 2 ç ”ç©¶å¦‚ä½•å¤„ç†å˜é•¿æ•°æ®çš„ Batching ç­–ç•¥ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
