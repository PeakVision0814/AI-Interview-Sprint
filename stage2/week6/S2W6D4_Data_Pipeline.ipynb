{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7575e78f",
   "metadata": {},
   "source": [
    "# S2W6D4ï¼šå·¥ä¸šçº§æ•°æ®æµæ°´çº¿ï¼ˆThe Data Pipelineï¼‰\n",
    "\n",
    "å‰å‡ å¤©æˆ‘ä»¬éƒ½æ˜¯åœ¨â€œæ‰‹å·¥ä½œåŠâ€é‡Œå·¥ä½œâ€”â€”æ‰‹åŠ¨å®šä¹‰ä¸€ä¸¤ä¸ªå­—ç¬¦ä¸²ï¼Œæ‰‹åŠ¨ Tokenizeï¼Œæ‰‹åŠ¨å–‚ç»™æ¨¡å‹ã€‚ä½†åœ¨çœŸæ­£çš„ç®—æ³•å²—ä½ï¼ˆæ— è®ºæ˜¯ NLP è¿˜æ˜¯å…·èº«æ™ºèƒ½ï¼‰ï¼Œæˆ‘ä»¬è¦å¤„ç†çš„æ•°æ®é‡é€šå¸¸æ˜¯ **ç™¾ä¸‡çº§** çš„ã€‚å¦‚æœè¿˜ç”¨ `for` å¾ªç¯ä¸€æ¡æ¡å¤„ç†ï¼Œä½ çš„è®­ç»ƒä»£ç å¯èƒ½è¦è·‘ä¸€ä¸‡å¹´ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face ç”Ÿæ€ä¸­æœ€é«˜æ•ˆçš„å·¥å…·åº“ â€”â€” **`datasets`**ã€‚\n",
    "\n",
    "**ğŸ“… æ—¥æœŸ**: Day 32\n",
    "**ğŸ¯ æ ¸å¿ƒç›®æ ‡**: æŒæ¡å¤„ç† **æµ·é‡æ•°æ®** çš„æ ‡å‡†èŒƒå¼ã€‚\n",
    "åœ¨ Day 1-3ï¼Œæˆ‘ä»¬æ˜¯â€œæ‰‹å·¥ä½œåŠâ€ï¼Œä¸€æ¬¡å¤„ç†ä¸€å¥è¯ã€‚\n",
    "åœ¨å·¥ä¸šç•Œï¼ˆæœç´¢/æ¨è/å¤§æ¨¡å‹è®­ç»ƒï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç† **TB çº§åˆ«** çš„æ•°æ®ã€‚å¦‚æœè¿˜ç”¨ `for` å¾ªç¯ + `Pandas`ï¼Œå†…å­˜æ—©å°±çˆ†äº†ã€‚ä»Šå¤©æˆ‘ä»¬è¦æŒæ¡ Hugging Face çš„æ ¸æ­¦å™¨ â€”â€” **`datasets` åº“** å’Œ **`DataCollator`**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a798d",
   "metadata": {},
   "source": [
    "## 1 ğŸ­ç†è®ºï¼šä¸ºä»€ä¹ˆè¦ç”¨`datasets`åº“ï¼Ÿ\n",
    "\n",
    "å¾ˆå¤šåˆå­¦è€…å–œæ¬¢ç”¨Pandas (`pd.read_csv`)ã€‚ä½†åœ¨AIè®­ç»ƒä¸­ï¼Œ`datasets`åº“æ‰æ˜¯æ ‡å‡†ç­”æ¡ˆã€‚\n",
    "\n",
    "- **å†…å­˜æ˜ å°„ï¼ˆMemory Mappingï¼‰**ï¼š\n",
    "    - **é—®é¢˜**ï¼šä½ æœ‰100GBçš„è¯­æ–™ï¼Œå†…å­˜åªæœ‰16GBï¼ŒPandasç›´æ¥çˆ†å†…å­˜ã€‚\n",
    "    - **è§£æ³•**ï¼š`datasets`ä½¿ç”¨Apache Arrowæ ¼å¼ï¼Œå®ƒæŠŠç¡¬ç›˜å½“å†…å­˜ç”¨ï¼Œè¯»å–1TBæ•°æ®å’Œ1MBæ•°æ®çš„åˆå§‹å¼€é”€å‡ ä¹ä¸€æ ·å¿«ã€‚\n",
    "- **Lazy Processing**ï¼š\n",
    "    - å®ƒä¸ä¼šä¸€æ¬¡æ€§æŠŠæ‰€æœ‰æ•°æ®éƒ½ç®—å®Œï¼Œè€Œæ˜¯ç­‰ä½ ç”¨åˆ°é‚£ä¸€æ¡æ—¶æ‰å»åŠ è½½ï¼ˆç»“åˆ Map æœºåˆ¶ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd5737",
   "metadata": {},
   "source": [
    "## 2\\. ğŸ’» ä»£ç å®æˆ˜ï¼šæ„å»ºæµæ°´çº¿\n",
    "\n",
    "### 2.1 å‡†å¤‡æ•°æ®é›†\n",
    "\n",
    "ä¸ºäº†æ¨¡æ‹ŸçœŸå®å·¥ä½œæµï¼Œæˆ‘ä»¬å…ˆåœ¨æœ¬åœ°ç”Ÿæˆä¸€ä¸ª CSV æ–‡ä»¶ï¼ˆæ¨¡æ‹Ÿä½ ä»å…¬å¸æ•°ä»“å¯¼å‡ºçš„æ•°æ®ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed84f2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡æ‹Ÿæ•°æ®å·²ç”Ÿæˆ: ../../data/raw_data/comments.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# 1. åˆ›å»ºæ¨¡æ‹Ÿæ•°æ® (é€šç”¨æ–‡æœ¬åˆ†ç±»åœºæ™¯ï¼šæ¯”å¦‚ç”µå•†è¯„è®ºæƒ…æ„Ÿåˆ†æ)\n",
    "data_dir = \"../../data/raw_data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "csv_path = os.path.join(data_dir, \"comments.csv\")\n",
    "\n",
    "# å†™å…¥ 5 æ¡é•¿çŸ­ä¸ä¸€çš„æ•°æ®\n",
    "# æ¨¡æ‹Ÿï¼šlabel=1 (å¥½è¯„), label=0 (å·®è¯„)\n",
    "data = [\n",
    "    [\"label\", \"text\"],\n",
    "    [1, \"è¿™ä¸ªç®—æ³•è¯¾ç¨‹çœŸçš„å¾ˆæ£’ï¼Œæ·±å…¥æµ…å‡ºï¼\"],\n",
    "    [0, \"ç¯å¢ƒé…ç½®å¤ªéº»çƒ¦äº†ï¼ŒæŠ¥é”™ä¸€å¤§å †ï¼Œä½“éªŒæå·®ã€‚\"],\n",
    "    [1, \"AIæ˜¯æœªæ¥çš„è¶‹åŠ¿ï¼Œå¿…é¡»æŒæ¡ã€‚\"],\n",
    "    [0, \"å¬ä¸æ‡‚ï¼Œé€€é’±ï¼\"],\n",
    "    [1, \"Data Pipelineçš„è®¾è®¡éå¸¸å·§å¦™ï¼Œè§£å†³äº†å†…å­˜æº¢å‡ºé—®é¢˜ã€‚\"]\n",
    "]\n",
    "\n",
    "with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"âœ… æ¨¡æ‹Ÿæ•°æ®å·²ç”Ÿæˆ: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852c044",
   "metadata": {},
   "source": [
    "### 2.2 åŠ è½½æ•°æ®ï¼ˆLoad Datasetï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515522bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goodminton/anaconda3/envs/utils/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 5 examples [00:00, 934.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# å·¥ä¸šç•Œæ ‡å‡†æ“ä½œï¼šä»æœ¬åœ°æ–‡ä»¶åŠ è½½\n",
    "# è¿™ä¸€æ­¥éå¸¸å¿«ï¼Œå› ä¸ºåªæ˜¯å»ºç«‹äº†æ˜ å°„ï¼Œæ²¡æœ‰çœŸæ­£è¯»å…¥å†…å­˜\n",
    "dataset = load_dataset(\"csv\", data_files=csv_path)\n",
    "\n",
    "print(dataset)\n",
    "# è¾“å‡º: DatasetDict({ train: Dataset({ features: ['label', 'text'], num_rows: 5 }) })\n",
    "# ä½ ä¼šå‘ç°å®ƒè‡ªåŠ¨æŠŠ CSV å˜æˆäº†ç±»ä¼¼å­—å…¸çš„ç»“æ„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda06f5d",
   "metadata": {},
   "source": [
    "### 2.3 æ ¸æ­¦å™¨ï¼š`map` å‡½æ•° (The Mapper)\n",
    "\n",
    "è¿™æ˜¯ä»Šå¤©**æœ€é‡è¦**çš„ä»£ç ã€‚æˆ‘ä»¬è¦æŠŠ Tokenizer åº”ç”¨åˆ°æ‰€æœ‰æ ·æœ¬ä¸Šã€‚åƒä¸‡ä¸è¦ç”¨ `for` å¾ªç¯ï¼è¦ç”¨ `.map()` å®ç°å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725e807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 543.42 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†åçš„åˆ—å: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# åŠ è½½æœ¬åœ° Tokenizer (å¤ç”¨ä¹‹å‰çš„è·¯å¾„)\n",
    "model_path = \"../../data/pretrained_models/bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# --- å®šä¹‰é¢„å¤„ç†å‡½æ•° ---\n",
    "def preprocess_function(examples):\n",
    "    # examples['text'] æ˜¯ä¸€ä¸ª Listï¼ŒTokenizer å¯ä»¥ç›´æ¥å¤„ç† List\n",
    "    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªåšæˆªæ–­ (Truncation)ï¼Œä¸åš Paddingï¼\n",
    "    # ä¸ºä»€ä¹ˆï¼Ÿ(é¢è¯•å‘ç‚¹ï¼Œè§ä¸‹æ–‡ DataCollator éƒ¨åˆ†)\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# --- ä¸€é”®æ˜ å°„ ---\n",
    "# batched=True: æ¯æ¬¡å¤„ç†ä¸€æ‰¹æ•°æ®ï¼ˆé»˜è®¤1000æ¡ï¼‰ï¼Œåˆ©ç”¨ tokenizer çš„å¹¶è¡Œèƒ½åŠ›\n",
    "# num_proc=1: å¦‚æœæ•°æ®é‡å¤§ï¼Œå¯ä»¥å¼€å¤šè¿›ç¨‹ (e.g., num_proc=4)\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(\"å¤„ç†åçš„åˆ—å:\", encoded_dataset['train'].column_names)\n",
    "# è¾“å‡ºåº”åŒ…å«: input_ids, token_type_ids, attention_mask, label, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a096431",
   "metadata": {},
   "source": [
    "### 2.4 åŠ¨æ€å¯¹é½ï¼šData Collator (The Dynamic Padder)\n",
    "\n",
    "**é¢è¯•å¿…è€ƒé¢˜**ï¼š*â€œä¸ºä»€ä¹ˆä¸åœ¨ `map` é˜¶æ®µç›´æ¥ `padding=True`ï¼Ÿâ€*\n",
    "\n",
    "  * **Static Padding (é™æ€å¯¹é½)**: å¦‚æœåœ¨ map é˜¶æ®µåšï¼Œæ‰€æœ‰å¥å­éƒ½ä¼šè¢«è¡¥é½åˆ° `max_length` (æ¯”å¦‚ 512)ã€‚å“ªæ€•ä¸€ä¸ª Batch é‡Œå…¨æ˜¯çŸ­å¥ï¼Œä¹Ÿè¦è¡¥åˆ° 512ã€‚**æµªè´¹æ˜¾å­˜å’Œè®¡ç®—æ—¶é—´**ã€‚\n",
    "  * **Dynamic Padding (åŠ¨æ€å¯¹é½)**: æˆ‘ä»¬æŠŠ Padding æ¨è¿Ÿåˆ° DataLoader å–å‡ºä¸€ä¸ª Batch çš„æ—¶å€™åšã€‚\n",
    "      * å¦‚æœè¿™ä¸ª Batch æœ€é•¿åªæœ‰ 10 ä¸ªå­—ï¼Œé‚£å°±å¤§å®¶éƒ½è¡¥åˆ° 10ã€‚\n",
    "      * **Data Collator** å°±æ˜¯åšè¿™ä¸ªè„æ´»ç´¯æ´»çš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afa049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ ---\n",
      "Batch Keys: KeysView({'labels': tensor([1, 0]), 'input_ids': tensor([[ 101,  100, 3221, 3313, 3341, 4638, 6633, 1232, 8024, 2553, 7557, 2958,\n",
      "         2995,  511,  102,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 4384, 1862, 6981, 5390, 1922, 7937, 4172,  749, 8024, 2845, 7231,\n",
      "          671, 1920, 1831, 8024,  860, 7741, 3353, 2345,  511,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])})\n",
      "Input IDs Shape: torch.Size([2, 22])\n",
      "Labels: tensor([1, 0])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. å®šä¹‰æ•´ç†å‘˜\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# --- ğŸ› ï¸ ä¿®å¤æ ¸å¿ƒï¼šæ”¹åä¸æ ¼å¼è®¾ç½® ---\n",
    "# 1. å…ˆæŠŠ 'label' æ”¹åä¸º 'labels' (å¤æ•°!)\n",
    "# è¿™æ˜¯ä¸ºäº†é…åˆ BERT æ¨¡å‹çš„ forward(labels=...) æ¥å£\n",
    "if 'label' in encoded_dataset['train'].column_names:\n",
    "    encoded_data00set = encoded_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# 2. ç§»é™¤ä¸éœ€è¦çš„ 'text' åˆ—\n",
    "final_dataset = encoded_dataset['train'].remove_columns([\"text\"])\n",
    "\n",
    "# 3. è®¾ç½®æ ¼å¼ä¸º PyTorch Tensor\n",
    "# DataCollator åªä¼šå¤„ç† Tensor ç±»å‹çš„åˆ—ï¼Œå¦‚æœè¿™é‡Œä¸è½¬ï¼Œlabel å¯èƒ½ä¼šè¢«ä¸¢å¼ƒ\n",
    "final_dataset.set_format(\"torch\")\n",
    "\n",
    "# 3. åˆ›å»º DataLoader\n",
    "dataloader = DataLoader(\n",
    "    final_dataset, \n",
    "    batch_size=2, \n",
    "    collate_fn=data_collator, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# --- éªŒè¯æµæ°´çº¿ ---\n",
    "print(\"\\n--- æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ ---\")\n",
    "for batch in dataloader:\n",
    "    print(\"Batch Keys:\", batch.keys()) # æ­¤æ—¶ä½ åº”è¯¥èƒ½çœ‹åˆ° 'labels'\n",
    "    print(\"Input IDs Shape:\", batch['input_ids'].shape)\n",
    "    \n",
    "    # --- ğŸ› ï¸ ä¿®å¤ç‚¹ï¼šè®¿é—® 'labels' (å¤æ•°) ---\n",
    "    print(\"Labels:\", batch['labels']) \n",
    "    print(\"-\" * 20)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1fde76",
   "metadata": {},
   "source": [
    "**é¢„æœŸç»“æœ**ï¼š\n",
    "ä½ ä¼šçœ‹åˆ° `Input IDs Shape` å¯èƒ½æ˜¯ `[2, 15]` æˆ–è€… `[2, 28]`ï¼ˆå–å†³äºè¿™ä¸ª Batch é‡Œæœ€é•¿çš„é‚£å¥æœ‰å¤šé•¿ï¼‰ï¼Œè€Œä¸æ˜¯å›ºå®šçš„ `[2, 512]`ã€‚**è¿™å°±æ˜¯æ•ˆç‡ï¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced47aa",
   "metadata": {},
   "source": [
    "## 3 ğŸ’¥ ç®—æ³•å²—é¢è¯•æ·±æŒ– (Interview Dive)\n",
    "\n",
    "**Q1: åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶ï¼Œ`map` å‡½æ•°ä¸­çš„ `batched=True` æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ**\n",
    "\n",
    "  * **A**: Tokenizer å†…éƒ¨æ˜¯ç”¨ Rust å†™çš„ï¼Œå¯¹åˆ—è¡¨å¤„ç†æœ‰æå¤§çš„å¹¶è¡ŒåŠ é€Ÿã€‚å¼€å¯ `batched=True` ä¸€æ¬¡æ€§ä¼ ç»™å®ƒ 1000 æ¡ï¼Œæ¯”è°ƒ 1000 æ¬¡ Python å‡½æ•°å¿«å‡ åå€ã€‚\n",
    "\n",
    "**Q2: ä»€ä¹ˆæ˜¯åŠ¨æ€ Paddingï¼Ÿæœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ**\n",
    "\n",
    "  * **A**: åœ¨ç»„è£… Batch æ—¶ï¼ŒæŒ‰ç…§å½“å‰ Batch çš„æœ€å¤§é•¿åº¦è¿›è¡Œè¡¥é½ï¼Œè€Œä¸æ˜¯å…¨å±€æœ€å¤§é•¿åº¦ã€‚è¿™èƒ½æ˜¾è‘—å‡å°‘æ— æ•ˆè®¡ç®—ï¼ˆPadding çš„éƒ¨åˆ†è™½ç„¶è¢« Mask äº†ï¼Œä½†çŸ©é˜µä¹˜æ³•è¿˜æ˜¯è¦ç®—çš„ï¼‰ï¼Œæå‡è®­ç»ƒé€Ÿåº¦çº¦ 30%-50%ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ccca8c",
   "metadata": {},
   "source": [
    "## 4 âš”ï¸ æ¯æ—¥ç®—æ³•é¢˜ (LeetCode Daily)\n",
    "\n",
    "ä»Šå¤©æˆ‘ä»¬è¦ç»ƒä¹ ä¸€é“ä¸ **â€œæ•°æ®åˆ†ç»„/å“ˆå¸Œâ€** å¼ºç›¸å…³çš„é¢˜ç›®ï¼Œè¿™åœ¨å¤§å‚ç®—æ³•å²—é¢è¯•ä¸­éå¸¸å¸¸è§ã€‚\n",
    "\n",
    "### ğŸ¯ é¢˜ç›®: [LeetCode 49. å­—æ¯å¼‚ä½è¯åˆ†ç»„ (Group Anagrams)](../../LeetCode%20practice/1-50.ipynb)\n",
    "\n",
    "  * **éš¾åº¦**: Medium\n",
    "  * **æè¿°**: ç»™ä½ ä¸€ä¸ªå­—ç¬¦ä¸²æ•°ç»„ï¼Œè¯·ä½ å°† **å­—æ¯å¼‚ä½è¯** ç»„åˆåœ¨ä¸€èµ·ã€‚\n",
    "      * è¾“å…¥: `[\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"]`\n",
    "      * è¾“å‡º: `[[\"bat\"], [\"nat\",\"tan\"], [\"ate\",\"eat\",\"tea\"]]`\n",
    "  * **å…³è”**: è¿™é“é¢˜æœ¬è´¨ä¸Šå°±æ˜¯ **Data Collator** çš„é€»è¾‘â€”â€”æŠŠç‰¹å¾ç›¸åŒï¼ˆè¿™é‡Œæ˜¯å­—æ¯ç»„æˆç›¸åŒï¼‰çš„æ•°æ®å½’ç±»åˆ°ä¸€èµ·ã€‚\n",
    "  * **æ ¸å¿ƒæ€è·¯**: å“ˆå¸Œè¡¨ (HashMap)ã€‚\n",
    "      * Key æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆæ’åºåçš„å­—ç¬¦ä¸²ï¼Œæˆ–è€…å­—ç¬¦è®¡æ•°å…ƒç»„ï¼‰ã€‚\n",
    "      * Value æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆåŸå­—ç¬¦ä¸²åˆ—è¡¨ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199a673",
   "metadata": {},
   "source": [
    "## âœ… Day 4 ä»»åŠ¡æ¸…å•\n",
    "\n",
    "1.  **è·‘é€šä»£ç **: ç”Ÿæˆ CSV -\\> Load -\\> Map -\\> Collate -\\> DataLoaderã€‚ç¡®ä¿ä½ çœ‹æ‡‚äº†æ¯ä¸€æ­¥çš„è¾“å…¥è¾“å‡ºã€‚\n",
    "2.  **ç†è§£å·®å¼‚**: å½»åº•ææ‡‚ **Static Padding (åœ¨ map é‡Œåš)** å’Œ **Dynamic Padding (åœ¨ collator é‡Œåš)** çš„åŒºåˆ«ã€‚\n",
    "3.  **LeetCode**: å®Œæˆ LeetCode 49ã€‚\n",
    "\n",
    "å®Œæˆåï¼Œè¯·å›å¤ **â€œDay 4 Pipeline æ­å»ºå®Œæ¯•â€**ã€‚\n",
    "æ˜å¤©ï¼ˆDay 5ï¼‰ï¼Œæˆ‘ä»¬å°†æŠŠâ€œæ¨¡å‹â€å’Œâ€œæ•°æ®â€ç»“åˆèµ·æ¥ï¼Œè¿›è¡Œ **Week 6 çš„ç»¼åˆå¤ç›˜**ï¼Œå¹¶ä¸ºä½ å³å°†åˆ°æ¥çš„ **Week 7 (å¾®è°ƒå®æˆ˜)** åšæœ€åçš„å‡†å¤‡ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
