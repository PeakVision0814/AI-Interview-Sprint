{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b751611",
   "metadata": {},
   "source": [
    "# 📒 S2W7D7: 模拟面试与知识点复盘 (Mock Interview Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9169754",
   "metadata": {},
   "source": [
    "## 📚 1 基础原理 (The Basics)\n",
    "\n",
    "### Q1: Softmax 的作用是什么？\n",
    "\n",
    "**你的回答**:\n",
    "\n",
    "> Softmax 是激活函数，将任意实数向量转化为一个规范的概率分布。它使得每个输出值都在 0 到 1 之间，并且所有输出值之和为 1。\n",
    "\n",
    "**👨‍🏫 面试官点评 (标准答案)**:\n",
    "回答得非常准确。核心关键词是 **归一化 (Normalization)**。\n",
    "\n",
    "  * **输入**: Logits (比如 `[2.0, 1.0, 0.1]`)，无界，难以直观理解。\n",
    "  * **输出**: Probabilities (比如 `[0.7, 0.2, 0.1]`)，和为 1。\n",
    "  * **数学公式**:\n",
    "    $$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.tensor([2.5, -0.1, 0.3])\n",
    "probs = F.softmax(logits, dim=0)\n",
    "\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Probs:  {probs}\")\n",
    "print(f\"Sum:    {probs.sum()}\") # 结果一定是 1.0\n",
    "```\n",
    "\n",
    "### Q2: CrossEntropyLoss (交叉熵损失) 的原理？\n",
    "\n",
    "**你的回答**:\n",
    "\n",
    "> (比较模糊，只记得名字)\n",
    "\n",
    "**👨‍🏫 面试官点评 (标准答案)**:\n",
    "交叉熵衡量的是 **“预测概率分布” 与 “真实分布 (Label)” 之间的差异**。\n",
    "\n",
    "  * **直观理解**: 惊讶度/惩罚。如果你对正确答案的预测概率越高，Loss 越小；反之 Loss 越大。\n",
    "  * **简化公式 (针对 One-Hot)**:\n",
    "    $$\\text{Loss} = - \\log(P_{\\text{correct}})$$\n",
    "  * **举例**:\n",
    "      * 正确类别是 0。\n",
    "      * 预测概率 $P_0 = 0.8 \\rightarrow Loss = -\\log(0.8) \\approx 0.22$ (惩罚小)\n",
    "      * 预测概率 $P_0 = 0.1 \\rightarrow Loss = -\\log(0.1) \\approx 2.30$ (惩罚大)\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# 模拟预测：Batch=1, 3分类\n",
    "logits = torch.tensor([[0.1, 0.8, 0.1]]) # 预测倾向于类别 1\n",
    "label_correct = torch.tensor([1])        # 真实标签是 1\n",
    "label_wrong = torch.tensor([0])          # 假设真实标签是 0 (模型猜错了)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_good = criterion(logits, label_correct)\n",
    "loss_bad = criterion(logits, label_wrong)\n",
    "\n",
    "print(f\"预测准确时的 Loss: {loss_good.item():.4f}\") # 应该是 0.22 左右\n",
    "print(f\"预测错误时的 Loss: {loss_bad.item():.4f}\")  # 应该是 2.30 左右\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ef3f5",
   "metadata": {},
   "source": [
    "## 🛠️ 2 模型与工程 (Model & Engineering)\n",
    "\n",
    "### Q3: BERT 的输入长度限制是多少？如何解决？\n",
    "\n",
    "**你的回答**:\n",
    "\n",
    "> 理论最大长度是 512 Token。如果超过会被截断。\n",
    "> **解决方案**: 分批次输入 (Chunking/Sliding Window)，每次输入小于 512，最后聚合结果。\n",
    "\n",
    "**👨‍🏫 面试官点评**:\n",
    "回答正确。BERT 的 Positional Encoding 只有 512 个位置，这是物理硬限制。长文本处理是 NLP 的经典难题。\n",
    "\n",
    "\n",
    "### Q4: 如何加速模型推理 (Inference Optimization)？\n",
    "\n",
    "**你的回答**:\n",
    "\n",
    "> (这是盲区，需要加强记忆)\n",
    "\n",
    "**👨‍🏫 面试官点评 (加分项)**:\n",
    "当 Python 推理太慢时，有三种主流方案：\n",
    "\n",
    "1.  **ONNX / TensorRT (最推荐)**:\n",
    "      * **原理**: 算子融合 (Operator Fusion) 和 静态图优化。\n",
    "      * **比喻**: PyTorch 是“边看菜谱边做菜的大厨 (动态图)”，ONNX 是“自动化流水线 (静态图)”，省去了 Python 和 C++ 反复交互的开销。\n",
    "2.  **量化 (Quantization)**:\n",
    "      * 从 FP32 (32位浮点) 转为 FP16 或 INT8。体积变小，计算变快。\n",
    "3.  **知识蒸馏 (Distillation)**:\n",
    "      * 使用 TinyBERT 或 DistilBERT 替换大模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8d5a4",
   "metadata": {},
   "source": [
    "## ⚖️ 第三部分：业务与算法 (Trade-offs & Algorithm)\n",
    "\n",
    "### Q5: 高风险场景 (如拆弹) 下，Precision 和 Recall 哪个更重要？\n",
    "\n",
    "**你的回答**:\n",
    "\n",
    "> Precision (查准率) 更重要。在不确定的时候坚决不能尝试，必须优先提升精确度。\n",
    "\n",
    "**👨‍🏫 面试官点评**:\n",
    "非常精准的业务判断。\n",
    "\n",
    "  * **Weighted Loss 的反思**: 我们项目中为了让机器人动起来，提升了 Recall。但在拆弹/手术场景，我们需要 **High Precision**。\n",
    "  * **策略**: 此时不应盲目加权 Recall，而应该提高 **置信度阈值 (Threshold)**，例如 `Confidence > 0.99` 才执行。\n",
    "\n",
    "\n",
    "\n",
    "### Q6: 双指针算法 (LeetCode 16/15) 的时间复杂度？\n",
    "\n",
    "**你的回答**:\n",
    "\n",
    "> $O(N^2)$。暴力解法是 $O(N^3)$，双指针省略了一次搜索。\n",
    "\n",
    "**👨‍🏫 面试官点评**:\n",
    "正确。\n",
    "\n",
    "  * **排序**: $O(N \\log N)$\n",
    "  * **双指针搜索**: 固定一个数 $i$，剩下两个数 $L, R$ 在有序数组中通过“两头往中间夹”的方式寻找，将 $O(N^2)$ 的搜索空间降维到了 $O(N)$。\n",
    "  * **总复杂度**: $O(N) \\times O(N) = O(N^2)$。\n",
    "\n",
    "\n",
    "\n",
    "## 🎓 Stage 2 总结\n",
    "\n",
    "  * **成就**: 完成了从 BERT 微调到 Weighted Loss 优化，再到独立推理引擎部署的完整闭环。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
