{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33f146f",
   "metadata": {},
   "source": [
    "# S2W7D1: æ¨¡å‹æ¶æ„ä¸åˆ†ç±»å¤´è®¾è®¡ (Model Architecture & The \"Head\")\n",
    "\n",
    "ä»Šå¤©æ˜¯æ„å»ºâ€œæœºå™¨äººæ„å›¾è¯†åˆ«ç³»ç»Ÿâ€çš„ç¬¬ä¸€å¤©ã€‚æˆ‘ä»¬ä¸æ€¥ç€è®­ç»ƒï¼Œå…ˆè¦æŠŠ**éª¨æ¶**æ­å¥½ã€‚\n",
    "\n",
    "è™½ç„¶ Hugging Face æä¾›äº† `AutoModelForSequenceClassification`ï¼Œä½†ä½œä¸ºç®—æ³•å·¥ç¨‹å¸ˆï¼Œ**æ‰‹å†™ä¸€ä¸ª Classifier Class** æ˜¯å¿…é¡»æŒæ¡çš„åŸºæœ¬åŠŸã€‚åªæœ‰è¿™æ ·ï¼Œä½ æ‰èƒ½éšå¿ƒæ‰€æ¬²åœ°ä¿®æ”¹ç½‘ç»œç»“æ„ï¼ˆæ¯”å¦‚åœ¨ BERT åé¢åŠ ä¸ª LSTMï¼Œæˆ–è€…æ”¹ç”¨å¤šå±‚ MLPï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494200b",
   "metadata": {},
   "source": [
    "## 1 ğŸ› ï¸ æ ¸å¿ƒä»£ç ï¼šå°è£… BERT åˆ†ç±»å™¨\n",
    "\n",
    "è¯·åœ¨ `project_root/src/models/` ç›®å½•ä¸‹æ–°å»ºæ–‡ä»¶ `model_bert.py`ã€‚è¿™æ®µä»£ç å°†å®šä¹‰æˆ‘ä»¬æœªæ¥ä¸€å‘¨è¦è®­ç»ƒçš„æ¨¡å‹æœ¬ä½“ã€‚\n",
    "\n",
    "**æ–‡ä»¶è·¯å¾„**: `project_root/src/models/model_bert.py`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, model_path, num_labels):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– BERT åˆ†ç±»å™¨\n",
    "        :param model_path: æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ (str)\n",
    "        :param num_labels: åˆ†ç±»çš„ç±»åˆ«æ•° (int)\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        # 1. åŠ è½½é¢„è®­ç»ƒçš„ BERT Backbone (è„ŠæŸ±)\n",
    "        # config.json å’Œ model.safetensors å¿…é¡»åœ¨ model_path ä¸‹\n",
    "        self.bert = BertModel.from_pretrained(model_path)\n",
    "        \n",
    "        # 2. å®šä¹‰åˆ†ç±»å¤´ (Classification Head)\n",
    "        # BERT base çš„ hidden_size æ˜¯ 768\n",
    "        # è¿™é‡Œçš„é€»è¾‘æ˜¯: [Batch, 768] -> [Batch, num_labels]\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "        \n",
    "        # (å¯é€‰) Dropout å±‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \"\"\"\n",
    "        # 1. BERT ç¼–ç \n",
    "        # output[0]: last_hidden_state (Batch, Seq, 768)\n",
    "        # output[1]: pooler_output (Batch, 768) -> è¿™æ˜¯ CLS ç»è¿‡åŠ å·¥åçš„å‘é‡ï¼Œé€‚åˆåšåˆ†ç±»\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        pooler_output = outputs.pooler_output\n",
    "        \n",
    "        # 2. ç»è¿‡ Dropout\n",
    "        pooler_output = self.dropout(pooler_output)\n",
    "        \n",
    "        # 3. ç»è¿‡çº¿æ€§å±‚ï¼Œå¾—åˆ° Logits\n",
    "        logits = self.classifier(pooler_output)\n",
    "        \n",
    "        return logits\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9897d",
   "metadata": {},
   "source": [
    "## 2 ğŸ§  ç†è®ºè§£æï¼šä» Encoder åˆ° Logits\n",
    "\n",
    "åœ¨ Notebook ä¸­ï¼Œæˆ‘ä»¬éœ€è¦éªŒè¯è¿™ä¸ªæ¨¡å‹æ˜¯å¦å·¥ä½œæ­£å¸¸ã€‚è¿™éƒ¨åˆ†ä¸ä»…æ˜¯æµ‹è¯•ï¼Œæ›´æ˜¯ä¸ºäº†è®©ä½ ç†è§£æ•°æ®çš„æµåŠ¨ã€‚\n",
    "\n",
    "**Notebook ä»£ç å— (S2W7D1\\_Model\\_Arch.ipynb):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d4a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡: /home/goodminton/study/AI-Interview-Sprint/data/pretrained_models/bert-base-chinese ...\n",
      "âœ… æ¨¡å‹å®ä¾‹åŒ–æˆåŠŸï¼\n",
      "------------------------------\n",
      "è¾“å…¥æ–‡æœ¬: è¯·ä½ æ‹¿èµ·é‚£ä¸ªæ¯å­\n",
      "è¾“å‡º Logits å½¢çŠ¶: torch.Size([1, 4])\n",
      "è¾“å‡º Logits å€¼: tensor([[ 0.1656, -0.5437, -0.7109, -0.7762]])\n",
      "éšæœºåˆå§‹åŒ–é¢„æµ‹ç»“æœ: MOVE (ç§»åŠ¨)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"../../\") \n",
    "from src.config import PRETRAINED_MODEL_DIR\n",
    "from src.models.model_bert import BertClassifier\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 1. æ¨¡æ‹Ÿä¸šåŠ¡åœºæ™¯ï¼šæœºå™¨äººæ„å›¾å®šä¹‰\n",
    "# å‡è®¾æˆ‘ä»¬æœ‰ 4 ç±»æ„å›¾\n",
    "label_map = {\n",
    "    0: \"MOVE (ç§»åŠ¨)\",\n",
    "    1: \"GRAB (æŠ“å–)\",\n",
    "    2: \"STOP (åœæ­¢)\",\n",
    "    3: \"OTHER (é—²èŠ/æ— å…³)\"\n",
    "}\n",
    "num_labels = len(label_map)\n",
    "\n",
    "# 2. å®ä¾‹åŒ–æ¨¡å‹\n",
    "print(f\"æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡: {PRETRAINED_MODEL_DIR} ...\")\n",
    "# æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº† src/config.py å®šä¹‰çš„è·¯å¾„ï¼Œéå¸¸ä¼˜é›…\n",
    "model = BertClassifier(model_path=PRETRAINED_MODEL_DIR, num_labels=num_labels)\n",
    "print(\"âœ… æ¨¡å‹å®ä¾‹åŒ–æˆåŠŸï¼\")\n",
    "\n",
    "# 3. æ„é€ ä¸€ä¸ª Dummy Input (å‡æ•°æ®)\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "text = \"è¯·ä½ æ‹¿èµ·é‚£ä¸ªæ¯å­\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 4. å‰å‘ä¼ æ’­æµ‹è¯•\n",
    "model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n",
    "with torch.no_grad():\n",
    "    logits = model(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask']\n",
    "    )\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"è¾“å…¥æ–‡æœ¬: {text}\")\n",
    "print(f\"è¾“å‡º Logits å½¢çŠ¶: {logits.shape}\") # é¢„æœŸ: [1, 4]\n",
    "print(f\"è¾“å‡º Logits å€¼: {logits}\")\n",
    "\n",
    "# 5. ç†è§£ Logits (æœªå½’ä¸€åŒ–çš„æ¦‚ç‡)\n",
    "# è¿˜æ²¡ç»è¿‡ Softmaxï¼Œæ‰€ä»¥å€¼å¯ä»¥æ˜¯è´Ÿæ— ç©·åˆ°æ­£æ— ç©·\n",
    "# è®­ç»ƒåˆæœŸï¼Œè¿™äº›å€¼æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œæ²¡æœ‰æ„ä¹‰\n",
    "predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "print(f\"éšæœºåˆå§‹åŒ–é¢„æµ‹ç»“æœ: {label_map[predicted_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b471a",
   "metadata": {},
   "source": [
    "## 3 ğŸ¤– å…·èº«æ™ºèƒ½åœºæ™¯ï¼šä¸ºä»€ä¹ˆéœ€è¦åˆ†ç±»å¤´ï¼Ÿ\n",
    "\n",
    "ä½ å¯èƒ½ä¼šé—®ï¼š*â€œä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ BERT è¾“å‡ºçš„å‘é‡åšç›¸ä¼¼åº¦åŒ¹é…ï¼ˆåƒ Day 3 é‚£æ ·ï¼‰ï¼Œè€Œéè¦è®­ç»ƒä¸€ä¸ªåˆ†ç±»å¤´ï¼Ÿâ€*\n",
    "\n",
    "è¿™æ˜¯\\*\\*â€œæ£€ç´¢å¼ (Retrieval)â€**ä¸**â€œåˆ†ç±»å¼ (Classification)â€\\*\\*çš„åŒºåˆ«ï¼Œä¹Ÿæ˜¯é¢è¯•å¸¸è€ƒç‚¹ã€‚\n",
    "\n",
    "  * **ç›¸ä¼¼åº¦åŒ¹é… (Day 3)**:\n",
    "      * **ä¼˜ç‚¹**: Zero-shotï¼Œä¸éœ€è¦é‡æ–°è®­ç»ƒï¼Œæ–°åŠ æŒ‡ä»¤åªéœ€åŠ åˆ°æ•°æ®åº“ã€‚\n",
    "      * **ç¼ºç‚¹**: éš¾ä»¥å¤„ç†å¤æ‚é€»è¾‘ã€‚ä¾‹å¦‚ \"ä¸è¦åœ\" å’Œ \"åœ\" è¯­ä¹‰æåº¦ç›¸ä¼¼ï¼Œä½™å¼¦ç›¸ä¼¼åº¦æé«˜ï¼Œä½†æ„å›¾å®Œå…¨ç›¸åã€‚\n",
    "  * **åˆ†ç±»å¤´å¾®è°ƒ (Week 7)**:\n",
    "      * **ä¼˜ç‚¹**: **ç²¾åº¦æé«˜**ã€‚é€šè¿‡å¾®è°ƒï¼Œæ¨¡å‹ä¼šå¼ºè¡Œè®°ä½ \"ä¸è¦ X\" å’Œ \"X\" çš„åŒºåˆ«ã€‚å¯¹äº \"ç´§æ€¥åœæ­¢\" è¿™ç§é«˜é£é™©æŒ‡ä»¤ï¼Œå¿…é¡»ç”¨åˆ†ç±»æ¨¡å‹ã€‚\n",
    "      * **å…·èº«åº”ç”¨**: æœºå™¨äººçš„å®‰å…¨æŒ‡ä»¤ï¼ˆStop, E-Stopï¼‰é€šå¸¸èµ°åˆ†ç±»æ¨¡å‹ï¼Œå› ä¸ºå®¹é”™ç‡ä½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e2ba9",
   "metadata": {},
   "source": [
    "## 4 ğŸ’¥ é¢è¯•å¿…é—®ï¼šPooler Output vs Last Hidden State\n",
    "\n",
    "é¢è¯•å®˜ï¼šâ€œä½ åœ¨ `model_bert.py` é‡Œç”¨äº† `outputs.pooler_output`ï¼Œå®ƒå’Œ `outputs[0]` (last\\_hidden\\_state) æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿâ€\n",
    "\n",
    "  * **Last Hidden State**: `[Batch, Seq_Len, 768]`ã€‚æ˜¯æ¯ä¸ª Token çš„å‘é‡ã€‚å¦‚æœä½ è¦åš**åºåˆ—æ ‡æ³¨**ï¼ˆæ¯”å¦‚æ‰¾å‡ºå¥å­é‡Œçš„ç‰©ä½“åè¯ï¼‰ï¼Œå¿…é¡»ç”¨è¿™ä¸ªã€‚\n",
    "  * **Pooler Output**: `[Batch, 768]`ã€‚æ˜¯ `[CLS]` ä½ç½®çš„å‘é‡ç»è¿‡ `Dense` + `Tanh` æ¿€æ´»åçš„ç»“æœã€‚å®ƒæ˜¯å®˜æ–¹é¢„è®­ç»ƒæ—¶ç”¨äº NSP ä»»åŠ¡çš„ï¼Œå¤©ç”Ÿé€‚åˆåš**æ•´å¥åˆ†ç±»**ã€‚\n",
    "  * **é¢è¯•åŠ åˆ†é¡¹**: *â€œå…¶å®ç°åœ¨çš„ Paper é‡Œï¼ˆå¦‚ RoBERTaï¼‰ï¼Œæœ‰æ—¶å€™å»ºè®®ç›´æ¥å– `Last Hidden State` çš„ç¬¬ä¸€ä¸ªå‘é‡ï¼ˆå³ CLS åŸå§‹å‘é‡ï¼‰è‡ªå·±åš Poolingï¼Œå› ä¸º BERT åŸç”Ÿçš„ Pooler å±‚åœ¨æœªå¾®è°ƒå‰å¯èƒ½å¸¦æœ‰ä¸€äº›é¢„è®­ç»ƒä»»åŠ¡çš„ Biasï¼Œä½†åœ¨å…¨é‡å¾®è°ƒä¸‹ï¼Œç›´æ¥ç”¨ Pooler Output ä¹Ÿæ˜¯æ ‡å‡†åšæ³•ã€‚â€*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f6c7a",
   "metadata": {},
   "source": [
    "## 5 âš”ï¸ æ¯æ—¥ç®—æ³•é¢˜ (LeetCode Daily)\n",
    "\n",
    "ä¸ºäº†é…åˆ Tokenizer çš„**æ»‘åŠ¨çª—å£**æ€ç»´ï¼Œä»Šå¤©æˆ‘ä»¬æŒ‘æˆ˜ä¸€é“ç»å…¸çš„â€œçª—å£â€é¢˜ã€‚\n",
    "\n",
    "### ğŸ¯ é¢˜ç›®: [LeetCode 3. æ— é‡å¤å­—ç¬¦çš„æœ€é•¿å­ä¸² (Longest Substring Without Repeating Characters)](../../LeetCode%20practice/1-50.ipynb)\n",
    "\n",
    "  * **éš¾åº¦**: Medium\n",
    "  * **æè¿°**: ç»™å®šä¸€ä¸ªå­—ç¬¦ä¸² `s` ï¼Œè¯·ä½ æ‰¾å‡ºå…¶ä¸­ä¸å«æœ‰é‡å¤å­—ç¬¦çš„ **æœ€é•¿å­ä¸²** çš„é•¿åº¦ã€‚\n",
    "  * **ç¤ºä¾‹**: è¾“å…¥ `s = \"abcabcbb\"`, è¾“å‡º `3` (å› ä¸º \"abc\")ã€‚\n",
    "  * **å…³è”**:\n",
    "      * **NLP**: è¿™å°±æ˜¯æœ€åŸºç¡€çš„ **Sliding Window (æ»‘åŠ¨çª—å£)** æœºåˆ¶ã€‚BERT çš„ Attention Mask å…¶å®ä¹Ÿæ˜¯ä¸€ç§æ§åˆ¶çª—å£çš„æ‰‹æ®µã€‚\n",
    "      * **æ€ç»´**: ç»´æŠ¤ä¸€ä¸ª `left` å’Œ `right` æŒ‡é’ˆï¼Œä»¥åŠä¸€ä¸ª Hash Setï¼ˆæˆ–å­—å…¸ï¼‰æ¥è®°å½•çª—å£å†…çš„å­—ç¬¦ã€‚\n",
    "  * **Key Idea**: å½“ `right` é‡åˆ°é‡å¤å­—ç¬¦æ—¶ï¼Œ`left` è¦ä¸€æ­¥æ­¥å³ç§»ï¼Œç›´åˆ°æŠŠé‚£ä¸ªé‡å¤å­—ç¬¦â€œæŒ¤å‡ºâ€çª—å£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a9b26",
   "metadata": {},
   "source": [
    "## âœ… Day 1 ä»»åŠ¡æ¸…å•\n",
    "\n",
    "1.  **å·¥ç¨‹æ­å»º**: æŒ‰ç…§æ–°ç»“æ„åˆ›å»ºç›®å½•ï¼Œç¼–å†™ `src/config.py`ã€‚\n",
    "2.  **æ ¸å¿ƒä»£ç **: å®Œæˆ `src/model_bert.py`ï¼Œç¡®ä¿æ²¡æœ‰è¯­æ³•é”™è¯¯ã€‚\n",
    "3.  **è·‘é€š Demo**: åœ¨ `S2W7D1` Notebook ä¸­æˆåŠŸåŠ è½½æ¨¡å‹å¹¶æ‰“å°å‡º Logits å½¢çŠ¶ `[1, 4]`ã€‚\n",
    "4.  **ç®—æ³•**: é€šè¿‡ LeetCode 3ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
