{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f8308a",
   "metadata": {},
   "source": [
    "# S2W7D4: è¯Šæ–­ä¸è°ƒä¼˜ (Diagnosis & Optimization)\n",
    "æ˜¨å¤©æˆ‘ä»¬ç»™æ¨¡å‹åšäº†ä¸€æ¬¡â€œä½“æ£€â€ï¼Œå‘ç°å®ƒè™½ç„¶åŠæ ¼äº†ï¼ˆAccuracy 60%ï¼‰ï¼Œä½†æ˜¯æœ‰ç‚¹â€œåç§‘â€ï¼ˆå¯¹ç¨€æœ‰æ ·æœ¬ Grab ä¸æ•¢é¢„æµ‹ï¼‰ã€‚åŒæ—¶æˆ‘ä»¬ä¿®æ”¹äº†ä»£ç ï¼Œå¼•å…¥äº† **Weighted Loss** è¯•å›¾çŸ«æ­£è¿™ä¸ªé—®é¢˜ã€‚\n",
    "\n",
    "ä»Šå¤©ï¼Œæˆ‘ä»¬è¦åŒ–èº«ä¸º **â€œAI åŒ»ç”Ÿâ€**ã€‚æˆ‘ä»¬ä¸èƒ½åªçœ‹åˆ†æ•°çš„æ¶¨è·Œï¼Œæˆ‘ä»¬è¦**çœ‹ç€ç—…äººçš„çœ¼ç›**ï¼ˆæŸ¥çœ‹é”™è¯¯æ ·æœ¬ï¼‰ï¼Œæ‰¾å‡ºå®ƒåˆ°åº•åœ¨å“ªé‡ŒçŠ¯ç³Šæ¶‚ï¼Œç„¶åé€šè¿‡**è°ƒå‚**ï¼ˆå¼€è¯ï¼‰æ¥æ²»å¥½å®ƒã€‚\n",
    "\n",
    "**è¿™æ˜¯é¢è¯•ä¸­â€œé¡¹ç›®æ·±æŒ–â€ç¯èŠ‚æœ€å®¹æ˜“è¢«é—®åˆ°çš„åœ°æ–¹**ï¼šâ€œä½ é‡åˆ°è¿‡ Bad Case å—ï¼Ÿä½ æ˜¯æ€ä¹ˆåˆ†æçš„ï¼Ÿæœ€åæ€ä¹ˆè§£å†³çš„ï¼Ÿâ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe46ea",
   "metadata": {},
   "source": [
    "## 1 ğŸ” æ ¸å¿ƒä»»åŠ¡ï¼šå¯è§†åŒ–â€œåæ ·æœ¬â€ (Bad Case Analysis)\n",
    "\n",
    "Loss åªæ˜¯ä¸€ä¸ªæ•°å­—ï¼Œå®ƒä¸ä¼šå‘Šè¯‰ä½ æ¨¡å‹ä¸ºä»€ä¹ˆæŠŠâ€œå‘å·¦è½¬â€å¬æˆäº†â€œåœæ­¢â€ã€‚æˆ‘ä»¬éœ€è¦æŠŠé¢„æµ‹é”™è¯¯çš„æ ·æœ¬æ‰“å°å‡ºæ¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a2e975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ é”™è¯¯æ ·æœ¬åˆ†ææŠ¥å‘Š:\n",
      "|    | Text             | True Label   | Pred Label   |   Confidence |\n",
      "|---:|:-----------------|:-------------|:-------------|-------------:|\n",
      "|  0 | æŠ“èµ·æ¥           | GRAB         | MOVE         |     0.474486 |\n",
      "|  1 | è¯·å‹¿ç§»åŠ¨         | STOP         | MOVE         |     0.420997 |\n",
      "|  2 | æŠŠé‚£ä¸ªæ¯å­æ‹¿ç»™æˆ‘ | GRAB         | MOVE         |     0.462568 |\n",
      "|  3 | æ¾å¼€æ‰‹           | GRAB         | MOVE         |     0.48105  |\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\") \n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer\n",
    "from src.config import PRETRAINED_MODEL_DIR, CHECKPOINT_DIR\n",
    "from src.models.model_bert import BertClassifier\n",
    "from src.dataset.dataset import IntentDataset\n",
    "from src.metrics import compute_metrics\n",
    "\n",
    "# 1. å‡†å¤‡éªŒè¯é›† (è¿™æ¬¡å¤šé€ ä¸€ç‚¹éš¾çš„æ•°æ®ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯)\n",
    "val_texts = [\n",
    "    \"å‘å·¦è½¬\", \"åœæ­¢\", \"æŠ“èµ·æ¥\", \"è¯·å‹¿ç§»åŠ¨\", \"å‰è¿›\", \n",
    "    \"æŠŠé‚£ä¸ªæ¯å­æ‹¿ç»™æˆ‘\", \"ç´§æ€¥åˆ¶åŠ¨\", \"æ…¢æ…¢å¾€å‰èµ°\", \"å‘å³è½¬åŠ¨ä¸€ç‚¹ç‚¹\", \"æ¾å¼€æ‰‹\"\n",
    "]\n",
    "# å‡è®¾ 0:MOVE, 1:STOP, 2:GRAB\n",
    "val_labels = [0, 1, 2, 1, 0, 2, 1, 0, 0, 2]\n",
    "label_map = {0: \"MOVE\", 1: \"STOP\", 2: \"GRAB\"}\n",
    "\n",
    "# 2. åŠ è½½æ˜¨å¤©è®­ç»ƒå¥½çš„æ¨¡å‹ (Checkpoint-24)\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "model = BertClassifier(model_path=PRETRAINED_MODEL_DIR, num_labels=3)\n",
    "# ä¿®æ­£åŠ è½½æ–¹å¼ï¼Œä½¿ç”¨ safetensors\n",
    "from safetensors.torch import load_file\n",
    "# è¯·ç¡®è®¤ checkpoint-24 æ˜¯ä½ æ˜¨å¤©çš„æœ€ä½³æƒé‡ç›®å½•\n",
    "best_ckpt = CHECKPOINT_DIR / \"checkpoint-24\" \n",
    "state_dict = load_file(f\"{best_ckpt}/model.safetensors\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# 3. è¿›è¡Œé¢„æµ‹\n",
    "training_args = TrainingArguments(output_dir=\"./results\", per_device_eval_batch_size=4)\n",
    "trainer = Trainer(model=model, args=training_args)\n",
    "predictions = trainer.predict(val_dataset)\n",
    "\n",
    "# 4. æå–ç»“æœ\n",
    "pred_logits = predictions.predictions\n",
    "pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "true_ids = predictions.label_ids\n",
    "\n",
    "# 5. â˜…â˜…â˜… æ ¸å¿ƒï¼šæ‰¾å‡º Bad Cases â˜…â˜…â˜…\n",
    "bad_cases = []\n",
    "for i in range(len(val_texts)):\n",
    "    if pred_ids[i] != true_ids[i]:\n",
    "        bad_cases.append({\n",
    "            \"Text\": val_texts[i],\n",
    "            \"True Label\": label_map[true_ids[i]],\n",
    "            \"Pred Label\": label_map[pred_ids[i]],\n",
    "            \"Confidence\": np.max(torch.softmax(torch.tensor(pred_logits[i]), dim=-1).numpy())\n",
    "        })\n",
    "\n",
    "# 6. ç”¨ Pandas æ¼‚äº®åœ°æ‰“å°å‡ºæ¥\n",
    "df = pd.DataFrame(bad_cases)\n",
    "print(\"âŒ é”™è¯¯æ ·æœ¬åˆ†ææŠ¥å‘Š:\")\n",
    "if not df.empty:\n",
    "    print(df.to_markdown())\n",
    "else:\n",
    "    print(\"ğŸ‰ å¤ªæ£’äº†ï¼éªŒè¯é›†å…¨å¯¹ï¼(è¯´æ˜è¯¥å¢åŠ éš¾åº¦äº†)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd3f7f",
   "metadata": {},
   "source": [
    "**è¯Šæ–­æ€è€ƒ (Interview Point)**:\n",
    "\n",
    "  * å¦‚æœæ¨¡å‹æŠŠ \"è¯·å‹¿ç§»åŠ¨\" é¢„æµ‹æˆ `MOVE`ï¼šè¯´æ˜å®ƒåªå…³æ³¨åˆ°äº†å…³é”®è¯ \"ç§»åŠ¨\"ï¼Œæ²¡å­¦åˆ°å¦å®šè¯ \"è¯·å‹¿\"ã€‚ -\\> **å¯¹ç­–**ï¼šå¢åŠ å¸¦æœ‰å¦å®šè¯çš„è®­ç»ƒæ•°æ®ã€‚\n",
    "  * å¦‚æœæ¨¡å‹æŠŠ \"æŠ“èµ·æ¥\" é¢„æµ‹æˆ `MOVE`ï¼šè¯´æ˜å®ƒè¿˜æ²¡å­¦ä¼š `GRAB` è¿™ä¸ªç”Ÿåƒ»ç±»åˆ«ã€‚ -\\> **å¯¹ç­–**ï¼šæ˜¨å¤©åŠ çš„ Weighted Loss åº”è¯¥èƒ½ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07a8a2",
   "metadata": {},
   "source": [
    "## 2 ğŸ›ï¸ æ ¸å¿ƒä»»åŠ¡ï¼šå¯ç”¨ Weighted Loss é‡è®­\n",
    "\n",
    "æ˜¨å¤©æˆ‘ä»¬ä¿®æ”¹äº† `model_bert.py`ï¼Œä½†è¿˜æ²¡çœŸæ­£ç”¨å®ƒé‡æ–°è®­ç»ƒã€‚ä»Šå¤©æˆ‘ä»¬è¦ç”¨å¸¦æƒé‡çš„ Loss å†è·‘ä¸€æ¬¡ï¼Œçœ‹çœ‹èƒ½ä¸èƒ½æŠŠæ˜¨å¤© Precision=0 çš„å°´å°¬è§£å†³æ‰ã€‚\n",
    "\n",
    "### æ­¥éª¤\n",
    "\n",
    "1.  **é‡å¯ Kernel** (ç¡®ä¿ `src.models.model_bert` çš„ä¿®æ”¹ç”Ÿæ•ˆ)ã€‚\n",
    "2.  **å¤åˆ¶ Day 2 çš„è®­ç»ƒä»£ç ** (æˆ–è€…ç›´æ¥åœ¨ Day 4 Notebook é‡Œé‡å†™ä¸€é Trainer æµç¨‹)ã€‚\n",
    "3.  **è¿è¡Œè®­ç»ƒ**ã€‚\n",
    "4.  **å†æ¬¡è¯„ä¼°**ï¼šé‡ç‚¹çœ‹ `GRAB` ç±»åˆ«çš„ Recall æœ‰æ²¡æœ‰ä¸Šå‡ã€‚\n",
    "\n",
    "**ä»£ç ç‰‡æ®µ (Day 4 Notebook):**\n",
    "\n",
    "```python\n",
    "# ... å®šä¹‰ train_dataset (è®°å¾—åŒ…å«ä¸€äº› Label=2 çš„æ ·æœ¬) ...\n",
    "\n",
    "# é‡æ–°åˆå§‹åŒ–æ¨¡å‹ (è¿™æ—¶å€™å®ƒä¼šè¯»å–æˆ‘ä»¬åœ¨ init é‡Œå†™å¥½çš„ class_weights)\n",
    "model_weighted = BertClassifier(model_path=PRETRAINED_MODEL_DIR, num_labels=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_weighted,\n",
    "    args=training_args, # ç¨å¾®è°ƒå¤§ epoch è¯•è¯•ï¼Œæ¯”å¦‚ 5\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# è®­ç»ƒå®Œç«‹åˆ»è¯„ä¼°\n",
    "print(trainer.evaluate())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7fe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹å·²åŠ è½½ (Weighted Loss å¯ç”¨)\n",
      "ğŸš€ å¼€å§‹é‡è®­å¹¶ä¿å­˜è‡³ /home/goodminton/study/AI-Interview-Sprint/output/train/checkpoints ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 01:39, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.174000</td>\n",
       "      <td>1.178933</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.817700</td>\n",
       "      <td>1.140815</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.811100</td>\n",
       "      <td>0.925142</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.659900</td>\n",
       "      <td>0.930355</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.568900</td>\n",
       "      <td>0.946947</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.723810</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.445700</td>\n",
       "      <td>0.867834</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.361900</td>\n",
       "      <td>0.759284</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.739826</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.734036</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.714262</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.738418</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.733106</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>0.727063</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.151900</td>\n",
       "      <td>0.722320</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.722833</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goodminton/anaconda3/envs/utils/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/goodminton/anaconda3/envs/utils/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… æœ€ä½³æ¨¡å‹å·²å¼ºåˆ¶ä¿å­˜è‡³: /home/goodminton/study/AI-Interview-Sprint/output/train/checkpoints/final_best\n",
      "ğŸ‘‰ è¯·åœ¨ Day 5 çš„æ¨ç†ä»£ç ä¸­ä½¿ç”¨è¿™ä¸ªè·¯å¾„ï¼\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\") \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer\n",
    "from src.config import PRETRAINED_MODEL_DIR, CHECKPOINT_DIR\n",
    "from src.models.model_bert import BertClassifier\n",
    "from src.dataset.dataset import IntentDataset\n",
    "from src.metrics import compute_metrics\n",
    "from src.config import CHECKPOINT_DIR\n",
    "import shutil # ç”¨äºæ¸…ç†æ—§æ–‡ä»¶\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. æ•°æ®å‡†å¤‡ (ä¿æŒä¸å˜)\n",
    "# -----------------------------------------------------------------------------\n",
    "train_texts = [\"å‘å‰èµ°\", \"å‘åé€€\", \"å·¦è½¬\", \"å³è½¬\", \"å‰è¿›åç±³\", \"åé€€äº”æ­¥\", \"å¾€å·¦è¾¹ç§»åŠ¨\", \"å¾€å³è¾¹ç§»åŠ¨\", \"ä¸€ç›´èµ°\", \"æ…¢é€Ÿå‰è¿›\", \"å…¨é€Ÿå‰è¿›\", \"è½¬ä¸ªåœˆ\", \"åœä¸‹\", \"ç´§æ€¥åˆ¶åŠ¨\", \"åˆ«åŠ¨\", \"æŠ“å–æ¯å­\", \"æŠŠè‹¹æœæ‹¿èµ·æ¥\", \"æ¾å¼€å¤¹çˆª\"]\n",
    "train_labels = [0]*12 + [1]*3 + [2]*3\n",
    "\n",
    "val_texts = [\"å‘å·¦è½¬\", \"åœæ­¢\", \"æŠ“èµ·æ¥\", \"è¯·å‹¿ç§»åŠ¨\", \"å‰è¿›\", \"æŠŠé‚£ä¸ªæ¯å­æ‹¿ç»™æˆ‘\", \"æ¾å¼€æ‰‹\"]\n",
    "val_labels = [0, 1, 2, 1, 0, 2, 2]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "train_dataset = IntentDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. åˆå§‹åŒ–æ¨¡å‹ (Weighted Loss)\n",
    "# -----------------------------------------------------------------------------\n",
    "model_weighted = BertClassifier(model_path=PRETRAINED_MODEL_DIR, num_labels=3)\n",
    "print(\"âœ… æ¨¡å‹å·²åŠ è½½ (Weighted Loss å¯ç”¨)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šé…ç½®ä¿å­˜ç­–ç•¥ â˜…â˜…â˜…\n",
    "# -----------------------------------------------------------------------------\n",
    "# å®šä¹‰ä¸€ä¸ªä¸“é—¨çš„ä¿å­˜è·¯å¾„ï¼Œæ–¹ä¾¿ Day 5 è°ƒç”¨\n",
    "save_dir = str(CHECKPOINT_DIR)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_dir,             # â˜… ä¿®æ”¹ï¼šæŒ‡å®šæ˜ç¡®çš„ä¿å­˜è·¯å¾„\n",
    "    num_train_epochs=15,             # å¤šè·‘å‡ è½®ï¼Œç¡®ä¿æ”¶æ•›\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=5,\n",
    "    \n",
    "    # --- è¯„ä¼°ä¸ä¿å­˜ç­–ç•¥ ---\n",
    "    eval_strategy=\"epoch\",           # æ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡\n",
    "    save_strategy=\"epoch\",           # â˜… ä¿®æ”¹ï¼šæ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡æƒé‡ (å¿…é¡»å¼€å¯ï¼)\n",
    "    save_total_limit=3,              # â˜… ä¿®æ”¹ï¼šåªä¿ç•™ 1 ä¸ªæœ€æ–°çš„ï¼Œé˜²æ­¢ç¡¬ç›˜çˆ†ç‚¸\n",
    "    load_best_model_at_end=True,     # â˜… ä¿®æ”¹ï¼šè®­ç»ƒç»“æŸæ—¶ï¼Œè‡ªåŠ¨åŠ è½½æ•ˆæœæœ€å¥½çš„é‚£ä¸ªæ¨¡å‹\n",
    "    metric_for_best_model=\"f1\",      # ä»¥ F1 åˆ†æ•°ä½œä¸ºè¡¡é‡æ ‡å‡†\n",
    "    \n",
    "    no_cuda=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. è®­ç»ƒä¸è¯„ä¼°\n",
    "# -----------------------------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model_weighted,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(f\"ğŸš€ å¼€å§‹é‡è®­å¹¶ä¿å­˜è‡³ {save_dir} ...\")\n",
    "trainer.train()\n",
    "\n",
    "# â˜…â˜…â˜… é¢å¤–ä¿é™©ï¼šæ‰‹åŠ¨ä¿å­˜æœ€ç»ˆçš„æœ€ä½³æ¨¡å‹ â˜…â˜…â˜…\n",
    "final_path = f\"{save_dir}/final_best\"\n",
    "trainer.save_model(final_path)\n",
    "print(f\"\\nâœ… æœ€ä½³æ¨¡å‹å·²å¼ºåˆ¶ä¿å­˜è‡³: {final_path}\")\n",
    "print(\"ğŸ‘‰ è¯·åœ¨ Day 5 çš„æ¨ç†ä»£ç ä¸­ä½¿ç”¨è¿™ä¸ªè·¯å¾„ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77583326",
   "metadata": {},
   "source": [
    "## 3 ğŸ§ª è¿›é˜¶ä»»åŠ¡ï¼šè¶…å‚æ•°è°ƒä¼˜ (Hyperparameter Tuning)\n",
    "\n",
    "ç®—æ³•å·¥ç¨‹å¸ˆçš„æ—¥å¸¸å°±æ˜¯â€œç‚¼ä¸¹â€ã€‚ä½ éœ€è¦æ‰‹åŠ¨è°ƒæ•´ `TrainingArguments` é‡Œçš„å‚æ•°ï¼Œæ„Ÿå—å®ƒä»¬å¯¹ç»“æœçš„å½±å“ã€‚\n",
    "\n",
    "è¯·å°è¯•ä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼Œå¹¶è®°å½• Accuracy çš„å˜åŒ–ï¼š\n",
    "\n",
    "| å‚æ•° | å»ºè®®å°è¯•å€¼ | é¢„æœŸå½±å“ |\n",
    "| :--- | :--- | :--- |\n",
    "| **Learning Rate** | `5e-5` (å˜å¤§) | æ”¶æ•›å˜å¿«ï¼Œä½†å¯èƒ½éœ‡è¡ä¸æ”¶æ•› (Loss ä¹±è·³)ã€‚ |\n",
    "| **Learning Rate** | `1e-5` (å˜å°) | æ”¶æ•›ææ…¢ï¼Œéœ€è¦æ›´å¤š Epochï¼Œä½†ç»“æœå¯èƒ½æ›´ç²¾ç»†ã€‚ |\n",
    "| **Batch Size** | `4` æˆ– `8` | æ¢¯åº¦æ›´ç¨³å®šï¼Œä½†æ˜¾å­˜å ç”¨å¢åŠ ã€‚ |\n",
    "| **Num Epochs** | `5` æˆ– `10` | è®­ç»ƒæ›´ä¹…ï¼Œå°å¿ƒè¿‡æ‹Ÿåˆ (Val Loss å‡é«˜)ã€‚ |\n",
    "\n",
    "**é¢è¯•å¿…é—®**: *â€œä½ åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨äº†ä»€ä¹ˆ Learning Rateï¼Ÿä¸ºä»€ä¹ˆï¼Ÿâ€*\n",
    "**æ ‡å‡†ç­”æ¡ˆ**: *â€œæˆ‘ä½¿ç”¨äº† BERT å¾®è°ƒçš„é»„é‡‘åŒºé—´ 2e-5ã€‚æˆ‘å°è¯•è¿‡ 5e-5ï¼Œå‘ç° Loss éœ‡è¡æ¯”è¾ƒå¤§ï¼›å°è¯•è¿‡ 1e-5ï¼Œæ”¶æ•›å¤ªæ…¢ã€‚2e-5 æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½çš„å¹³è¡¡ç‚¹ã€‚â€*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c7d72",
   "metadata": {},
   "source": [
    "## 4 âš”ï¸ æ¯æ—¥ç®—æ³•é¢˜ (LeetCode 8)\n",
    "\n",
    "æ—¢ç„¶æˆ‘ä»¬åœ¨å¤„ç† NLP æ„å›¾è¯†åˆ«ï¼ˆæŠŠå­—ç¬¦ä¸²å˜æˆæŒ‡ä»¤ï¼‰ï¼Œä»Šå¤©æˆ‘ä»¬æ¥åšä¸€é“æå…¶ç»å…¸çš„**å­—ç¬¦ä¸²è§£æ (String Parsing)** é¢˜ç›®ã€‚è¿™é“é¢˜è€ƒå¯Ÿçš„æ˜¯ä½ å¤„ç†â€œè„æ•°æ®â€å’Œè¾¹ç•Œæ¡ä»¶çš„ç»†å¿ƒç¨‹åº¦ï¼Œè¿™åœ¨ NLP é¢„å¤„ç†ä¸­éå¸¸é‡è¦ã€‚\n",
    "\n",
    "### ğŸ¯ é¢˜ç›®: [LeetCode 8. å­—ç¬¦ä¸²è½¬æ¢æ•´æ•° (atoi)](../../LeetCode%20practice/1-50.ipynb)\n",
    "\n",
    "  * **éš¾åº¦**: Medium\n",
    "  * **æ ‡ç­¾**: å­—ç¬¦ä¸²\n",
    "  * **æè¿°**: å®ç° `myAtoi(string s)` å‡½æ•°ï¼Œå°†å­—ç¬¦ä¸²è½¬æ¢æˆä¸€ä¸ª 32 ä½æœ‰ç¬¦å·æ•´æ•°ã€‚\n",
    "      * **æ­¥éª¤**:\n",
    "        1.  **ä¸¢å¼ƒå‰å¯¼ç©ºæ ¼**ã€‚\n",
    "        2.  **æ£€æŸ¥ç¬¦å·** (`+` æˆ– `-`)ã€‚\n",
    "        3.  **è¯»å…¥æ•°å­—**ï¼šç›´åˆ°åˆ°è¾¾éæ•°å­—å­—ç¬¦æˆ–ç»“å°¾ã€‚\n",
    "        4.  **æˆªæ–­**ï¼šå¦‚æœè¶…è¿‡ 32 ä½æ•´æ•°èŒƒå›´ $[-2^{31}, 2^{31}-1]$ï¼Œåˆ™æˆªæ–­ã€‚\n",
    "      * **ç¤ºä¾‹**: `\"   -42 with words\"` -\\> `-42`\n",
    "  * **ä¸ºä»€ä¹ˆé€‰è¿™é¢˜**: ä½ çš„â€œæŒ‡ä»¤è¯†åˆ«â€é¡¹ç›®ä¸­ï¼Œå¦‚æœç”¨æˆ·è¾“å…¥ \"å‘å·¦è½¬ 30 åº¦\"ï¼Œä½ éœ€è¦è§£æå‡º `30` è¿™ä¸ªå‚æ•°ã€‚è¿™é“é¢˜å°±æ˜¯å‚æ•°è§£æçš„ç®€åŒ–ç‰ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a50e40",
   "metadata": {},
   "source": [
    "## âœ… Day 4 ä»»åŠ¡æ¸…å•\n",
    "\n",
    "1.  **åæ ·æœ¬å¯è§†åŒ–**: è¿è¡Œä»£ç ï¼Œæ‰“å°å‡º Pandas è¡¨æ ¼ï¼Œäº²çœ¼çœ‹çœ‹æ¨¡å‹é”™åœ¨å“ªã€‚\n",
    "2.  **éªŒè¯ Weighted Loss**: é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè§‚å¯Ÿ `eval_recall` æ˜¯å¦æå‡ï¼Œç¡®è®¤æ¨¡å‹æ˜¯å¦æ•¢äºé¢„æµ‹ `GRAB` äº†ã€‚\n",
    "3.  **æ‰‹åŠ¨è°ƒå‚**: è‡³å°‘å°è¯•ä¿®æ”¹ä¸€æ¬¡ Learning Rateï¼Œå¹¶è®°å½•ç»“æœå¯¹æ¯”ã€‚\n",
    "4.  **ç®—æ³•**: é€šè¿‡ LeetCode 8ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
