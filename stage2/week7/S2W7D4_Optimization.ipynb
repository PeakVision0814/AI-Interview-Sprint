{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f8308a",
   "metadata": {},
   "source": [
    "# S2W7D4: è¯Šæ–­ä¸è°ƒä¼˜ (Diagnosis & Optimization)\n",
    "æ˜¨å¤©æˆ‘ä»¬ç»™æ¨¡å‹åšäº†ä¸€æ¬¡â€œä½“æ£€â€ï¼Œå‘ç°å®ƒè™½ç„¶åŠæ ¼äº†ï¼ˆAccuracy 60%ï¼‰ï¼Œä½†æ˜¯æœ‰ç‚¹â€œåç§‘â€ï¼ˆå¯¹ç¨€æœ‰æ ·æœ¬ Grab ä¸æ•¢é¢„æµ‹ï¼‰ã€‚åŒæ—¶æˆ‘ä»¬ä¿®æ”¹äº†ä»£ç ï¼Œå¼•å…¥äº† **Weighted Loss** è¯•å›¾çŸ«æ­£è¿™ä¸ªé—®é¢˜ã€‚\n",
    "\n",
    "ä»Šå¤©ï¼Œæˆ‘ä»¬è¦åŒ–èº«ä¸º **â€œAI åŒ»ç”Ÿâ€**ã€‚æˆ‘ä»¬ä¸èƒ½åªçœ‹åˆ†æ•°çš„æ¶¨è·Œï¼Œæˆ‘ä»¬è¦**çœ‹ç€ç—…äººçš„çœ¼ç›**ï¼ˆæŸ¥çœ‹é”™è¯¯æ ·æœ¬ï¼‰ï¼Œæ‰¾å‡ºå®ƒåˆ°åº•åœ¨å“ªé‡ŒçŠ¯ç³Šæ¶‚ï¼Œç„¶åé€šè¿‡**è°ƒå‚**ï¼ˆå¼€è¯ï¼‰æ¥æ²»å¥½å®ƒã€‚\n",
    "\n",
    "**è¿™æ˜¯é¢è¯•ä¸­â€œé¡¹ç›®æ·±æŒ–â€ç¯èŠ‚æœ€å®¹æ˜“è¢«é—®åˆ°çš„åœ°æ–¹**ï¼šâ€œä½ é‡åˆ°è¿‡ Bad Case å—ï¼Ÿä½ æ˜¯æ€ä¹ˆåˆ†æçš„ï¼Ÿæœ€åæ€ä¹ˆè§£å†³çš„ï¼Ÿâ€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe46ea",
   "metadata": {},
   "source": [
    "## 1 ğŸ” æ ¸å¿ƒä»»åŠ¡ï¼šå¯è§†åŒ–â€œåæ ·æœ¬â€ (Bad Case Analysis)\n",
    "\n",
    "Loss åªæ˜¯ä¸€ä¸ªæ•°å­—ï¼Œå®ƒä¸ä¼šå‘Šè¯‰ä½ æ¨¡å‹ä¸ºä»€ä¹ˆæŠŠâ€œå‘å·¦è½¬â€å¬æˆäº†â€œåœæ­¢â€ã€‚æˆ‘ä»¬éœ€è¦æŠŠé¢„æµ‹é”™è¯¯çš„æ ·æœ¬æ‰“å°å‡ºæ¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a2e975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ é”™è¯¯æ ·æœ¬åˆ†ææŠ¥å‘Š:\n",
      "|    | Text             | True Label   | Pred Label   |   Confidence |\n",
      "|---:|:-----------------|:-------------|:-------------|-------------:|\n",
      "|  0 | æŠ“èµ·æ¥           | GRAB         | MOVE         |     0.474486 |\n",
      "|  1 | è¯·å‹¿ç§»åŠ¨         | STOP         | MOVE         |     0.420997 |\n",
      "|  2 | æŠŠé‚£ä¸ªæ¯å­æ‹¿ç»™æˆ‘ | GRAB         | MOVE         |     0.462568 |\n",
      "|  3 | æ¾å¼€æ‰‹           | GRAB         | MOVE         |     0.48105  |\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\") \n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer\n",
    "from src.config import PRETRAINED_MODEL_DIR, CHECKPOINT_DIR\n",
    "from src.models.model_bert import BertClassifier\n",
    "from src.dataset.dataset import IntentDataset\n",
    "from src.metrics import compute_metrics\n",
    "\n",
    "# 1. å‡†å¤‡éªŒè¯é›† (è¿™æ¬¡å¤šé€ ä¸€ç‚¹éš¾çš„æ•°æ®ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯)\n",
    "val_texts = [\n",
    "    \"å‘å·¦è½¬\", \"åœæ­¢\", \"æŠ“èµ·æ¥\", \"è¯·å‹¿ç§»åŠ¨\", \"å‰è¿›\", \n",
    "    \"æŠŠé‚£ä¸ªæ¯å­æ‹¿ç»™æˆ‘\", \"ç´§æ€¥åˆ¶åŠ¨\", \"æ…¢æ…¢å¾€å‰èµ°\", \"å‘å³è½¬åŠ¨ä¸€ç‚¹ç‚¹\", \"æ¾å¼€æ‰‹\"\n",
    "]\n",
    "# å‡è®¾ 0:MOVE, 1:STOP, 2:GRAB\n",
    "val_labels = [0, 1, 2, 1, 0, 2, 1, 0, 0, 2]\n",
    "label_map = {0: \"MOVE\", 1: \"STOP\", 2: \"GRAB\"}\n",
    "\n",
    "# 2. åŠ è½½æ˜¨å¤©è®­ç»ƒå¥½çš„æ¨¡å‹ (Checkpoint-24)\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "model = BertClassifier(model_path=PRETRAINED_MODEL_DIR, num_labels=3)\n",
    "# ä¿®æ­£åŠ è½½æ–¹å¼ï¼Œä½¿ç”¨ safetensors\n",
    "from safetensors.torch import load_file\n",
    "# è¯·ç¡®è®¤ checkpoint-24 æ˜¯ä½ æ˜¨å¤©çš„æœ€ä½³æƒé‡ç›®å½•\n",
    "best_ckpt = CHECKPOINT_DIR / \"checkpoint-24\" \n",
    "state_dict = load_file(f\"{best_ckpt}/model.safetensors\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# 3. è¿›è¡Œé¢„æµ‹\n",
    "training_args = TrainingArguments(output_dir=\"./results\", per_device_eval_batch_size=4)\n",
    "trainer = Trainer(model=model, args=training_args)\n",
    "predictions = trainer.predict(val_dataset)\n",
    "\n",
    "# 4. æå–ç»“æœ\n",
    "pred_logits = predictions.predictions\n",
    "pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "true_ids = predictions.label_ids\n",
    "\n",
    "# 5. â˜…â˜…â˜… æ ¸å¿ƒï¼šæ‰¾å‡º Bad Cases â˜…â˜…â˜…\n",
    "bad_cases = []\n",
    "for i in range(len(val_texts)):\n",
    "    if pred_ids[i] != true_ids[i]:\n",
    "        bad_cases.append({\n",
    "            \"Text\": val_texts[i],\n",
    "            \"True Label\": label_map[true_ids[i]],\n",
    "            \"Pred Label\": label_map[pred_ids[i]],\n",
    "            \"Confidence\": np.max(torch.softmax(torch.tensor(pred_logits[i]), dim=-1).numpy())\n",
    "        })\n",
    "\n",
    "# 6. ç”¨ Pandas æ¼‚äº®åœ°æ‰“å°å‡ºæ¥\n",
    "df = pd.DataFrame(bad_cases)\n",
    "print(\"âŒ é”™è¯¯æ ·æœ¬åˆ†ææŠ¥å‘Š:\")\n",
    "if not df.empty:\n",
    "    print(df.to_markdown())\n",
    "else:\n",
    "    print(\"ğŸ‰ å¤ªæ£’äº†ï¼éªŒè¯é›†å…¨å¯¹ï¼(è¯´æ˜è¯¥å¢åŠ éš¾åº¦äº†)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd3f7f",
   "metadata": {},
   "source": [
    "**è¯Šæ–­æ€è€ƒ (Interview Point)**:\n",
    "\n",
    "  * å¦‚æœæ¨¡å‹æŠŠ \"è¯·å‹¿ç§»åŠ¨\" é¢„æµ‹æˆ `MOVE`ï¼šè¯´æ˜å®ƒåªå…³æ³¨åˆ°äº†å…³é”®è¯ \"ç§»åŠ¨\"ï¼Œæ²¡å­¦åˆ°å¦å®šè¯ \"è¯·å‹¿\"ã€‚ -\\> **å¯¹ç­–**ï¼šå¢åŠ å¸¦æœ‰å¦å®šè¯çš„è®­ç»ƒæ•°æ®ã€‚\n",
    "  * å¦‚æœæ¨¡å‹æŠŠ \"æŠ“èµ·æ¥\" é¢„æµ‹æˆ `MOVE`ï¼šè¯´æ˜å®ƒè¿˜æ²¡å­¦ä¼š `GRAB` è¿™ä¸ªç”Ÿåƒ»ç±»åˆ«ã€‚ -\\> **å¯¹ç­–**ï¼šæ˜¨å¤©åŠ çš„ Weighted Loss åº”è¯¥èƒ½ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07a8a2",
   "metadata": {},
   "source": [
    "## 2 ğŸ›ï¸ æ ¸å¿ƒä»»åŠ¡ï¼šå¯ç”¨ Weighted Loss é‡è®­\n",
    "\n",
    "æ˜¨å¤©æˆ‘ä»¬ä¿®æ”¹äº† `model_bert.py`ï¼Œä½†è¿˜æ²¡çœŸæ­£ç”¨å®ƒé‡æ–°è®­ç»ƒã€‚ä»Šå¤©æˆ‘ä»¬è¦ç”¨å¸¦æƒé‡çš„ Loss å†è·‘ä¸€æ¬¡ï¼Œçœ‹çœ‹èƒ½ä¸èƒ½æŠŠæ˜¨å¤© Precision=0 çš„å°´å°¬è§£å†³æ‰ã€‚\n",
    "\n",
    "### æ­¥éª¤\n",
    "\n",
    "1.  **é‡å¯ Kernel** (ç¡®ä¿ `src.models.model_bert` çš„ä¿®æ”¹ç”Ÿæ•ˆ)ã€‚\n",
    "2.  **å¤åˆ¶ Day 2 çš„è®­ç»ƒä»£ç ** (æˆ–è€…ç›´æ¥åœ¨ Day 4 Notebook é‡Œé‡å†™ä¸€é Trainer æµç¨‹)ã€‚\n",
    "3.  **è¿è¡Œè®­ç»ƒ**ã€‚\n",
    "4.  **å†æ¬¡è¯„ä¼°**ï¼šé‡ç‚¹çœ‹ `GRAB` ç±»åˆ«çš„ Recall æœ‰æ²¡æœ‰ä¸Šå‡ã€‚\n",
    "\n",
    "**ä»£ç ç‰‡æ®µ (Day 4 Notebook):**\n",
    "\n",
    "```python\n",
    "# ... å®šä¹‰ train_dataset (è®°å¾—åŒ…å«ä¸€äº› Label=2 çš„æ ·æœ¬) ...\n",
    "\n",
    "# é‡æ–°åˆå§‹åŒ–æ¨¡å‹ (è¿™æ—¶å€™å®ƒä¼šè¯»å–æˆ‘ä»¬åœ¨ init é‡Œå†™å¥½çš„ class_weights)\n",
    "model_weighted = BertClassifier(model_path=PRETRAINED_MODEL_DIR, num_labels=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_weighted,\n",
    "    args=training_args, # ç¨å¾®è°ƒå¤§ epoch è¯•è¯•ï¼Œæ¯”å¦‚ 5\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# è®­ç»ƒå®Œç«‹åˆ»è¯„ä¼°\n",
    "print(trainer.evaluate())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be7fe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†æ•°é‡: 18\n",
      "ç±»åˆ«åˆ†å¸ƒ: MOVE=12, STOP=3, GRAB=3\n",
      "âœ… æ¨¡å‹å·²åŠ è½½ï¼Œå·²å¯ç”¨ Weighted Loss (è¯·ç¡®ä¿ model_bert.py å·²ä¿®æ”¹å¹¶ä¿å­˜)\n",
      "\n",
      "ğŸš€ å¼€å§‹ Weighted Loss è®­ç»ƒ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.147100</td>\n",
       "      <td>1.072649</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>1.064112</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.736300</td>\n",
       "      <td>1.034247</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>0.926951</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.838171</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.391500</td>\n",
       "      <td>0.791867</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.741361</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.709983</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.298700</td>\n",
       "      <td>0.693440</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>0.686686</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goodminton/anaconda3/envs/utils/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/goodminton/anaconda3/envs/utils/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/goodminton/anaconda3/envs/utils/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/goodminton/anaconda3/envs/utils/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6866857409477234, 'eval_accuracy': 0.7142857142857143, 'eval_f1': 0.7142857142857143, 'eval_precision': 0.7619047619047619, 'eval_recall': 0.7142857142857143, 'eval_runtime': 0.0924, 'eval_samples_per_second': 75.742, 'eval_steps_per_second': 21.641, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\") \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer\n",
    "from src.config import PRETRAINED_MODEL_DIR, CHECKPOINT_DIR\n",
    "from src.models.model_bert import BertClassifier\n",
    "from src.dataset.dataset import IntentDataset\n",
    "from src.metrics import compute_metrics\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. æ„é€ â€œä¸å¹³è¡¡â€çš„è®­ç»ƒæ•°æ® (Imbalanced Data Construction)\n",
    "# -----------------------------------------------------------------------------\n",
    "# åœºæ™¯æ¨¡æ‹Ÿï¼š\n",
    "# Label 0 (MOVE): 12æ¡ (å å¤§å¤šæ•°ï¼Œæ¨¡å‹å®¹æ˜“å·æ‡’åªçŒœè¿™ä¸ª)\n",
    "# Label 1 (STOP): 3æ¡  (ç¨€æœ‰å…³é”®æŒ‡ä»¤)\n",
    "# Label 2 (GRAB): 3æ¡  (ç¨€æœ‰å…³é”®æŒ‡ä»¤)\n",
    "train_texts = [\n",
    "    # --- 0: MOVE (å¤§é‡) ---\n",
    "    \"å‘å‰èµ°\", \"å‘åé€€\", \"å·¦è½¬\", \"å³è½¬\", \"å‰è¿›åç±³\", \"åé€€äº”æ­¥\", \n",
    "    \"å¾€å·¦è¾¹ç§»åŠ¨\", \"å¾€å³è¾¹ç§»åŠ¨\", \"ä¸€ç›´èµ°\", \"æ…¢é€Ÿå‰è¿›\", \"å…¨é€Ÿå‰è¿›\", \"è½¬ä¸ªåœˆ\",\n",
    "    # --- 1: STOP (å°‘é‡) ---\n",
    "    \"åœä¸‹\", \"ç´§æ€¥åˆ¶åŠ¨\", \"åˆ«åŠ¨\",\n",
    "    # --- 2: GRAB (å°‘é‡) ---\n",
    "    \"æŠ“å–æ¯å­\", \"æŠŠè‹¹æœæ‹¿èµ·æ¥\", \"æ¾å¼€å¤¹çˆª\"\n",
    "]\n",
    "train_labels = [\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # 12ä¸ª 0\n",
    "    1, 1, 1,                             # 3ä¸ª 1\n",
    "    2, 2, 2                              # 3ä¸ª 2\n",
    "]\n",
    "\n",
    "# éªŒè¯é›† (ä¿æŒä¹‹å‰çš„å³å¯ï¼Œç”¨æ¥æµ‹è¯•æ³›åŒ–èƒ½åŠ›)\n",
    "val_texts = [\"å‘å·¦è½¬\", \"åœæ­¢\", \"æŠ“èµ·æ¥\", \"è¯·å‹¿ç§»åŠ¨\", \"å‰è¿›\", \"æŠŠé‚£ä¸ªæ¯å­æ‹¿ç»™æˆ‘\", \"æ¾å¼€æ‰‹\"]\n",
    "val_labels = [0, 1, 2, 1, 0, 2, 2]\n",
    "\n",
    "print(f\"è®­ç»ƒé›†æ•°é‡: {len(train_texts)}\")\n",
    "print(f\"ç±»åˆ«åˆ†å¸ƒ: MOVE={train_labels.count(0)}, STOP={train_labels.count(1)}, GRAB={train_labels.count(2)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. æ•°æ®å¤„ç† (Data Pipeline)\n",
    "# -----------------------------------------------------------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "train_dataset = IntentDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = IntentDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. åˆå§‹åŒ–æ¨¡å‹ (Model Initialization)\n",
    "# -----------------------------------------------------------------------------\n",
    "# â˜…â˜…â˜… å…³é”®ç‚¹ï¼šè¿™é‡Œåˆå§‹åŒ–æ—¶ï¼Œä¼šè°ƒç”¨ä½ åœ¨ model_bert.py é‡Œå†™çš„ class_weights â˜…â˜…â˜…\n",
    "model_weighted = BertClassifier(model_path=PRETRAINED_MODEL_DIR, num_labels=3)\n",
    "print(\"âœ… æ¨¡å‹å·²åŠ è½½ï¼Œå·²å¯ç”¨ Weighted Loss (è¯·ç¡®ä¿ model_bert.py å·²ä¿®æ”¹å¹¶ä¿å­˜)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. é…ç½®è®­ç»ƒå‚æ•° (Training Arguments)\n",
    "# -----------------------------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_weighted\", # æ¢ä¸ªè¾“å‡ºç›®å½•ï¼Œåˆ«è¦†ç›–ä¹‹å‰çš„\n",
    "    num_train_epochs=10,             # â˜… å¢åŠ  Epochï¼Œå› ä¸ºæ ·æœ¬å°‘ï¼Œå¤šå­¦å‡ è½®è®© Loss æ”¶æ•›\n",
    "    per_device_train_batch_size=4,   # Batch Size ç¨å¾®å¤§ä¸€ç‚¹ç‚¹\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=2e-5,              # é»„é‡‘å­¦ä¹ ç‡\n",
    "    logging_steps=5,\n",
    "    eval_strategy=\"epoch\",     # æ¯ä¸ª epoch æµ‹ä¸€æ¬¡\n",
    "    save_strategy=\"no\",              # å®éªŒé˜¶æ®µå…ˆä¸å­˜ checkpointï¼Œçœç©ºé—´\n",
    "    no_cuda=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. è®­ç»ƒä¸è¯„ä¼° (Train & Evaluate)\n",
    "# -----------------------------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model_weighted,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"\\nğŸš€ å¼€å§‹ Weighted Loss è®­ç»ƒ...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:\")\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77583326",
   "metadata": {},
   "source": [
    "## 3 ğŸ§ª è¿›é˜¶ä»»åŠ¡ï¼šè¶…å‚æ•°è°ƒä¼˜ (Hyperparameter Tuning)\n",
    "\n",
    "ç®—æ³•å·¥ç¨‹å¸ˆçš„æ—¥å¸¸å°±æ˜¯â€œç‚¼ä¸¹â€ã€‚ä½ éœ€è¦æ‰‹åŠ¨è°ƒæ•´ `TrainingArguments` é‡Œçš„å‚æ•°ï¼Œæ„Ÿå—å®ƒä»¬å¯¹ç»“æœçš„å½±å“ã€‚\n",
    "\n",
    "è¯·å°è¯•ä¿®æ”¹ä»¥ä¸‹å‚æ•°ï¼Œå¹¶è®°å½• Accuracy çš„å˜åŒ–ï¼š\n",
    "\n",
    "| å‚æ•° | å»ºè®®å°è¯•å€¼ | é¢„æœŸå½±å“ |\n",
    "| :--- | :--- | :--- |\n",
    "| **Learning Rate** | `5e-5` (å˜å¤§) | æ”¶æ•›å˜å¿«ï¼Œä½†å¯èƒ½éœ‡è¡ä¸æ”¶æ•› (Loss ä¹±è·³)ã€‚ |\n",
    "| **Learning Rate** | `1e-5` (å˜å°) | æ”¶æ•›ææ…¢ï¼Œéœ€è¦æ›´å¤š Epochï¼Œä½†ç»“æœå¯èƒ½æ›´ç²¾ç»†ã€‚ |\n",
    "| **Batch Size** | `4` æˆ– `8` | æ¢¯åº¦æ›´ç¨³å®šï¼Œä½†æ˜¾å­˜å ç”¨å¢åŠ ã€‚ |\n",
    "| **Num Epochs** | `5` æˆ– `10` | è®­ç»ƒæ›´ä¹…ï¼Œå°å¿ƒè¿‡æ‹Ÿåˆ (Val Loss å‡é«˜)ã€‚ |\n",
    "\n",
    "**é¢è¯•å¿…é—®**: *â€œä½ åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨äº†ä»€ä¹ˆ Learning Rateï¼Ÿä¸ºä»€ä¹ˆï¼Ÿâ€*\n",
    "**æ ‡å‡†ç­”æ¡ˆ**: *â€œæˆ‘ä½¿ç”¨äº† BERT å¾®è°ƒçš„é»„é‡‘åŒºé—´ 2e-5ã€‚æˆ‘å°è¯•è¿‡ 5e-5ï¼Œå‘ç° Loss éœ‡è¡æ¯”è¾ƒå¤§ï¼›å°è¯•è¿‡ 1e-5ï¼Œæ”¶æ•›å¤ªæ…¢ã€‚2e-5 æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½çš„å¹³è¡¡ç‚¹ã€‚â€*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c7d72",
   "metadata": {},
   "source": [
    "## 4 âš”ï¸ æ¯æ—¥ç®—æ³•é¢˜ (LeetCode 8)\n",
    "\n",
    "æ—¢ç„¶æˆ‘ä»¬åœ¨å¤„ç† NLP æ„å›¾è¯†åˆ«ï¼ˆæŠŠå­—ç¬¦ä¸²å˜æˆæŒ‡ä»¤ï¼‰ï¼Œä»Šå¤©æˆ‘ä»¬æ¥åšä¸€é“æå…¶ç»å…¸çš„**å­—ç¬¦ä¸²è§£æ (String Parsing)** é¢˜ç›®ã€‚è¿™é“é¢˜è€ƒå¯Ÿçš„æ˜¯ä½ å¤„ç†â€œè„æ•°æ®â€å’Œè¾¹ç•Œæ¡ä»¶çš„ç»†å¿ƒç¨‹åº¦ï¼Œè¿™åœ¨ NLP é¢„å¤„ç†ä¸­éå¸¸é‡è¦ã€‚\n",
    "\n",
    "### ğŸ¯ é¢˜ç›®: [LeetCode 8. å­—ç¬¦ä¸²è½¬æ¢æ•´æ•° (atoi)](../../LeetCode%20practice/1-50.ipynb)\n",
    "\n",
    "  * **éš¾åº¦**: Medium\n",
    "  * **æ ‡ç­¾**: å­—ç¬¦ä¸²\n",
    "  * **æè¿°**: å®ç° `myAtoi(string s)` å‡½æ•°ï¼Œå°†å­—ç¬¦ä¸²è½¬æ¢æˆä¸€ä¸ª 32 ä½æœ‰ç¬¦å·æ•´æ•°ã€‚\n",
    "      * **æ­¥éª¤**:\n",
    "        1.  **ä¸¢å¼ƒå‰å¯¼ç©ºæ ¼**ã€‚\n",
    "        2.  **æ£€æŸ¥ç¬¦å·** (`+` æˆ– `-`)ã€‚\n",
    "        3.  **è¯»å…¥æ•°å­—**ï¼šç›´åˆ°åˆ°è¾¾éæ•°å­—å­—ç¬¦æˆ–ç»“å°¾ã€‚\n",
    "        4.  **æˆªæ–­**ï¼šå¦‚æœè¶…è¿‡ 32 ä½æ•´æ•°èŒƒå›´ $[-2^{31}, 2^{31}-1]$ï¼Œåˆ™æˆªæ–­ã€‚\n",
    "      * **ç¤ºä¾‹**: `\"   -42 with words\"` -\\> `-42`\n",
    "  * **ä¸ºä»€ä¹ˆé€‰è¿™é¢˜**: ä½ çš„â€œæŒ‡ä»¤è¯†åˆ«â€é¡¹ç›®ä¸­ï¼Œå¦‚æœç”¨æˆ·è¾“å…¥ \"å‘å·¦è½¬ 30 åº¦\"ï¼Œä½ éœ€è¦è§£æå‡º `30` è¿™ä¸ªå‚æ•°ã€‚è¿™é“é¢˜å°±æ˜¯å‚æ•°è§£æçš„ç®€åŒ–ç‰ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a50e40",
   "metadata": {},
   "source": [
    "## âœ… Day 4 ä»»åŠ¡æ¸…å•\n",
    "\n",
    "1.  **åæ ·æœ¬å¯è§†åŒ–**: è¿è¡Œä»£ç ï¼Œæ‰“å°å‡º Pandas è¡¨æ ¼ï¼Œäº²çœ¼çœ‹çœ‹æ¨¡å‹é”™åœ¨å“ªã€‚\n",
    "2.  **éªŒè¯ Weighted Loss**: é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè§‚å¯Ÿ `eval_recall` æ˜¯å¦æå‡ï¼Œç¡®è®¤æ¨¡å‹æ˜¯å¦æ•¢äºé¢„æµ‹ `GRAB` äº†ã€‚\n",
    "3.  **æ‰‹åŠ¨è°ƒå‚**: è‡³å°‘å°è¯•ä¿®æ”¹ä¸€æ¬¡ Learning Rateï¼Œå¹¶è®°å½•ç»“æœå¯¹æ¯”ã€‚\n",
    "4.  **ç®—æ³•**: é€šè¿‡ LeetCode 8ã€‚\n",
    "\n",
    "å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹è¯Šæ–­ï¼è¯·å›å¤ **â€œStart Day 4â€**ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
