{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64d51d0",
   "metadata": {},
   "source": [
    "# S2W7D2: æ‹¥æŠ± Trainer API (The Industrial Standard)\n",
    "\n",
    "æ˜¨å¤©æˆ‘ä»¬é€ å¥½äº†â€œè½¦â€ï¼ˆæ¨¡å‹ç»“æ„ `BertClassifier`ï¼‰ï¼Œä»Šå¤©æˆ‘ä»¬è¦ç»™å®ƒè£…ä¸Šâ€œå¼•æ“â€ï¼ˆ`Trainer`ï¼‰å¹¶æ³¨å…¥â€œç‡ƒæ–™â€ï¼ˆæ•°æ®ï¼‰ï¼Œè®©å®ƒçœŸæ­£è·‘èµ·æ¥ã€‚\n",
    "\n",
    "åœ¨æ­¤ä¹‹å‰ï¼Œå¾ˆå¤šåŒå­¦ï¼ˆåŒ…æ‹¬ä»¥å‰çš„æˆ‘ï¼‰å–œæ¬¢æ‰‹å†™ `for batch in dataloader:` å¾ªç¯ã€‚ä½†åœ¨å¤§å‚ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å‡ ä¹å…¨éƒ¨è½¬å‘äº† **Hugging Face Trainer API**ã€‚\n",
    "\n",
    "ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºæ‰‹å†™å¾ªç¯è¦è‡ªå·±å¤„ç†ï¼š\n",
    "\n",
    "  * âŒ åˆ†å¸ƒå¼è®­ç»ƒ (Multi-GPU)\n",
    "  * âŒ æ¢¯åº¦ç´¯ç§¯ (Gradient Accumulation)\n",
    "  * âŒ æ··åˆç²¾åº¦è®­ç»ƒ (FP16)\n",
    "  * âŒ æ—¥å¿—è®°å½•ä¸æ–­ç‚¹ç»­è®­\n",
    "\n",
    "è€Œ Trainer æŠŠè¿™äº›è„æ´»ç´¯æ´»å…¨åŒ…äº†ã€‚æˆ‘ä»¬éœ€è¦æŒæ¡è¿™ä¸ª**å·¥ä¸šç•Œæ ‡å‡†**ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125895c",
   "metadata": {},
   "source": [
    "## 1 ğŸ› ï¸ æ ¸å¿ƒç»„ä»¶ï¼šæ„å»ºæ•°æ®é›† (Dataset)\n",
    "\n",
    "Traineréœ€è¦åƒæ•°æ®ã€‚ä¸ºäº†ç¬¦åˆæˆ‘ä»¬å®šä¹‰çš„å·¥ä¸šçº§ç›®å½•ç»“æ„ï¼Œè¯·åœ¨ `src/` ä¸‹æ–°å»º `dataset.py`ã€‚æˆ‘ä»¬å°†æŠŠæ•°æ®å¤„ç†é€»è¾‘å°è£…åœ¨è¿™é‡Œï¼Œä¿æŒä¸»æµç¨‹å¹²å‡€ã€‚\n",
    "\n",
    "**æ–‡ä»¶è·¯å¾„**: `project_root/src/dataset.py`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        \"\"\"\n",
    "        :param texts: æ–‡æœ¬åˆ—è¡¨ [\"æ‰“å¼€ç¯\", \"å‘å·¦è½¬\"]\n",
    "        :param labels: æ ‡ç­¾åˆ—è¡¨ [0, 1]\n",
    "        :param tokenizer: åˆ†è¯å™¨å®ä¾‹\n",
    "        :param max_len: æœ€å¤§æˆªæ–­é•¿åº¦\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length', # æ³¨æ„ï¼šè¿™é‡Œæš‚æ—¶ç”¨ max_lengthï¼Œåé¢ä¼šè®²åŠ¨æ€ padding\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # Trainer è¦æ±‚è¿”å›å­—å…¸ï¼Œä¸” key å¿…é¡»èƒ½å¯¹åº”ä¸Šæ¨¡å‹ forward çš„å‚æ•°å\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b662f",
   "metadata": {},
   "source": [
    "## 2 ğŸ’» ç¬”è®°æœ¬å®æˆ˜ï¼šè·‘é€šè®­ç»ƒæµç¨‹\n",
    "\n",
    "### 2.1 ç¯å¢ƒé…ç½®ä¸æ¨¡æ‹Ÿæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cc0291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å¤„ç†æ•°æ®...\n",
      "æ ·æœ¬ç¤ºä¾‹: {'input_ids': tensor([ 101, 1403, 1184, 6624,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer\n",
    "from src.config import PRETRAINED_MODEL_DIR, CHECKPOINT_DIR, LOG_DIR\n",
    "from src.models.model_bert import BertClassifier\n",
    "from src.dataset.dataset import IntentDataset # å‡è®¾ä½ åˆšæ‰æŠŠ dataset.py æ”¾åœ¨ src/dataset/ ä¸‹ï¼Œæˆ–è€…ç›´æ¥æ”¾åœ¨ src/ ä¸‹\n",
    "\n",
    "# 1. æ¨¡æ‹Ÿä¸€ç‚¹â€œæœºå™¨äººæŒ‡ä»¤â€æ•°æ® (ç”¨äºè·‘é€šæµç¨‹)\n",
    "train_texts = [\n",
    "    \"å‘å‰èµ°\", \"å‘åé€€\", \"å·¦è½¬\", \"å³è½¬\", \"åœä¸‹\", \"åˆ«åŠ¨\", \"æŠ“å–æ¯å­\", \"æ”¾ä¸‹ç‰©ä½“\",\n",
    "    \"å‰è¿›åç±³\", \"åé€€äº”æ­¥\", \"å‘å·¦çœ‹\", \"å‘å³çœ‹\", \"åœæ­¢ç§»åŠ¨\", \"ç´§æ€¥åˆ¶åŠ¨\", \"å¼€å¯å¤¹çˆª\", \"æ¾å¼€å¤¹çˆª\"\n",
    "]\n",
    "# å‡è®¾ 0:ç§»åŠ¨, 1:åœæ­¢, 2:æ“ä½œ\n",
    "train_labels = [0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 1, 2, 2]\n",
    "\n",
    "# 2. åˆå§‹åŒ– Tokenizer å’Œ Dataset\n",
    "print(\"æ­£åœ¨å¤„ç†æ•°æ®...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "# å®ä¾‹åŒ–æˆ‘ä»¬åœ¨ src/dataset.py é‡Œå†™çš„ç±»\n",
    "train_dataset = IntentDataset(train_texts, train_labels, tokenizer)\n",
    "\n",
    "# æ£€æŸ¥ä¸€ä¸‹æ•°æ®é•¿ä»€ä¹ˆæ · (Debug ä¹ æƒ¯)\n",
    "print(f\"æ ·æœ¬ç¤ºä¾‹: {train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cae295",
   "metadata": {},
   "source": [
    "### 2.2 æ ¸å¿ƒé…ç½® (TrainingArguments) â€”â€” é‡ç‚¹ï¼\n",
    "\n",
    "è¿™æ˜¯é¢è¯•ä¸­ä¼šé—®â€œä½ æ€ä¹ˆè°ƒå‚â€çš„åœ°æ–¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0817d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒå‚æ•°é…ç½®å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(CHECKPOINT_DIR),  # æƒé‡ä¿å­˜è·¯å¾„ (src/config.py å®šä¹‰å¥½çš„)\n",
    "    \n",
    "    # --- æ ¸å¿ƒè¶…å‚æ•° ---\n",
    "    num_train_epochs=3,              # è·‘å‡ è½®\n",
    "    per_device_train_batch_size=2,   # æ˜¾å­˜å°å°±è®¾å°ç‚¹ (2, 4, 8, 16)\n",
    "    learning_rate=2e-5,              # BERT å¾®è°ƒé»„é‡‘å­¦ä¹ ç‡ (1e-5 ~ 5e-5)\n",
    "    \n",
    "    # --- ä¼˜åŒ–ç­–ç•¥ ---\n",
    "    logging_steps=2,                 # å¤šå°‘æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    save_strategy=\"epoch\",           # æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡æƒé‡\n",
    "    seed=42,                         # è®¾å®šéšæœºç§å­ï¼Œä¿è¯å¯å¤ç°\n",
    "    \n",
    "    # --- ç¡¬ä»¶è®¾ç½® ---\n",
    "    no_cuda=False,                   # æœ‰ GPU è‚¯å®šç”¨ GPU\n",
    ")\n",
    "\n",
    "print(\"è®­ç»ƒå‚æ•°é…ç½®å®Œæˆã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1450cd",
   "metadata": {},
   "source": [
    "### ç¬¬ä¸‰æ­¥ï¼šç‚¹ç«èµ·é£ (Trainer Loop)\n",
    "\n",
    "æ³¨æ„ï¼šå› ä¸ºæˆ‘ä»¬çš„ `BertClassifier` è¾“å‡ºçš„æ˜¯ `logits` è€Œä¸æ˜¯ `loss`ï¼ˆåŸç”Ÿ BERT è¾“å‡ºä¼šåŒ…å« lossï¼‰ï¼ŒTrainer é»˜è®¤éœ€è¦æ¨¡å‹è¿”å› `loss`ã€‚\n",
    "\n",
    "  * **æ–¹æ³• A**: ä¿®æ”¹ `model_bert.py`ï¼Œåœ¨ forward é‡Œè®¡ç®— lossã€‚\n",
    "  * **æ–¹æ³• B (æ¨è)**: è‡ªå®šä¹‰ `compute_loss` (ç¨å¾®é«˜çº§ä¸€ç‚¹ï¼Œä½†æˆ‘ä»¬ä»Šå¤©å…ˆç”¨ç®€å•çš„æ–¹æ³•)ã€‚\n",
    "\n",
    "**ä¿®æ”¹ `src/models/model_bert.py` çš„ Forward å‡½æ•° (Day 1 ä»£ç è¡¥å……)ï¼š**\n",
    "ä¸ºäº†é…åˆ Trainerï¼Œæˆ‘ä»¬éœ€è¦è®©æ¨¡å‹è‡ªå·±è®¡ç®— Lossã€‚è¯·å›åˆ° `src/models/model_bert.py`ï¼Œä¿®æ”¹ `forward` éƒ¨åˆ†ï¼š\n",
    "\n",
    "```python\n",
    "    # ä¿®æ”¹ src/models/model_bert.py\n",
    "    def forward(self, input_ids, attention_mask, labels=None, token_type_ids=None): # æŠŠ token_type_ids åŠ å›æ¥ï¼Œè™½ç„¶æœ‰æ—¶å€™ä¸ç”¨ï¼Œä½†ä¸ºäº†å…¼å®¹æ€§æœ€å¥½åŠ ä¸Š\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooler_output = outputs.pooler_output\n",
    "        pooler_output = self.dropout(pooler_output)\n",
    "        logits = self.classifier(pooler_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # view(-1) å°† labels å±•å¹³æˆä¸€ç»´å‘é‡ [batch_size]ï¼Œé˜²æ­¢å‡ºç° [batch_size, 1] çš„æƒ…å†µ\n",
    "            # è¿™è¡Œä»£ç æ˜¯ä¿®å¤è¿™ä¸ªæŠ¥é”™çš„æ ¸å¿ƒï¼\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "            \n",
    "            return (loss, logits)\n",
    "        \n",
    "        return logits\n",
    "```\n",
    "\n",
    "*(è¯·åŠ¡å¿…å…ˆå»ä¿®æ”¹ `.py` æ–‡ä»¶ï¼Œå†è¿è¡Œä¸‹é¢çš„ Notebook ä»£ç )*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c8eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1358/664258046.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹è®­ç»ƒ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 00:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.302100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.871700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.698800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.970100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=0.938076933224996, metrics={'train_runtime': 14.4898, 'train_samples_per_second': 3.313, 'train_steps_per_second': 1.656, 'total_flos': 0.0, 'train_loss': 0.938076933224996, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertClassifier(model_path=PRETRAINED_MODEL_DIR, num_labels=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ å¼€å§‹è®­ç»ƒ...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1099e0",
   "metadata": {},
   "source": [
    "## 3 ğŸ’¥ é¢è¯•æ·±æŒ–ï¼šDynamic Padding ä¸ DataCollator\n",
    "\n",
    "é¢è¯•å®˜çœ‹ç€ä½ çš„ä»£ç é—®ï¼š*â€œæˆ‘åœ¨ä½ çš„Dataseté‡Œçœ‹åˆ°äº† `padding='max_length'`ï¼Œè¿™æ„å‘³ç€æ‰€æœ‰å¥å­éƒ½è¢«å¼ºåˆ¶è¡¥é½åˆ°äº† 128 é•¿åº¦ã€‚å¦‚æœå¤§éƒ¨åˆ†å¥å­åªæœ‰ 10 ä¸ªå­—ï¼Œè¿™æ˜¯ä¸æ˜¯æå¤§çš„æµªè´¹ï¼Ÿâ€*\n",
    "\n",
    "  * **Static Padding (é™æ€è¡¥é½)**:\n",
    "      * Dataset é‡Œå†™æ­» `max_len=128`ã€‚\n",
    "      * å¥å­ \"Stop\" (é•¿åº¦1) -\\> è¡¥ 127 ä¸ª 0ã€‚\n",
    "      * **ç¼ºç‚¹**: æ˜¾å­˜æµªè´¹ï¼Œè®¡ç®—æµªè´¹ã€‚\n",
    "  * **Dynamic Padding (åŠ¨æ€è¡¥é½)**:\n",
    "      * Dataset é‡Œ**ä¸è¡¥é½** (åªæˆªæ–­)ã€‚\n",
    "      * åœ¨ `DataLoader` å–å‡ºä¸€ä¸ª Batch çš„æ—¶å€™ï¼Œçœ‹è¿™ä¸ª Batch é‡Œæœ€é•¿çš„å¥å­æ˜¯å¤šé•¿ï¼ˆæ¯”å¦‚ 15ï¼‰ï¼Œç„¶åæŠŠå…¶ä»–å¥å­è¡¥é½åˆ° 15ã€‚\n",
    "      * **ä¼˜ç‚¹**: æå¤§åœ°èŠ‚çœæ˜¾å­˜ï¼Œè®­ç»ƒé€Ÿåº¦å¿« 30% ä»¥ä¸Šã€‚\n",
    "\n",
    "**ğŸ”§ ä¼˜åŒ–ä»£ç  (åŠ åˆ†é¡¹)**:\n",
    "åœ¨ `Trainer` ä¸­ä½¿ç”¨ `DataCollatorWithPadding` å®ç°åŠ¨æ€è¡¥é½ã€‚\n",
    "\n",
    "```python\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Dataset é‡Œå»æ‰ padding='max_length'\n",
    "# encoding = tokenizer(..., padding=False, ...)\n",
    "\n",
    "# Trainer é‡ŒåŠ ä¸Š\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(..., data_collator=data_collator)\n",
    "```\n",
    "\n",
    "*(ä»Šå¤©å…ˆè·‘é€šé™æ€ Padding å³å¯ï¼Œæ˜å¤©ä¼˜åŒ–æ—¶æˆ‘ä»¬å†ç»†è®²è¿™ä¸ª)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e11c8f",
   "metadata": {},
   "source": [
    "## 4\\. âš”ï¸ æ¯æ—¥ç®—æ³•é¢˜ (LeetCode 5)\n",
    "\n",
    "æ—¢ç„¶æ˜¯å¤„ç†åºåˆ—ï¼Œä»Šå¤©æŒ‘æˆ˜ç»å…¸çš„â€œå›æ–‡ä¸²â€ã€‚\n",
    "\n",
    "### ğŸ¯ é¢˜ç›®: [LeetCode 5](../../LeetCode%20practice/1-50.ipynb). æœ€é•¿å›æ–‡å­ä¸² (Longest Palindromic Substring)\n",
    "\n",
    "  * **éš¾åº¦**: Medium\n",
    "  * **æè¿°**: ç»™ä½ ä¸€ä¸ªå­—ç¬¦ä¸² `s`ï¼Œæ‰¾åˆ° `s` ä¸­æœ€é•¿çš„å›æ–‡å­ä¸²ã€‚\n",
    "      * è¾“å…¥: `babad` -\\> è¾“å‡º: `bab` æˆ– `aba`\n",
    "  * **æ€è·¯ (ä¸­å¿ƒæ‰©æ•£æ³•)**:\n",
    "      * å›æ–‡ä¸²æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿå®ƒæ˜¯**é•œåƒ**çš„ã€‚\n",
    "      * éå†å­—ç¬¦ä¸²æ¯ä¸ªå­—ç¬¦ï¼Œä»¥æ­¤ä¸º**ä¸­å¿ƒ**ï¼Œå‘å·¦å³ä¸¤è¾¹æ‰©æ•£ï¼Œç›´åˆ°ä¸¤è¾¹å­—ç¬¦ä¸ä¸€æ ·ä¸ºæ­¢ã€‚\n",
    "      * **æ³¨æ„**: ä¸­å¿ƒå¯èƒ½æ˜¯ä¸€ä¸ªå­—ç¬¦ (aba)ï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸¤ä¸ªå­—ç¬¦ (abba)ã€‚\n",
    "\n",
    "<!-- end list -->\n",
    "\n",
    "```python\n",
    "# ä¼ªä»£ç æ€è·¯\n",
    "for i in range(len(s)):\n",
    "    len1 = expand(center=i)      # å¥‡æ•°é•¿åº¦\n",
    "    len2 = expand(center=i, i+1) # å¶æ•°é•¿åº¦\n",
    "    max_len = max(len1, len2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc560611",
   "metadata": {},
   "source": [
    "\n",
    "### âœ… Day 2 ä»»åŠ¡æ¸…å•\n",
    "\n",
    "1.  **æ–°å»ºæ–‡ä»¶**: `src/dataset.py`ï¼Œå®ç° `IntentDataset`ã€‚\n",
    "2.  **ä¿®æ”¹æ¨¡å‹**: æ›´æ–° `model_bert.py` çš„ `forward` å‡½æ•°ï¼ŒåŠ å…¥ loss è®¡ç®—é€»è¾‘ï¼ˆè¿™æ˜¯ Trainer è¿è¡Œçš„å‰æï¼‰ã€‚\n",
    "3.  **è¿è¡Œ Notebook**: è·‘é€š `S2W7D2_Trainer_Flow.ipynb`ï¼Œçœ‹åˆ°è¿›åº¦æ¡è·‘å®Œï¼Œæ²¡æœ‰æŠ¥é”™ã€‚\n",
    "4.  **ç®—æ³•**: é€šè¿‡ LeetCode 5ã€‚\n",
    "\n",
    "**ç‰¹åˆ«æé†’**: ä¿®æ”¹ `.py` æ–‡ä»¶åï¼ŒNotebook å¦‚æœå·²ç» import è¿‡ï¼Œ**å¿…é¡» Restart Kernel** æ‰èƒ½ç”Ÿæ•ˆï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
