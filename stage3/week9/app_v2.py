import os
# å‡è®¾ä½ ä¹‹å‰å†™å¥½äº†è¿™äº›æ¨¡å—
# from embedding_utils import get_embedding
# from vector_db_utils import search_similar_chunks
# from llm_utils import call_llm_api

# --- æ¨¡æ‹Ÿæ¨¡å— (å¦‚æœä½ è¿˜æ²¡æœ‰å°è£…å¥½ï¼Œè¯·æ›¿æ¢ä¸ºçœŸå®è°ƒç”¨) ---
def mock_get_embedding(text):
    # å®é™…åº”ç”¨ä¸­è°ƒç”¨ OpenAI/HuggingFace æ¥å£
    return [0.1, 0.2, 0.3] 

def mock_vector_search(query_vec, top_k=3):
    # å®é™…åº”ç”¨ä¸­åœ¨ä½ çš„å‘é‡æ•°æ®åº“(FAISS/Chroma/List)ä¸­æŸ¥æ‰¾
    # è¿™é‡Œæˆ‘ä»¬ç¡¬ç¼–ç ï¼Œæ¨¡æ‹Ÿé’ˆå¯¹"ç›—çªƒç½ªæ•°é¢å·¨å¤§"çš„æ£€ç´¢ç»“æœ
    return [
        "ã€Šæœ€é«˜äººæ°‘æ³•é™¢ã€æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢å…³äºåŠç†ç›—çªƒåˆ‘äº‹æ¡ˆä»¶é€‚ç”¨æ³•å¾‹è‹¥å¹²é—®é¢˜çš„è§£é‡Šã€‹ç¬¬ä¸€æ¡ï¼šç›—çªƒå…¬ç§è´¢ç‰©ä»·å€¼ä¸€åƒå…ƒè‡³ä¸‰åƒå…ƒä»¥ä¸Šã€ä¸‰ä¸‡å…ƒè‡³åä¸‡å…ƒä»¥ä¸Šã€ä¸‰åä¸‡å…ƒè‡³äº”åä¸‡å…ƒä»¥ä¸Šçš„ï¼Œåº”å½“åˆ†åˆ«è®¤å®šä¸ºåˆ‘æ³•ç¬¬äºŒç™¾å…­åå››æ¡è§„å®šçš„â€œæ•°é¢è¾ƒå¤§â€ã€â€œæ•°é¢å·¨å¤§â€ã€â€œæ•°é¢ç‰¹åˆ«å·¨å¤§â€ã€‚",
        "åˆ‘æ³•ç¬¬äºŒç™¾å…­åå››æ¡ï¼šç›—çªƒå…¬ç§è´¢ç‰©ï¼Œæ•°é¢è¾ƒå¤§çš„ï¼Œæˆ–è€…å¤šæ¬¡ç›—çªƒã€å…¥æˆ·ç›—çªƒã€æºå¸¦å‡¶å™¨ç›—çªƒã€æ‰’çªƒçš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ï¼Œå¹¶å¤„æˆ–è€…å•å¤„ç½šé‡‘...",
    ]

def call_llm(system_prompt, user_prompt):
    # è¿™é‡Œè°ƒç”¨ä½ çš„ LLM (DeepSeek/GPT/Gemini)
    # ä¸‹é¢æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ‰“å°ï¼Œå±•ç¤ºå‘é€ç»™ LLM çš„æœ€ç»ˆ Prompt é•¿ä»€ä¹ˆæ ·
    print("\n" + "="*20 + " DEBUG: å‘é€ç»™ LLM çš„ Prompt " + "="*20)
    print(f"ã€Systemã€‘: {system_prompt}")
    print(f"ã€Userã€‘: {user_prompt}")
    print("="*60 + "\n")
    
    # æ¨¡æ‹Ÿ LLM çš„å›ç­”
    return "æ ¹æ®å¸æ³•è§£é‡Šï¼Œç›—çªƒå…¬ç§è´¢ç‰©ä»·å€¼ä¸‰ä¸‡å…ƒè‡³åä¸‡å…ƒä»¥ä¸Šçš„ï¼Œåº”å½“è®¤å®šä¸ºâ€œæ•°é¢å·¨å¤§â€ã€‚"

# --- æ ¸å¿ƒ RAG æµç¨‹ ---

def main():
    print("âš–ï¸  æ³•å¾‹å°åŠ©æ‰‹ v0.2 (RAG Enabled) - è¾“å…¥ 'exit' é€€å‡º")
    
    # 1. å®šä¹‰ System Prompt (å¢åŠ ä¸¥æ ¼é™åˆ¶)
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹åŠ©æ‰‹ã€‚"
        "è¯·ä»…æ ¹æ®ä»¥ä¸‹æä¾›çš„ã€å‚è€ƒä¸Šä¸‹æ–‡ã€‘å›ç­”é—®é¢˜ï¼Œ"
        "å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯´ä¸çŸ¥é“ï¼Œä¸¥ç¦ç¼–é€ ã€‚"
    )

    while True:
        user_query = input("\nè¯·è¾“å…¥æ³•å¾‹é—®é¢˜: ")
        if user_query.strip().lower() == 'exit':
            break

        print("æ­£åœ¨æ£€ç´¢ç›¸å…³æ³•å¾‹æ¡æ–‡...")

        # 2. Embedding (User Input -> Vector)
        query_vector = mock_get_embedding(user_query)

        # 3. Vector Search (Vector -> Top-K Chunks)
        # è¿™ä¸€æ­¥æ˜¯ RAG çš„å…³é”®ï¼šåªæŠŠç›¸å…³çš„çŸ¥è¯†æ‹¿å‡ºæ¥
        relevant_chunks = mock_vector_search(query_vector, top_k=2)
        
        # å°†æ£€ç´¢åˆ°çš„æ–‡æœ¬æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²
        context_str = "\n\n".join(relevant_chunks)

        # 4. Construct Prompt (System + Context + User Input)
        # æˆ‘ä»¬æŠŠæ£€ç´¢åˆ°çš„å†…å®¹å¡ç»™ç”¨æˆ· Promptï¼Œæˆ–è€…æ”¾åœ¨ System Prompt é‡Œéƒ½å¯ä»¥
        # è¿™é‡Œé‡‡ç”¨å¸¸è§çš„ç»“æ„ï¼š
        final_user_prompt = f"ã€å‚è€ƒä¸Šä¸‹æ–‡ã€‘:\n{context_str}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘: {user_query}"

        # 5. LLM Generation
        answer = call_llm(system_prompt, final_user_prompt)
        
        print(f"ğŸ¤– å›ç­”: {answer}")

if __name__ == "__main__":
    main()