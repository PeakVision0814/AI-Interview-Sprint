{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "972ba147",
   "metadata": {},
   "source": [
    "# S3W9D4: 法律文档 ETL 管道 (The Pipeline)\n",
    "\n",
    "**今日目标**：\n",
    "编写 `src/rag/ingest.py`，实现 **Extract (读取) -> Transform (切分+清洗) -> Load (向量化+存储)** 的全自动化流程。\n",
    "\n",
    "**1准备数据文件**\n",
    "请在你的项目根目录下创建 `data` 文件夹，并放入一个测试用的法律文档。\n",
    "\n",
    "```bash\n",
    "mkdir -p data\n",
    "# 创建一个模拟的刑法文本文件\n",
    "touch data/penal_code.txt\n",
    "\n",
    "```\n",
    "\n",
    "请将以下内容复制到 `data/penal_code.txt` 中（这是真实刑法的片段）：\n",
    "\n",
    "```text\n",
    "第二百六十四条 【盗窃罪】\n",
    "盗窃公私财物，数额较大的，或者多次盗窃、入户盗窃、携带凶器盗窃、扒窃的，处三年以下有期徒刑、拘役或者管制，并处或者单处罚金；数额巨大或者有其他严重情节的，处三年以上十年以下有期徒刑，并处罚金；数额特别巨大或者有其他特别严重情节的，处十年以上有期徒刑或者无期徒刑，并处罚金或者没收财产。\n",
    "\n",
    "第二百六十六条 【诈骗罪】\n",
    "诈骗公私财物，数额较大的，处三年以下有期徒刑、拘役或者管制，并处或者单处罚金；数额巨大或者有其他严重情节的，处三年以上十年以下有期徒刑，并处罚金；数额特别巨大或者有其他特别严重情节的，处十年以上有期徒刑或者无期徒刑，并处罚金或者没收财产。\n",
    "\n",
    "第二百三十四条 【故意伤害罪】\n",
    "故意伤害他人身体的，处三年以下有期徒刑、拘役或者管制。\n",
    "犯前款罪，致人重伤的，处三年以上十年以下有期徒刑；致人死亡或者以特别残忍手段致人重伤造成严重残疾的，处十年以上有期徒刑、无期徒刑或者死刑。本法另有规定的，依照规定。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b5b78a",
   "metadata": {},
   "source": [
    "## 1 理论知识讲解 (CS Principle)\n",
    "\n",
    "**ETL** 是数据工程的基石：\n",
    "\n",
    "1. **Extract (抽取)**：\n",
    "    * 从文件系统读取 `.txt` / `.pdf`。\n",
    "    * *挑战*：编码格式（UTF-8 vs GBK）、文件读取异常。\n",
    "\n",
    "\n",
    "2. **Transform (转换)**：\n",
    "    * **Cleaning**：去除多余的空格、换行符。\n",
    "    * **Chunking**：调用 Day 1 的切分器。\n",
    "    * **Metadata Injection**：**这步最关键！** 原始文本切分后会丢失上下文。我们需要手动把文件名、章节名作为 `metadata` 附在每一个 Chunk 上（例如 `{\"source\": \"penal_code.txt\"}`），否则将来检索出来你都不知道这段话出自哪里。\n",
    "\n",
    "\n",
    "3. **Load (加载)**：\n",
    "    * 调用 Day 3 的 `VectorDB.add_documents()`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522753bf",
   "metadata": {},
   "source": [
    "## 2. 代码实现 (The Ingestion Script)\n",
    "\n",
    "我们将利用之前封装好的模块，像搭积木一样构建管道。\n",
    "\n",
    "**创建/编辑文件**：`src/rag/ingest.py`\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "# 路径处理：确保能导入 src 下的模块\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../\")))\n",
    "\n",
    "from src.rag.etl import TextChunker\n",
    "from src.rag.vector_db import VectorDB\n",
    "\n",
    "def load_and_process_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Step 1: Extract - 读取文件\n",
    "    \"\"\"\n",
    "    print(f\"📄 Processing file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return [], []\n",
    "    \n",
    "    # 简单的元数据提取 (实际工程中可能需要解析文件名或正则提取章节号)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    \"\"\"\n",
    "    Step 2: Transform - 切分与清洗\n",
    "    \"\"\"\n",
    "    # 初始化我们 Day 1 写的切分器\n",
    "    chunker = TextChunker(chunk_size=300, chunk_overlap=50)\n",
    "    chunks = chunker.split(text)\n",
    "    \n",
    "    # 构造 Metadata\n",
    "    metadatas = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadatas.append({\n",
    "            \"source\": file_name,\n",
    "            \"chunk_id\": i,\n",
    "            \"length\": len(chunk)\n",
    "        })\n",
    "    \n",
    "    print(f\"✂️  Split into {len(chunks)} chunks.\")\n",
    "    return chunks, metadatas\n",
    "\n",
    "def main():\n",
    "    # 配置\n",
    "    DATA_DIR = \"data\"\n",
    "    DB_PATH = \"chroma_db_data\" # 注意这里要和 Day 3 保持一致\n",
    "    \n",
    "    # 初始化组件\n",
    "    print(\"🚀 Starting ETL Pipeline...\")\n",
    "    \n",
    "    # 初始化 DB (Day 3 的组件)\n",
    "    # 注意：如果目录存在，它会加载旧数据；如果不存在，会新建\n",
    "    vector_db = VectorDB(persist_path=DB_PATH)\n",
    "    \n",
    "    # 遍历 data 目录下的所有 txt 文件\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(DATA_DIR, filename)\n",
    "            \n",
    "            # 1. 处理数据\n",
    "            chunks, metadatas = load_and_process_file(file_path)\n",
    "            \n",
    "            if chunks:\n",
    "                # 2. Step 3: Load - 存入数据库\n",
    "                print(f\"💾 Ingesting to VectorDB...\")\n",
    "                vector_db.add_documents(chunks, metadatas)\n",
    "                print(f\"✅ Successfully ingested {filename}\")\n",
    "\n",
    "    print(\"🎉 ETL Pipeline Completed!\")\n",
    "\n",
    "    # --- 验证环节 ---\n",
    "    print(\"\\n🔍 Verifying with a test query...\")\n",
    "    results = vector_db.search(\"故意伤害致死怎么判？\", top_k=1)\n",
    "    for res in results:\n",
    "        print(f\"Answer found in [{res['metadata']['source']}]:\")\n",
    "        print(f\"Content: {res['text'][:50]}...\") # 只打印前50个字\n",
    "        print(f\"Distance: {res['distance']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 运行与验证\n",
    "\n",
    "在终端中运行：\n",
    "\n",
    "```bash\n",
    "python src/rag/ingest.py\n",
    "\n",
    "```\n",
    "\n",
    "**预期输出**：\n",
    "\n",
    "1. 应该能看到 Loading Model (from local cache) 的日志。\n",
    "2. 看到 `Processing file: .../data/penal_code.txt`。\n",
    "3. 看到 `Split into ... chunks`。\n",
    "4. 最后验证环节，针对问题“故意伤害致死”，应该能检索出包含“第二百三十四条”的那段文本。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 今日 LeetCode 练习\n",
    "\n",
    "我们今天处理的是 **Pipeline (管道)**，数据像流水一样经过一个个关卡。这和 **Queue (队列)** 的先进先出特性非常像。\n",
    "\n",
    "你之前的列表中有这道题，非常适合今天练习。\n",
    "\n",
    "* **题目**：[225. Implement Stack using Queues (用队列实现栈)](https://leetcode.cn/problems/implement-stack-using-queues/)\n",
    "* **状态**：在你的列表中 (题号 225)。\n",
    "* **关联**：Day 2 我们做了“最小栈”，Day 3 做了链表归并。今天挑战一下用 FIFO (队列) 的积木去搭 LIFO (栈) 的城堡。\n",
    "* **核心难点**：\n",
    "* 队列是“两头通”（一头进一头出）。\n",
    "* 栈是“死胡同”（只能从一头进出）。\n",
    "* **思路**：当你 `push` 一个新元素进队列时，它排在队尾。为了让它变成“栈顶”（队头），你需要把前面的所有老元素取出来，重新排到它后面去。\n",
    "\n",
    "\n",
    "\n",
    "**请尝试实现并提交。**\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 今日任务总结\n",
    "\n",
    "* [ ] 创建了 `data/penal_code.txt` 样本数据。\n",
    "* [ ] 编写了 `src/rag/ingest.py`，打通了 `Chunker` -> `Embedding` -> `VectorDB` 的全流程。\n",
    "* [ ] 验证了数据入库后能被检索到。\n",
    "* [ ] 练习 LeetCode 225。\n",
    "\n",
    "**这一步非常关键**。现在你的 AI 已经不再是空架子了，它肚子里真的有了刑法知识。\n",
    "明天（Day 5），我们将编写一个 **Web 界面 (Streamlit) 或者 CLI 工具**，把 LLM (大脑) 接进来，让它利用这些知识真正回答用户的问题（RAG 的最终形态）。\n",
    "\n",
    "准备好开始写代码了吗？"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
