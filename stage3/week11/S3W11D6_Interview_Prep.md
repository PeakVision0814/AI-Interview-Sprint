# 模拟面试准备：Agentic RAG 系统设计

## Q1: 你的 RAG 系统中，Chunk Size (切片大小) 设为多少？为什么这么设？

### 💡 核心考察点
考察你是否真的跑过实验，以及是否理解 Embedding 模型的输入限制和语义完整性之间的权衡。

### 🗣️ 参考回答
在我的项目中，我将 Chunk Size 设置为 **500 字符** (或大约 250-300 Tokens)，并设置了 **50 字符的 Overlap (重叠)**。

**这么做主要基于两个考虑：**

1.  **Embedding 模型的限制 (硬约束)**：
    * 我使用的是 `m3e-base` 或 `bge-m3` 等 BERT 架构的模型。
    * BERT 模型的最大上下文窗口通常是 **512 Tokens**。如果切片过大（比如 1000 Tokens），超过部分的语义会被模型截断或丢弃，导致向量表征不准确。
2.  **语义粒度 (软约束)**：
    * **过小 (e.g., 100 chars)**：句子支离破碎，向量无法捕捉完整的语义信息（比如“它”指代谁可能在上一段）。
    * **过大 (e.g., 2000 chars)**：语义被稀释（Noise）。一段话里包含太多杂乱的主题，检索时容易出现“向量距离近但实际细节不相关”的情况。
    * **Overlap 的作用**：为了防止切分时刚好把一句话切断，保留 50 字符的重叠能保证边界处的语义连续性。

---

## Q2: 如果检索回来的内容不相关，你的系统怎么处理？（如何解决检索噪音？）

### 💡 核心考察点
考察你是否了解向量检索 (ANN) 的局限性，以及 Cross-Encoder (Reranker) 的必要性。

### 🗣️ 参考回答
这是一个非常经典的问题。向量数据库（如 Chroma）做的是 **Top-K 近邻搜索**，它是一个“只会说 Yes 的人”——即使库里完全没有相关文档，它也会硬着头皮返回距离最近的几条垃圾数据。

为了解决这个问题，我引入了 **"漏斗过滤机制"**：

1.  **初筛 (Recall)**：先用向量检索召回 Top-10 文档（追求查全率）。
2.  **精排 (Rerank)**：使用 **Cross-Encoder (如 BGE-Reranker)** 对这 10 个文档与 Query 进行一对一的深度语义打分。
3.  **阈值截断 (Threshold Filtering)**：
    * 这是关键一步。我会设定一个**相关性阈值**（比如 Score < 0.3）。
    * 如果 Reranker 打分低于 0.3，我判定为“不相关”并直接丢弃。
    * **兜底策略**：如果所有文档都被过滤掉了，Agent 不会强行回答，而是直接告诉用户：“抱歉，知识库中没有相关信息。”

通过这种**Reranker + 阈值**的策略，有效避免了 LLM 基于错误上下文产生的“幻觉”。

---

## Q3: Function Calling (工具调用) 的底层原理是什么？LLM 怎么知道调用哪个函数？

### 💡 核心考察点
考察你是否认为 Agent 是“魔法”，还是理解其背后的文本预测本质。

### 🗣️ 参考回答
Function Calling 的本质不是模型真的“运行”了代码，而是模型被微调成能够**输出特定格式的 JSON 字符串**。

整个过程遵循 **ReAct (Reason + Act)** 模式：

1.  **Schema 注入 (System Prompt)**：
    * 我在 System Prompt 中通过 JSON Schema 严格定义了工具的名称、参数类型和描述。这对 LLM 来说就是一本“工具说明书”。
2.  **意图识别与参数填充 (Prediction)**：
    * 当用户提问时，LLM 结合上下文判断是否需要使用工具。
    * 如果需要，它不再生成自然语言，而是生成一个包含函数名和参数的 **JSON 对象**（例如 `{"tool": "get_weather", "args": {"city": "Hangzhou"}}`）。
3.  **程序侧执行 (Runtime Execution)**：
    * 我的 Python 程序捕获到这个 JSON，**暂停** LLM 的生成。
    * 在本地执行真正的 Python 函数，拿到结果。
4.  **结果回填 (Observation)**：
    * 我把函数运行的结果（如“气温25度”）封装成一条新的 Message 发回给 LLM。
    * LLM 看到这个“观察结果”后，继续进行下一轮的文本生成，给用户最终答案。

所以，LLM 只是**决策者**（写订单的厨师），而 Python Runtime 才是**执行者**（切菜的帮厨）。