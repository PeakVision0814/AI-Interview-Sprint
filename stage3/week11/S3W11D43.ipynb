{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c28fb4b",
   "metadata": {},
   "source": [
    "# S3W11D3: ä»â€œè„šæœ¬â€åˆ°â€œç³»ç»Ÿâ€ (Engineering Refactor)\n",
    "\n",
    "## ğŸ¯ ä»Šæ—¥ç›®æ ‡ (Today's Goals)\n",
    "\n",
    "1. **é…ç½®ä¸­å¿ƒåŒ–**: åˆ›å»º `src/config.py`ï¼Œå‘Šåˆ«ç¡¬ç¼–ç  (Magic Numbers)ã€‚\n",
    "2. **æ ¸å¿ƒå°è£…**: åˆ›å»º `src/rag/engine.py`ï¼Œå®ç° `RAGPipeline` ç±»ï¼Œç»Ÿä¸€ç®¡ç†â€œå…¥åº“â€å’Œâ€œæ£€ç´¢â€æµç¨‹ã€‚\n",
    "3. **ç®—æ³•æ”»åš**: æŒæ¡ BST çš„å¢åˆ æ“ä½œï¼Œç‰¹åˆ«æ˜¯ **LC 450 (åˆ é™¤èŠ‚ç‚¹)** çš„ä¸‰ç§æƒ…å†µå¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223accc",
   "metadata": {},
   "source": [
    "## 1. å·¥ç¨‹é‡æ„ï¼šé…ç½®ä¸­å¿ƒåŒ– (`src/config.py`)\n",
    "\n",
    "**ç¬¬ä¸€æ€§åŸç†**: **Single Source of Truth (å•ä¸€çœŸç†æ¥æº)**ã€‚\n",
    "ä¸è¦åœ¨ä»£ç é‡Œå†™ `chunk_size=500` æˆ–è€… `top_k=3`ã€‚å¦‚æœæ˜å¤©ä½ æƒ³æ”¹æˆ 5ï¼Œéš¾é“è¦æœéå…¨é¡¹ç›®å»æ”¹å—ï¼Ÿ\n",
    "\n",
    "è¯·åˆ›å»º/æ›´æ–° `src/config.py`:\n",
    "\n",
    "```python\n",
    "# src/config.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "load_dotenv()\n",
    "\n",
    "class Config:\n",
    "    # --- åŸºç¡€è·¯å¾„ ---\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "    DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "    CHROMA_DB_DIR = os.path.join(DATA_DIR, \"chroma_db_data\")\n",
    "    \n",
    "    # --- LLM é…ç½® ---\n",
    "    LLM_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\") or os.getenv(\"SILICONFLOW_API_KEY\")\n",
    "    LLM_BASE_URL = os.getenv(\"DEEPSEEK_BASE_URL\") or \"https://api.siliconflow.cn/v1\"\n",
    "    LLM_MODEL_NAME = \"deepseek-chat\"\n",
    "    \n",
    "    # --- RAG å‚æ•° ---\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    CHUNK_SIZE = 500       # æ¯ä¸ªåˆ‡ç‰‡çš„å¤§å°\n",
    "    CHUNK_OVERLAP = 50     # åˆ‡ç‰‡é‡å éƒ¨åˆ†\n",
    "    RETRIEVAL_TOP_K = 3    # æ¯æ¬¡æ£€ç´¢å‡ æ¡\n",
    "    \n",
    "    # --- Prompt æ¨¡æ¿ ---\n",
    "    RAG_SYSTEM_PROMPT = \"\"\"\n",
    "    ä½ æ˜¯ä¸€ä¸ªåŸºäºçŸ¥è¯†åº“çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ã€å‚è€ƒæ–‡æ¡£ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n",
    "    å¦‚æœå‚è€ƒæ–‡æ¡£ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´\"ä¸çŸ¥é“\"ï¼Œä¸è¦ç¼–é€ ã€‚\n",
    "    \n",
    "    ã€å‚è€ƒæ–‡æ¡£ã€‘:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "# å®ä¾‹åŒ–ï¼Œæ–¹ä¾¿å…¶ä»–æ¨¡å—ç›´æ¥ import config\n",
    "config = Config()\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9a15d",
   "metadata": {},
   "source": [
    "## 2. å·¥ç¨‹é‡æ„ï¼šRAG æ ¸å¿ƒå¼•æ“ (`src/rag/engine.py`)\n",
    "\n",
    "è¿™æ˜¯æˆ‘ä»¬ RAG ç³»ç»Ÿçš„â€œä¸»æ¿â€ã€‚å®ƒè´Ÿè´£æŠŠ Embeddingã€VectorDB å’Œ LLM è¿æ¥èµ·æ¥ã€‚\n",
    "\n",
    "è¯·åˆ›å»º/æ›´æ–° `src/rag/engine.py`:\n",
    "\n",
    "```python\n",
    "# src/rag/engine.py\n",
    "import os\n",
    "from typing import List\n",
    "# å¼•å…¥é…ç½®\n",
    "from src.config import config\n",
    "# å¼•å…¥ä¹‹å‰çš„ç»„ä»¶ (å‡è®¾ä½ ä¹‹å‰å·²ç»å†™å¥½äº†è¿™äº›ï¼Œå¦‚æœæ²¡æœ‰ï¼Œéœ€è¦æŠŠ Week 9/10 çš„ä»£ç æ¬è¿è¿‡æ¥)\n",
    "# è¿™é‡Œä¸ºäº†æ¼”ç¤ºå®Œæ•´æ€§ï¼Œæˆ‘ä½¿ç”¨äº†ä¼ªä»£ç å¯¼å…¥ï¼Œä½ éœ€è¦æ ¹æ®ä½ çš„å®é™…æ–‡ä»¶è·¯å¾„è°ƒæ•´\n",
    "from src.rag.vector_db import VectorDBHandler  # å‡è®¾ä½ å°è£…äº† Chroma æ“ä½œ\n",
    "from src.llm.client import LLMClient           # å‡è®¾ä½ å°è£…äº† OpenAI Client\n",
    "from src.rag.embedding import EmbeddingModel   # å‡è®¾ä½ å°è£…äº† SentenceTransformer\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"åˆå§‹åŒ– RAG æµæ°´çº¿çš„æ‰€æœ‰ç»„ä»¶\"\"\"\n",
    "        print(\"âš™ï¸ åˆå§‹åŒ– RAG Pipeline...\")\n",
    "        \n",
    "        # 1. åŠ è½½ Embedding æ¨¡å‹\n",
    "        self.embedding_model = EmbeddingModel(model_name=config.EMBEDDING_MODEL)\n",
    "        \n",
    "        # 2. è¿æ¥å‘é‡æ•°æ®åº“\n",
    "        self.vector_db = VectorDBHandler(\n",
    "            persist_directory=config.CHROMA_DB_DIR,\n",
    "            embedding_fn=self.embedding_model\n",
    "        )\n",
    "        \n",
    "        # 3. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯\n",
    "        self.llm = LLMClient(\n",
    "            api_key=config.LLM_API_KEY,\n",
    "            base_url=config.LLM_BASE_URL,\n",
    "            model=config.LLM_MODEL_NAME\n",
    "        )\n",
    "    \n",
    "    def ingest_documents(self, file_path: str):\n",
    "        \"\"\"\n",
    "        [ETL] æ•°æ®å…¥åº“æµç¨‹: è¯»å– -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å‚¨\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ“¥ æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}\")\n",
    "        # 1. è¯»å– & åˆ‡åˆ† (è¿™é‡Œå¯ä»¥ä½¿ç”¨ LangChain çš„ Splitterï¼Œä¹Ÿå¯ä»¥æ‰‹å†™)\n",
    "        # chunks = load_and_split(file_path, config.CHUNK_SIZE, config.CHUNK_OVERLAP)\n",
    "        \n",
    "        # æš‚ç”¨ä¼ªä»£ç ä»£æ›¿\n",
    "        chunks = [\"è¿™æ˜¯æµ‹è¯•æ–‡æ¡£æ®µè½1...\", \"è¿™æ˜¯æµ‹è¯•æ–‡æ¡£æ®µè½2...\"] \n",
    "        \n",
    "        # 2. å­˜å…¥å‘é‡åº“\n",
    "        self.vector_db.add_texts(chunks)\n",
    "        print(f\"âœ… å…¥åº“å®Œæˆï¼Œå…± {len(chunks)} ä¸ªç‰‡æ®µã€‚\")\n",
    "\n",
    "    def query(self, user_query: str) -> str:\n",
    "        \"\"\"\n",
    "        [Inference] æ£€ç´¢ + ç”Ÿæˆæµç¨‹\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ” ç”¨æˆ·æé—®: {user_query}\")\n",
    "        \n",
    "        # 1. æ£€ç´¢ (Retrieve)\n",
    "        # vector_db å†…éƒ¨ä¼šè‡ªåŠ¨æŠŠ query å˜æˆå‘é‡\n",
    "        relevant_docs = self.vector_db.search(\n",
    "            query=user_query, \n",
    "            top_k=config.RETRIEVAL_TOP_K\n",
    "        )\n",
    "        \n",
    "        # 2. æ„å»ºä¸Šä¸‹æ–‡ (Context Construction)\n",
    "        context_str = \"\\n\\n\".join(relevant_docs)\n",
    "        \n",
    "        # 3. ç”Ÿæˆ Prompt (Augment)\n",
    "        system_prompt = config.RAG_SYSTEM_PROMPT.format(context=context_str)\n",
    "        \n",
    "        # 4. è°ƒç”¨ LLM (Generate)\n",
    "        answer = self.llm.chat_completion(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_query}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return answer\n",
    "\n",
    "# å•ä¾‹æ¨¡å¼ï¼šåˆ›å»ºä¸€ä¸ªå…¨å±€å¯ç”¨çš„ pipeline å®ä¾‹\n",
    "# è¿™æ ·ä¸‹æ¬¡ import rag_engine æ—¶ä¸éœ€è¦é‡æ–°åŠ è½½æ¨¡å‹\n",
    "rag_engine = RAGPipeline()\n",
    "\n",
    "```\n",
    "\n",
    "**âœ… åŠ¨ä½œ**:\n",
    "\n",
    "1. ç¡®ä¿ä½ çš„ `src/rag/vector_db.py` ç­‰æ–‡ä»¶å­˜åœ¨ä¸”å¯ç”¨ã€‚\n",
    "2. å¦‚æœæœ‰ç¼ºå¤±çš„æ¨¡å—ï¼Œä»Šå¤©çš„ä»»åŠ¡å°±æ˜¯**æŠŠ Notebook é‡Œçš„ä»£ç æ¬è¿è¿›å»**ã€‚\n",
    "3. æ‰§è¡Œ `git add .` å’Œ `git commit -m \"refactor: implement RAGPipeline and centralized config\"`ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b6ad1",
   "metadata": {},
   "source": [
    "## 3. ç®—æ³•æ”»åšï¼šBST çš„å¢åˆ  (The Surgical Operations)\n",
    "\n",
    "å¦‚æœè¯´æ˜¨å¤©çš„â€œéªŒè¯ BSTâ€æ˜¯**åªè¯»**æ“ä½œï¼Œä»Šå¤©çš„â€œå¢åˆ â€å°±æ˜¯**å†™**æ“ä½œã€‚\n",
    "\n",
    "### LC 701. äºŒå‰æœç´¢æ ‘ä¸­çš„æ’å…¥æ“ä½œ (Insert)\n",
    "\n",
    "* **éš¾åº¦**: Medium (å…¶å®æ˜¯ Easy)\n",
    "* **æ€è·¯**:\n",
    "* æ¯”æ ¹å°ï¼Ÿå¾€å·¦èµ°ã€‚å·¦è¾¹æ²¡äººï¼Ÿä½ å°±åœ¨è¿™å®‰å®¶ã€‚\n",
    "* æ¯”æ ¹å¤§ï¼Ÿå¾€å³èµ°ã€‚å³è¾¹æ²¡äººï¼Ÿä½ å°±åœ¨è¿™å®‰å®¶ã€‚\n",
    "\n",
    "\n",
    "* **ä»£ç ** (ç›´æ¥å†™åœ¨ LeetCode ä¸Š):\n",
    "```python\n",
    "def insertIntoBST(self, root: Optional[TreeNode], val: int) -> Optional[TreeNode]:\n",
    "    if not root:\n",
    "        return TreeNode(val)\n",
    "    if val < root.val:\n",
    "        root.left = self.insertIntoBST(root.left, val)\n",
    "    else:\n",
    "        root.right = self.insertIntoBST(root.right, val)\n",
    "    return root\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a30286",
   "metadata": {},
   "source": [
    "### ğŸ”¥ LC 450. åˆ é™¤äºŒå‰æœç´¢æ ‘ä¸­çš„èŠ‚ç‚¹ (Delete) - **ä»Šæ—¥é‡éš¾ç‚¹**\n",
    "\n",
    "åˆ é™¤èŠ‚ç‚¹æ¯”æ’å…¥éš¾å¾—å¤šï¼Œå› ä¸ºåˆ æ‰ä¸€ä¸ªèŠ‚ç‚¹åï¼Œå¯èƒ½ä¼šç•™ä¸‹ä¸€ä¸ªâ€œå‘â€ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾äººæ¥å¡«è¿™ä¸ªå‘ï¼Œè¿˜è¦ä¿æŒ BST çš„æ€§è´¨ï¼ˆå·¦å°å³å¤§ï¼‰ã€‚\n",
    "\n",
    "**ä¸‰ç§æƒ…å†µ (Case Analysis)**:\n",
    "\n",
    "1. **å¶å­èŠ‚ç‚¹**: ç›´æ¥åˆ ï¼Œæ²¡åé¡¾ä¹‹å¿§ã€‚\n",
    "2. **åªæœ‰ä¸€ä¸ªå­©å­**: çˆ¶çˆ±å¦‚å±±ï¼Œè®©å­©å­ç›´æ¥é¡¶æ›¿è‡ªå·±çš„ä½ç½®ã€‚\n",
    "3. **æœ‰ä¸¤ä¸ªå­©å­ (Trouble!)**:\n",
    "* æ¯”å¦‚ä½ è¦åˆ æ‰æ ¹èŠ‚ç‚¹ `5`ï¼Œå®ƒå·¦è¾¹æœ‰ `3`ï¼Œå³è¾¹æœ‰ `7`ã€‚è°æ¥å½“æ–°çš„ç‹ï¼Ÿ\n",
    "* **ç»§æ‰¿æ³•åˆ™**: åªæœ‰ä¸¤ä¸ªäººæœ‰èµ„æ ¼ç»§æ‰¿çš‡ä½ï¼š\n",
    "* **å·¦æ´¾é¢†è¢– (Predecessor)**: å·¦å­æ ‘é‡Œ**æœ€å¤§**çš„é‚£ä¸ªã€‚\n",
    "* **å³æ´¾é¢†è¢– (Successor)**: å³å­æ ‘é‡Œ**æœ€å°**çš„é‚£ä¸ªã€‚\n",
    "\n",
    "\n",
    "* **é€šç”¨åšæ³•**: æ‰¾åˆ° **å³å­æ ‘é‡Œæœ€å°çš„èŠ‚ç‚¹ (Successor)**ï¼ŒæŠŠå®ƒçš„å€¼å¤åˆ¶ç»™å½“å‰èŠ‚ç‚¹ï¼Œç„¶åæŠŠé‚£ä¸ª Successor åˆ æ‰ã€‚\n",
    "\n",
    "\n",
    "\n",
    "#### ä»£ç å®ç° (æ ‡å‡†æ¨¡æ¿)\n",
    "\n",
    "```python\n",
    "class Solution:\n",
    "    def deleteNode(self, root: Optional[TreeNode], key: int) -> Optional[TreeNode]:\n",
    "        if not root:\n",
    "            return None\n",
    "        \n",
    "        # 1. å…ˆæ‰¾èŠ‚ç‚¹\n",
    "        if key < root.val:\n",
    "            root.left = self.deleteNode(root.left, key)\n",
    "        elif key > root.val:\n",
    "            root.right = self.deleteNode(root.right, key)\n",
    "        else:\n",
    "            # 2. æ‰¾åˆ°äº†ï¼Œå¼€å§‹å¤„ç†åˆ é™¤é€»è¾‘\n",
    "            \n",
    "            # Case 1 & 2: åªæœ‰ä¸€ä¸ªå­©å­æˆ–æ²¡æœ‰å­©å­\n",
    "            if not root.left:\n",
    "                return root.right\n",
    "            if not root.right:\n",
    "                return root.left\n",
    "            \n",
    "            # Case 3: ä¸¤ä¸ªå­©å­éƒ½åœ¨\n",
    "            # ç­–ç•¥: æ‰¾å³å­æ ‘ä¸­æœ€å°çš„èŠ‚ç‚¹ (Successor) æ¥é¡¶æ›¿\n",
    "            successor = self.get_min(root.right)\n",
    "            \n",
    "            # è¿™é‡Œçš„æ“ä½œå¾ˆçµæ€§ï¼šä¸æ˜¯ç§»åŠ¨èŠ‚ç‚¹å¯¹è±¡ï¼Œè€Œæ˜¯ç›´æ¥â€œå¤ºèˆâ€ï¼ˆæ›¿æ¢å€¼ï¼‰\n",
    "            root.val = successor.val\n",
    "            \n",
    "            # æ—¢ç„¶å€¼å·²ç»æ‹¿ä¸Šæ¥äº†ï¼Œé‚£ä¸ª successor ä¹Ÿæ²¡ç”¨äº†ï¼Œå»å³å­æ ‘é‡ŒæŠŠå®ƒåˆ æ‰\n",
    "            root.right = self.deleteNode(root.right, successor.val)\n",
    "            \n",
    "        return root\n",
    "\n",
    "    def get_min(self, node):\n",
    "        # ä¸€ç›´å¾€å·¦èµ°ï¼Œæœ€å·¦è¾¹çš„å°±æ˜¯æœ€å°çš„\n",
    "        while node.left:\n",
    "            node = node.left\n",
    "        return node\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f92150b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. ä»Šæ—¥ä»»åŠ¡æ€»ç»“\n",
    "\n",
    "1. [ ] **Engineering**: æˆåŠŸåˆ›å»º `src/config.py` å’Œ `src/rag/engine.py`ã€‚\n",
    "* *è‡ªæµ‹*: èƒ½å¤Ÿåœ¨ä¸€ä¸ªæ–°çš„ Python æ–‡ä»¶é‡Œ `from src.rag.engine import rag_engine` å¹¶ä¸æŠ¥é”™ã€‚\n",
    "\n",
    "\n",
    "2. [ ] **Algorithm**: æ·±åˆ»ç†è§£å¹¶ AC LC 450 (åˆ é™¤èŠ‚ç‚¹)ã€‚**ä¸€å®šè¦è‡ªå·±ç”»å›¾ç†è§£â€œç»§ä»»è€…â€é€»è¾‘**ã€‚\n",
    "3. [ ] **Git**: æäº¤ä¸€ä¸ªå¹²å‡€çš„ Refactor Commitã€‚\n",
    "\n",
    "ğŸ’¡ **å¯¼å¸ˆæç¤º**:\n",
    "ä»Šå¤©çš„é‡æ„å¯èƒ½æ¯”è¾ƒæ¯ç‡¥ï¼Œå› ä¸ºéƒ½æ˜¯åœ¨æ¬è¿ä»£ç ã€‚ä½†è¯·ç›¸ä¿¡æˆ‘ï¼Œ**å¦‚æœä¸åšè¿™ä¸€æ­¥ï¼Œä½ çš„é¡¹ç›®åœ¨é¢è¯•å®˜çœ¼é‡Œå°±æ˜¯ä¸€ä¸ªâ€œå­¦ç”Ÿä½œä¸šâ€ï¼›åšäº†è¿™ä¸€æ­¥ï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªâ€œå·¥ç¨‹é¡¹ç›®â€ã€‚**\n",
    "\n",
    "åŠ æ²¹ï¼å¦‚æœé…ç½®è·¯å¾„é‡åˆ°é—®é¢˜ï¼ˆæ¯”å¦‚ `ModuleNotFoundError`ï¼‰ï¼Œéšæ—¶æŠŠæŠ¥é”™å‘ç»™æˆ‘ã€‚"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
