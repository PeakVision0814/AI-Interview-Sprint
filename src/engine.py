# src/rag/engine.py
import os
from typing import List, Optional
import json

# å¼•å…¥é…ç½®
from src.config import config
# å¼•å…¥å·¥å…· (ç¡®ä¿è¿™äº›æ¨¡å—ä½ å·²ç»æ¬è¿åˆ°äº† src/llm å’Œ src/rag ä¸‹)
from src.llm.client import LLMClient         # éœ€è‡ªè¡Œå°è£…æˆ–ä½¿ç”¨ openai åŸç”Ÿ
from src.rag.vector_db import VectorDBHandler # éœ€è‡ªè¡Œå°è£…
from src.rag.embedding import EmbeddingModel  # éœ€è‡ªè¡Œå°è£…

class RAGPipeline:
    def __init__(self):
        """åˆå§‹åŒ– RAG æµæ°´çº¿çš„æ‰€æœ‰ç»„ä»¶"""
        print(f"âš™ï¸ åˆå§‹åŒ– RAG Pipeline (Model: {config.LLM_MODEL_NAME})...")
        
        # 1. åŠ è½½ Embedding æ¨¡å‹ (æ¯”å¦‚ SentenceTransformer)
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ç¡®ä¿ EmbeddingModel ç±»å·²å®ç°
        self.embedding_model = EmbeddingModel(model_name=config.EMBEDDING_MODEL)
        
        # 2. è¿æ¥å‘é‡æ•°æ®åº“ (Chroma)
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ç¡®ä¿ VectorDBHandler ç±»å·²å®ç°
        self.vector_db = VectorDBHandler(
            persist_directory=str(config.CHROMA_DB_DIR), # Pathè½¬strï¼Œé˜²æ­¢æŠ¥é”™
            embedding_fn=self.embedding_model
        )
        
        # 3. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ (DeepSeek / SiliconFlow)
        # å¦‚æœä½ è¿˜æ²¡æœ‰å°è£… LLMClientï¼Œå¯ä»¥ç›´æ¥åœ¨è¿™é‡Œç”¨ OpenAI(api_key=...)
        self.llm = LLMClient(
            api_key=config.LLM_API_KEY,
            base_url=config.LLM_BASE_URL,
            model_name=config.LLM_MODEL_NAME
        )
    
    def ingest_documents(self, file_path: str):
        """
        [ETL] æ•°æ®å…¥åº“æµç¨‹: è¯»å– -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å‚¨
        """
        print(f"ğŸ“¥ [ETL] æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
        # è¿™é‡Œåº”è¯¥è°ƒç”¨ TextSplitter
        # ä¸ºäº†æ¼”ç¤ºï¼Œæš‚ä¸”å‡è®¾è¾“å…¥å°±æ˜¯åˆ—è¡¨
        # å®é™…é¡¹ç›®ä¸­ï¼šchunks = text_splitter.split_documents(load(file_path))
        chunks = [f"è¿™æ˜¯ä» {file_path} è¯»å–çš„æµ‹è¯•ç‰‡æ®µ..."] 
        
        # å­˜å…¥å‘é‡åº“
        self.vector_db.add_texts(chunks)
        print(f"âœ… å…¥åº“å®Œæˆï¼Œå…± {len(chunks)} ä¸ªç‰‡æ®µã€‚")

    def query(self, user_query: str) -> str:
        """
        [Inference] RAG æ ¸å¿ƒé“¾è·¯ï¼šæ£€ç´¢ + ç”Ÿæˆ
        """
        print(f"ğŸ” [RAG] ç”¨æˆ·æé—®: {user_query}")
        
        # 1. æ£€ç´¢ (Retrieve)
        relevant_docs = self.vector_db.search(
            query=user_query, 
            top_k=config.RETRIEVAL_TOP_K
        )
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡ (Augment)
        context_str = "\n\n".join(relevant_docs)
        if not context_str:
            context_str = "æš‚æ— ç›¸å…³æ–‡æ¡£ã€‚"
        
        # 3. ç»„è£… Prompt
        system_prompt = config.RAG_SYSTEM_PROMPT.format(context=context_str)
        
        # 4. ç”Ÿæˆ (Generate)
        print("ğŸ¤– [LLM] æ­£åœ¨ç”Ÿæˆå›ç­”...")
        answer = self.llm.chat_completion(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_query}
            ]
        )
        
        return answer

# å•ä¾‹æ¨¡å¼
try:
    rag_engine = RAGPipeline()
except Exception as e:
    print(f"âš ï¸ RAG Engine åˆå§‹åŒ–å¤±è´¥ (å¯èƒ½æ˜¯ä¾èµ–ç»„ä»¶æœªå®Œæˆ): {e}")
    rag_engine = None