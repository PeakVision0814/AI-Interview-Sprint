import os
import sys
# è·¯å¾„å¤„ç†ï¼šç¡®ä¿èƒ½å¯¼å…¥ src ä¸‹çš„æ¨¡å—
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

from src.rag.etl import TextChunker
from src.rag.vector_db import VectorDB

def load_and_process_file(file_path: str):
    """
    Step 1: Extract - è¯»å–æ–‡ä»¶
    """
    print(f"ğŸ“„ Processing file: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
    except Exception as e:
        print(f"Error reading file: {e}")
        return [], []
    
    # ç®€å•çš„å…ƒæ•°æ®æå– (å®é™…å·¥ç¨‹ä¸­å¯èƒ½éœ€è¦è§£ææ–‡ä»¶åæˆ–æ­£åˆ™æå–ç« èŠ‚å·)
    file_name = os.path.basename(file_path)
    
    """
    Step 2: Transform - åˆ‡åˆ†ä¸æ¸…æ´—
    """
    # åˆå§‹åŒ–æˆ‘ä»¬ Day 1 å†™çš„åˆ‡åˆ†å™¨
    chunker = TextChunker(chunk_size=300, chunk_overlap=50)
    chunks = chunker.split(text)
    
    # æ„é€  Metadata
    metadatas = []
    for i, chunk in enumerate(chunks):
        metadatas.append({
            "source": file_name,
            "chunk_id": i,
            "length": len(chunk)
        })
    
    print(f"âœ‚ï¸  Split into {len(chunks)} chunks.")
    return chunks, metadatas

def main():
    # é…ç½®
    DATA_DIR = "data"
    DB_PATH = "chroma_db_data" # æ³¨æ„è¿™é‡Œè¦å’Œ Day 3 ä¿æŒä¸€è‡´
    
    # åˆå§‹åŒ–ç»„ä»¶
    print("ğŸš€ Starting ETL Pipeline...")
    
    # åˆå§‹åŒ– DB (Day 3 çš„ç»„ä»¶)
    # æ³¨æ„ï¼šå¦‚æœç›®å½•å­˜åœ¨ï¼Œå®ƒä¼šåŠ è½½æ—§æ•°æ®ï¼›å¦‚æœä¸å­˜åœ¨ï¼Œä¼šæ–°å»º
    vector_db = VectorDB(persist_path=DB_PATH)
    
    # éå† data ç›®å½•ä¸‹çš„æ‰€æœ‰ txt æ–‡ä»¶
    for filename in os.listdir(DATA_DIR):
        if filename.endswith(".txt"):
            file_path = os.path.join(DATA_DIR, filename)
            
            # 1. å¤„ç†æ•°æ®
            chunks, metadatas = load_and_process_file(file_path)
            
            if chunks:
                # 2. Step 3: Load - å­˜å…¥æ•°æ®åº“
                print(f"ğŸ’¾ Ingesting to VectorDB...")
                vector_db.add_documents(chunks, metadatas)
                print(f"âœ… Successfully ingested {filename}")

    print("ğŸ‰ ETL Pipeline Completed!")

    # --- éªŒè¯ç¯èŠ‚ ---
    print("\nğŸ” Verifying with a test query...")
    results = vector_db.search("æ•…æ„ä¼¤å®³è‡´æ­»æ€ä¹ˆåˆ¤ï¼Ÿ", top_k=1)
    for res in results:
        print(f"Answer found in [{res['metadata']['source']}]:")
        print(f"Content: {res['text'][:50]}...") # åªæ‰“å°å‰50ä¸ªå­—
        print(f"Distance: {res['distance']:.4f}")

if __name__ == "__main__":
    main()
