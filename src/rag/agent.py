# src/rag/agent.py
import os
import json
from openai import OpenAI
from dotenv import load_dotenv
from src.llm.tools import TOOLS_SCHEMA, AVAILABLE_FUNCTIONS

load_dotenv()

class RAGAgent:
    def __init__(self):
        # åˆå§‹åŒ– DeepSeek / SiliconFlow å®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=os.getenv("DEEPSEEK_API_KEY"), 
            base_url=os.getenv("DEEPSEEK_BASE_URL")
        )
        self.model_name = "deepseek-chat" # æˆ–è€… "deepseek-ai/DeepSeek-V2.5"
        self.system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI ç®—æ³•é¢è¯•åŠ©æ‰‹ã€‚
        ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·å‡†å¤‡ AI é¢è¯•ï¼Œæˆ–è€…å›ç­”å…³äºæ·±åº¦å­¦ä¹ çš„æŠ€æœ¯é—®é¢˜ã€‚
        
        1. å¯¹äºæ—¥å¸¸é—®å€™ï¼ˆå¦‚â€œä½ å¥½â€ã€â€œä½ æ˜¯è°â€ï¼‰ï¼Œè¯·ç›´æ¥çƒ­æƒ…å›å¤ï¼Œ**ä¸è¦**è°ƒç”¨å·¥å…·ã€‚
        2. å¯¹äºå…·ä½“çš„æŠ€æœ¯é—®é¢˜ï¼ˆå¦‚â€œä»€ä¹ˆæ˜¯ RAGâ€ã€â€œè§£é‡Šä¸‹ BERTâ€ï¼‰ï¼Œè¯·åŠ¡å¿…è°ƒç”¨ search_knowledge_base å·¥å…·æ£€ç´¢ä¿¡æ¯ã€‚
        3. å›ç­”è¦ç®€æ´ã€ä¸“ä¸šã€‚
        """
        # å¯¹è¯è®°å¿† (History)
        self.messages = [{"role": "system", "content": self.system_prompt}]

    def chat(self, user_input: str):
        # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.messages.append({"role": "user", "content": user_input})
        
        # 2. ç¬¬ä¸€è½®è°ƒç”¨ LLM (æ€è€ƒ)
        print("ğŸ¤– Agent is thinking...")
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=self.messages,
            tools=TOOLS_SCHEMA,
            tool_choice="auto" 
        )
        
        response_msg = response.choices[0].message
        tool_calls = response_msg.tool_calls
        
        # 3. åˆ¤æ–­æ˜¯å¦éœ€è¦è¡ŒåŠ¨
        if tool_calls:
            print(f"ğŸ› ï¸ Agent decided to use tool: {tool_calls[0].function.name}")
            
            # å°† LLM çš„â€œæƒ³è°ƒç”¨å·¥å…·â€çš„æƒ³æ³•åŠ å…¥å†å²
            self.messages.append(response_msg)
            
            # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
            for tool_call in tool_calls:
                fn_name = tool_call.function.name
                fn_to_call = AVAILABLE_FUNCTIONS[fn_name]
                fn_args = json.loads(tool_call.function.arguments)
                
                # çœŸæ­£æ‰§è¡Œå‡½æ•°
                tool_output = fn_to_call(**fn_args)
                
                # å°†ç»“æœå›å¡«ç»™ LLM
                self.messages.append({
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": fn_name,
                    "content": tool_output
                })
            
            # 4. ç¬¬äºŒè½®è°ƒç”¨ LLM (ç”Ÿæˆæœ€ç»ˆå›ç­”)
            final_response = self.client.chat.completions.create(
                model=self.model_name,
                messages=self.messages
            )
            reply = final_response.choices[0].message.content
        else:
            # ä¸éœ€è¦å·¥å…·ï¼Œç›´æ¥å›å¤
            print("ğŸ’¬ Agent decided to chat directly.")
            reply = response_msg.content
        
        # å°†æœ€ç»ˆå›ç­”åŠ å…¥å†å²
        self.messages.append({"role": "assistant", "content": reply})
        return reply

# --- æµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    agent = RAGAgent()
    
    print("\n--- Test 1: Chit-chat ---")
    print("User: ä½ å¥½")
    print("Agent:", agent.chat("ä½ å¥½"))
    
    print("\n--- Test 2: Technical Query ---")
    print("User: è®²è®² transformer æ˜¯ä»€ä¹ˆ")
    print("Agent:", agent.chat("è®²è®² transformer æ˜¯ä»€ä¹ˆ"))