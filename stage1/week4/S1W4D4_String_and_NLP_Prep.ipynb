{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f74f80",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Week 4 Day 4: æ–‡æœ¬ä¸åºåˆ— (Strings & NLP Prep)\n",
    "\n",
    "## 1 æ ¸å¿ƒæ¦‚å¿µï¼šå¦‚ä½•è®©è®¡ç®—æœºâ€œè¯»æ‡‚â€å­—ï¼Ÿ\n",
    "\n",
    "**åº•å±‚è”ç³» (The Connection):**\n",
    "\n",
    "  * **LeetCode**: å­—ç¬¦ä¸²ï¼ˆStringï¼‰æ˜¯ä¸€ä¸²å­—ç¬¦çš„æ•°ç»„ã€‚æˆ‘ä»¬éœ€è¦å¤„ç†å­—ç¬¦çš„é¡ºåºã€æŸ¥æ‰¾å­ä¸²ç­‰ã€‚\n",
    "  * **AI (NLP)**: ç¥ç»ç½‘ç»œä¸è®¤è¯† \"Apple\"ã€‚\n",
    "    1.  **Tokenization (åˆ†è¯)**: æŠŠ \"Apple is red\" åˆ‡æˆ `[\"Apple\", \"is\", \"red\"]`ã€‚\n",
    "    2.  **Indexing (ç´¢å¼•)**: æŸ¥å­—å…¸ï¼Œå˜æˆæ•°å­— `[102, 5, 99]`ã€‚\n",
    "    3.  **Embedding**: å˜æˆå‘é‡ï¼ˆStage 2 é‡ç‚¹ï¼‰ã€‚\n",
    "\n",
    "ä»Šå¤©æˆ‘ä»¬å…ˆè§£å†³å‰ä¸¤æ­¥ï¼š**æ€ä¹ˆä¼˜é›…åœ°å¤„ç†å­—ç¬¦ä¸²**ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d72229",
   "metadata": {},
   "source": [
    "## 2 AI å·¥ç¨‹é¢„çƒ­ (The \"Job\" Skills)\n",
    "\n",
    "åœ¨è¿›å…¥ç®—æ³•é¢˜ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå†™ä¸€æ®µä»£ç ï¼Œæ¨¡æ‹Ÿ BERT æ¨¡å‹æ˜¯å¦‚ä½•å¤„ç†è¾“å…¥çš„ã€‚è¿™æ¯”ç›´æ¥åšé¢˜æ›´æœ‰â€œå…·èº«æ„Ÿâ€ã€‚\n",
    "\n",
    "### 2.1 æ‰‹å†™ä¸€ä¸ªç®€æ˜“åˆ†è¯å™¨ (Tokenizer)\n",
    "\n",
    "è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œç†è§£æ–‡æœ¬æ˜¯å¦‚ä½•å˜æˆ Tensor çš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebcf538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯è¡¨å¤§å°: 10\n",
      "è¯è¡¨å†…å®¹: {'ai': 1, 'and': 2, 'great': 3, 'i': 4, 'is': 5, 'learning': 6, 'love': 7, 'machine': 8, 'python': 9, '<UNK>': 0}\n",
      "\n",
      "=== æµ‹è¯•åˆ†è¯å™¨ ===\n",
      "Token IDs (Tensor): tensor([4, 7, 9])\n",
      "è¿˜åŸæ–‡æœ¬: i love python\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, text_corpus):\n",
    "        # 1. æ„å»ºè¯è¡¨ (Vocabulary)\n",
    "        # set å»é‡ -> sorted æ’åº -> enumerate å»ºç«‹ç´¢å¼•\n",
    "        tokens = sorted(list(set(text_corpus.split())))\n",
    "        self.token_to_id = {t: i+1 for i, t in enumerate(tokens)} # +1 æ˜¯å› ä¸º 0 é€šå¸¸ç•™ç»™ padding\n",
    "        self.token_to_id['<UNK>'] = 0 # æœªçŸ¥è¯\n",
    "        \n",
    "        # åå‘æŸ¥æ‰¾è¡¨ (ID -> Token)\n",
    "        self.id_to_token = {v: k for k, v in self.token_to_id.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        # åˆ†è¯å¹¶è½¬ ID\n",
    "        words = text.split()\n",
    "        ids = [self.token_to_id.get(w, 0) for w in words]\n",
    "        return torch.tensor(ids)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # ID è½¬å›æ–‡æœ¬\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "        return \" \".join([self.id_to_token.get(i, '<UNK>') for i in ids])\n",
    "\n",
    "# --- äº¤äº’å¼æµ‹è¯• ---\n",
    "# 1. è®­ç»ƒä¸€ä¸ªç®€å•çš„è¯è¡¨\n",
    "corpus = \"i love ai and python machine learning is great\"\n",
    "tokenizer = SimpleTokenizer(corpus)\n",
    "print(f\"è¯è¡¨å¤§å°: {len(tokenizer.token_to_id)}\")\n",
    "print(f\"è¯è¡¨å†…å®¹: {tokenizer.token_to_id}\")\n",
    "\n",
    "# 2. æ‰‹åŠ¨è¾“å…¥æµ‹è¯•\n",
    "print(\"\\n=== æµ‹è¯•åˆ†è¯å™¨ ===\")\n",
    "input_text = input(\"è¯·è¾“å…¥ä¸€æ®µè‹±æ–‡ (ä¾‹å¦‚: i love python): \").strip()\n",
    "\n",
    "# ç¼–ç \n",
    "encoded_ids = tokenizer.encode(input_text)\n",
    "print(f\"Token IDs (Tensor): {encoded_ids}\")\n",
    "\n",
    "# è§£ç \n",
    "decoded_text = tokenizer.decode(encoded_ids)\n",
    "print(f\"è¿˜åŸæ–‡æœ¬: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5efa73",
   "metadata": {},
   "source": [
    "> **æ³¨æ„**: å¦‚æœä½ è¾“å…¥çš„å•è¯ä¸åœ¨ `corpus` é‡Œï¼ˆæ¯”å¦‚ \"robot\"ï¼‰ï¼Œå®ƒä¼šè¢«è½¬æˆ `0` (`<UNK>`)ã€‚è¿™å°±æ˜¯ NLP ä¸­å¸¸è§çš„ **OOV (Out Of Vocabulary)** é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880da2a8",
   "metadata": {},
   "source": [
    "## 3 LeetCode ç®—æ³•çªå‡» (The \"Interview\" Skills)\n",
    "\n",
    "å­—ç¬¦ä¸²é¢˜ç›®æœ€è€ƒéªŒå¯¹ **Edge Case (è¾¹ç•Œæ¡ä»¶)** çš„å¤„ç†ï¼Œæ¯”å¦‚ç©ºå­—ç¬¦ä¸²ã€åªæœ‰ä¸€ä¸ªå­—ç¬¦ã€å…¨æ˜¯ç©ºæ ¼ç­‰ã€‚\n",
    "\n",
    "### 3.1 æœ€é•¿å…¬å…±å‰ç¼€ (Longest Common Prefix) - Easy\n",
    "\n",
    "  * **LeetCode é“¾æ¥**: [14. Longest Common Prefix](../../LeetCode%20practice/1-50.ipynb)\n",
    "  * **AI å…³è”**: åœ¨å¤„ç†åºåˆ—æ•°æ®æ—¶ï¼Œæå–å…¬å…±ç‰¹å¾ï¼ˆPattern Recognitionï¼‰çš„åŸºç¡€é€»è¾‘ã€‚\n",
    "  * **æ ¸å¿ƒæ€è·¯**: **æ¨ªå‘æ‰«æ**æˆ–**æ’åºåæ¯”è¾ƒ**ã€‚\n",
    "      * æŠ€å·§ï¼šå¦‚æœæŠŠå­—ç¬¦ä¸²æ•°ç»„æ’åºï¼Œåªéœ€è¦æ¯”è¾ƒ**ç¬¬ä¸€ä¸ª**å’Œ**æœ€åä¸€ä¸ª**å­—ç¬¦ä¸²å³å¯ï¼ˆå› ä¸ºä¸­é—´çš„è‚¯å®šæ›´åƒï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Solution:\n",
    "    def longestCommonPrefix(self, strs: List[str]) -> str:\n",
    "        if not strs:\n",
    "            return \"\"\n",
    "        \n",
    "        # æŠ€å·§ï¼šæŒ‰å­—æ¯åºæ’åº\n",
    "        # æ¯”å¦‚ [\"flower\", \"flow\", \"flight\"] -> [\"flight\", \"flow\", \"flower\"]\n",
    "        # åªè¦æ¯”è¾ƒ \"flight\" å’Œ \"flower\" çš„å…¬å…±éƒ¨åˆ†ï¼Œé‚£å°±æ˜¯æ‰€æœ‰äººçš„å…¬å…±éƒ¨åˆ†\n",
    "        strs.sort()\n",
    "        \n",
    "        first = strs[0]\n",
    "        last = strs[-1]\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(first) and i < len(last) and first[i] == last[i]:\n",
    "            i += 1\n",
    "            \n",
    "        return first[:i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e1892",
   "metadata": {},
   "source": [
    "### 3.2 å­—ç¬¦ä¸²ç›¸åŠ  (Add Strings) - Easy (é«˜é¢‘)\n",
    "\n",
    "  * **LeetCode é“¾æ¥**: [415. Add Strings](https://leetcode.com/problems/add-strings/)\n",
    "  * **æ ¸å¿ƒæ€è·¯**: **æ¨¡æ‹Ÿå¤§æ•°è¿ç®—**ã€‚ä¸èƒ½ç›´æ¥è½¬ `int()`ï¼ˆé¢˜ç›®é€šå¸¸é™åˆ¶ï¼‰ï¼Œè¦ä¸€ä½ä¸€ä½ç®—ã€‚åŒæŒ‡é’ˆä»åå¾€å‰èµ°ï¼Œå¤„ç† `carry` (è¿›ä½)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def addStrings(self, num1: str, num2: str) -> str:\n",
    "        i, j = len(num1) - 1, len(num2) - 1\n",
    "        carry = 0\n",
    "        res = []\n",
    "        \n",
    "        while i >= 0 or j >= 0 or carry:\n",
    "            # å­—ç¬¦è½¬æ•°å­—: ord('5') - ord('0') = 5\n",
    "            n1 = int(num1[i]) if i >= 0 else 0\n",
    "            n2 = int(num2[j]) if j >= 0 else 0\n",
    "            \n",
    "            temp_sum = n1 + n2 + carry\n",
    "            carry = temp_sum // 10\n",
    "            res.append(str(temp_sum % 10))\n",
    "            \n",
    "            i -= 1\n",
    "            j -= 1\n",
    "            \n",
    "        # ç»“æœæ˜¯åçš„ï¼Œè¦å€’å›æ¥\n",
    "        return \"\".join(res[::-1])\n",
    "\n",
    "# --- äº¤äº’å¼æµ‹è¯• ---\n",
    "solution = Solution()\n",
    "print(\"\\n=== æµ‹è¯•å­—ç¬¦ä¸²ç›¸åŠ  ===\")\n",
    "n1 = input(\"è¯·è¾“å…¥ç¬¬ä¸€ä¸ªæ•°å­— (å­—ç¬¦ä¸²): \").strip()\n",
    "n2 = input(\"è¯·è¾“å…¥ç¬¬äºŒä¸ªæ•°å­— (å­—ç¬¦ä¸²): \").strip()\n",
    "\n",
    "result = solution.addStrings(n1, n2)\n",
    "print(f\"è®¡ç®—ç»“æœ: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854523c",
   "metadata": {},
   "source": [
    "### 3.3 æ— é‡å¤å­—ç¬¦çš„æœ€é•¿å­ä¸² (Longest Substring Without Repeating Characters) - Medium (ç»å…¸å¿…è€ƒ)\n",
    "\n",
    "  * **LeetCode é“¾æ¥**: [3. Longest Substring Without Repeating Characters](../../LeetCode%20practice/1-50.ipynb)\n",
    "  * **AI å…³è”**: **Attention Mechanism (æ³¨æ„åŠ›æœºåˆ¶)** çš„é›å½¢ã€‚æ¨¡å‹åœ¨çœ‹åºåˆ—æ—¶ï¼Œå…³æ³¨çš„æ˜¯â€œUniqueâ€çš„ä¿¡æ¯ã€‚\n",
    "  * **æ ¸å¿ƒæ€è·¯**: **æ»‘åŠ¨çª—å£ + å“ˆå¸Œè¡¨**ã€‚\n",
    "      * å³æŒ‡é’ˆ `right` ä¸»åŠ¨æ‰©å¼ ã€‚\n",
    "      * ä¸€æ—¦é‡åˆ°é‡å¤å­—ç¬¦ï¼Œå·¦æŒ‡é’ˆ `left` è¢«åŠ¨æ”¶ç¼©ï¼ˆè·³åˆ°é‡å¤å­—ç¬¦çš„ä¸‹ä¸€ä½ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9262948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def lengthOfLongestSubstring(self, s: str) -> int:\n",
    "        dic, res, i = {}, 0, -1\n",
    "        for j in range(len(s)):\n",
    "            if s[j] in dic:\n",
    "                i = max(dic[s[j]], i) # æ›´æ–°å·¦æŒ‡é’ˆ i\n",
    "            dic[s[j]] = j # å“ˆå¸Œè¡¨è®°å½•\n",
    "            res = max(res, j - i) # æ›´æ–°ç»“æœ\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f0035",
   "metadata": {},
   "source": [
    "## 4 æ€»ç»“ä¸ä½œä¸š\n",
    "\n",
    "### ä»Šæ—¥å¤ç›˜\n",
    "\n",
    "1.  **NLP åŸºç¡€**: å­—ç¬¦ä¸² -\\> Tokenizer -\\> IDs -\\> Tensorã€‚è¿™å°±æ˜¯æ‰€æœ‰å¤§æ¨¡å‹ï¼ˆåŒ…æ‹¬æˆ‘ Geminiï¼‰ç†è§£ä½ è¯´è¯çš„ç¬¬ä¸€æ­¥ã€‚\n",
    "2.  **ç®—æ³•æŠ€å·§**:\n",
    "      * **åŒæŒ‡é’ˆä»åå¾€å‰**ï¼šå¤„ç†â€œè¿›ä½â€ç±»é—®é¢˜çš„é€šç”¨è§£æ³•ã€‚\n",
    "      * **Map + æ»‘åŠ¨çª—å£**ï¼šè§£å†³â€œä¸é‡å¤â€ã€â€œè¿ç»­å­ä¸²â€ç±»é—®é¢˜çš„æ ¸æ­¦å™¨ã€‚\n",
    "\n",
    "### è¯¾åä½œä¸š\n",
    "\n",
    "1.  è¿è¡Œå¹¶æ‰‹åŠ¨æµ‹è¯•ä»¥ä¸Šä»£ç ã€‚å»ºè®®åœ¨æµ‹è¯•â€œæ— é‡å¤å­ä¸²â€æ—¶ï¼Œè¾“å…¥ `\"pwwkew\"`ï¼Œçœ‹çœ‹ç»“æœæ˜¯ä¸æ˜¯ 3 (`wke`)ï¼Œå¹¶ç†è§£ä¸ºä»€ä¹ˆå·¦æŒ‡é’ˆä¼šè·³è·ƒã€‚\n",
    "2.  **æ€è€ƒ**: `SimpleTokenizer` é‡Œæˆ‘ä»¬ç”¨çš„æ˜¯ç©ºæ ¼åˆ†è¯ (`split()`)ã€‚ä¸­æ–‡æ²¡æœ‰ç©ºæ ¼ï¼ˆæ¯”å¦‚â€œæˆ‘çˆ±äººå·¥æ™ºèƒ½â€ï¼‰ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿï¼ˆè¿™å°†åœ¨ Stage 2 çš„ BERT å­¦ä¹ ä¸­æ­æ™“ï¼‰ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
