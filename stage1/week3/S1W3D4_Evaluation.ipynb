{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef73b27",
   "metadata": {},
   "source": [
    "# 🚀 S1W3D4 (Day 17): 编写评估循环\n",
    "\n",
    "**今日重点**：模式切换、计算图阻断、预测逻辑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901ccbc",
   "metadata": {},
   "source": [
    "## 🎯 核心原理 (一定要弄清楚！)\n",
    "\n",
    "在写代码前，必须彻底理解这三个概念，它们是面试中关于评估过程的必考题。\n",
    "\n",
    "### 🟢 原理一：`model.eval()` vs `model.train()`\n",
    "\n",
    "  * **问题**：为什么考试时要喊一声“现在是考试模式”？\n",
    "  * **原理**：神经网络中有些层在“训练”和“预测”时的表现是**完全不同**的。\n",
    "      * **Dropout**：训练时随机“关掉”一部分神经元（防止依赖），预测时必须**全开**（火力全开）。\n",
    "      * **BatchNorm**：训练时用当前 Batch 的均值方差做归一化；预测时必须用**历史累计**的全局均值方差（保证稳定性）。\n",
    "  * **结论**：如果你忘了写 `model.eval()`，你的准确率可能会莫名其妙地低，或者每次运行结果都不一样。\n",
    "\n",
    "### 🟢 原理二：`torch.no_grad()` (计算图阻断)\n",
    "\n",
    "  * **问题**：为什么评估时要加这句代码？\n",
    "  * **原理**：\n",
    "      * PyTorch 默认会记录所有操作构建“计算图”以便反向传播求导。这非常消耗显存和算力。\n",
    "      * **考试不需要求导**！我们只需要结果，不需要更新参数。\n",
    "      * `with torch.no_grad():` 就像一个开关，告诉 PyTorch：“在这个缩进块里，别浪费内存去记梯度了，我不需要反向传播。”\n",
    "  * **结论**：不加也能跑，但会**浪费巨量显存**，甚至导致 Out of Memory (OOM)，且速度变慢。\n",
    "\n",
    "### 🟢 原理三：Logits 到 预测结果 (`argmax`)\n",
    "\n",
    "  * **问题**：模型输出的是 `[-1.2, 2.5, 0.1, ...]` 这一堆数，怎么变成数字 \"1\"？\n",
    "  * **原理**：\n",
    "      * 模型输出的叫 **Logits** (得分)。得分越高，概率越大。\n",
    "      * 我们不需要用 Softmax 算出具体概率（比如 80%），我们只需要知道\\*\\*“哪个下标的得分最高”\\*\\*。\n",
    "      * **`torch.argmax(output, dim=1)`**：这个函数会返回最大值的**索引 (Index)**。\n",
    "      * 例如：`[0.1, 0.9, 0.0]` -\\> 最大值是 0.9 -\\> 它的索引是 **1** -\\> 预测结果就是数字 1。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57018d",
   "metadata": {},
   "source": [
    "## 💻 准备工作：环境与导入\n",
    "\n",
    "首先需要设置好`src`的路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fe00b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 根目录准备就绪\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- 1. 挂载项目根目录 ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\")) \n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# --- 2. 导入 src 模块 ---\n",
    "from src.utils import get_data_loaders\n",
    "from src.models import SimpleCNN\n",
    "from src.engine import train, evaluate  # 导入引擎\n",
    "\n",
    "# --- 3. 准备设备 ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 4. 加载数据与模型 ---\n",
    "# 这里的 test_loader 是今天的重点！\n",
    "train_loader, test_loader = get_data_loaders(batch_size=64, data_root=os.path.join(project_root, 'data'))\n",
    "\n",
    "# 实例化模型\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# 此时模型是随机初始化的（很笨），预测准确率应该在 10% 左右\n",
    "print(\"✅ 根目录准备就绪\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61476443",
   "metadata": {},
   "source": [
    "## 📝 编写评估函数 (Evaluation Function)\n",
    "\n",
    "已写入`../../src/models/engine.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# def evaluate(model, device, test_loader):\n",
    "#     # 1. 切换模式 (原理一)\n",
    "#     model.eval()\n",
    "    \n",
    "#     # 定义统计变量\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "    \n",
    "#     # 定义损失函数 (用于计算测试集的平均 Loss)\n",
    "#     criterion = nn.CrossEntropyLoss(reduction='sum') \n",
    "#     # 注意：这里用了 reduction='sum'，表示把一个 Batch 的 Loss 加起来，而不是求平均。\n",
    "#     # 这样方便最后除以总样本数。\n",
    "    \n",
    "#     # 2. 阻断计算图 (原理二)\n",
    "#     with torch.no_grad():\n",
    "#         # 遍历测试集\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "            \n",
    "#             # 前向传播\n",
    "#             output = model(data)\n",
    "            \n",
    "#             # 计算 Batch 总 Loss\n",
    "#             test_loss += criterion(output, target).item()\n",
    "            \n",
    "#             # 3. 获取预测结果 (原理三)\n",
    "#             # output 形状: [64, 10]\n",
    "#             # dim=1 表示在“列”方向（10个类别）找最大值\n",
    "#             pred = output.argmax(dim=1, keepdim=True) \n",
    "            \n",
    "#             # 计算正确个数\n",
    "#             # pred.eq(target) 会对比预测值和真实值，相等的为 True(1)，不等的为 False(0)\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     # 计算平均 Loss 和 准确率\n",
    "#     test_loss /= len(test_loader.dataset) # 平均 Loss = 总 Loss / 总样本数\n",
    "#     accuracy = 100. * correct / len(test_loader.dataset) # 准确率 %\n",
    "\n",
    "#     print(f'\\n🔴 [Test Set] Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    \n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4295971",
   "metadata": {},
   "source": [
    "## 🧪 4. 运行测试 (Baseline)\n",
    "\n",
    "现在我们的模型还没训练，是一张白纸。我们先测一下，看看它是不是只有 10% 的准确率（瞎猜）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372ccd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔴 [Test Set] Average loss: 2.3056, Accuracy: 740/10000 (7.40%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这里的 model 是刚初始化的\n",
    "acc = evaluate(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae235e",
   "metadata": {},
   "source": [
    "## 🔄 5. 终极融合：训练 + 评估 (Train & Eval Loop)\n",
    "\n",
    "现在我们将昨天的 **Train** 和今天的 **Evaluate** 结合起来。\n",
    "**这是深度学习脚本的完全体形态。**\n",
    "\n",
    "我们希望：**每训练完一个 Epoch，就立刻在测试集上考一次试，看看有没有进步。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d5a365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [50/938 (5%)]\tLoss: 0.531822\n",
      "Train Epoch: 1 [100/938 (11%)]\tLoss: 0.330147\n",
      "Train Epoch: 1 [150/938 (16%)]\tLoss: 0.116773\n",
      "Train Epoch: 1 [200/938 (21%)]\tLoss: 0.270419\n",
      "Train Epoch: 1 [250/938 (27%)]\tLoss: 0.219862\n",
      "Train Epoch: 1 [300/938 (32%)]\tLoss: 0.091976\n",
      "Train Epoch: 1 [350/938 (37%)]\tLoss: 0.236950\n",
      "Train Epoch: 1 [400/938 (43%)]\tLoss: 0.054950\n",
      "Train Epoch: 1 [450/938 (48%)]\tLoss: 0.041809\n",
      "Train Epoch: 1 [500/938 (53%)]\tLoss: 0.065647\n",
      "Train Epoch: 1 [550/938 (59%)]\tLoss: 0.211401\n",
      "Train Epoch: 1 [600/938 (64%)]\tLoss: 0.068390\n",
      "Train Epoch: 1 [650/938 (69%)]\tLoss: 0.011472\n",
      "Train Epoch: 1 [700/938 (75%)]\tLoss: 0.080514\n",
      "Train Epoch: 1 [750/938 (80%)]\tLoss: 0.033956\n",
      "Train Epoch: 1 [800/938 (85%)]\tLoss: 0.054457\n",
      "Train Epoch: 1 [850/938 (91%)]\tLoss: 0.074679\n",
      "Train Epoch: 1 [900/938 (96%)]\tLoss: 0.084595\n",
      "\n",
      "🔴 [Test set] Average loss: 0.0668, Accuracy: 9787/10000 (97.87%)\n",
      "\n",
      "Train Epoch: 2 [50/938 (5%)]\tLoss: 0.117493\n",
      "Train Epoch: 2 [100/938 (11%)]\tLoss: 0.052727\n",
      "Train Epoch: 2 [150/938 (16%)]\tLoss: 0.061274\n",
      "Train Epoch: 2 [200/938 (21%)]\tLoss: 0.076345\n",
      "Train Epoch: 2 [250/938 (27%)]\tLoss: 0.059854\n",
      "Train Epoch: 2 [300/938 (32%)]\tLoss: 0.023377\n",
      "Train Epoch: 2 [350/938 (37%)]\tLoss: 0.170021\n",
      "Train Epoch: 2 [400/938 (43%)]\tLoss: 0.087984\n",
      "Train Epoch: 2 [450/938 (48%)]\tLoss: 0.067819\n",
      "Train Epoch: 2 [500/938 (53%)]\tLoss: 0.008285\n",
      "Train Epoch: 2 [550/938 (59%)]\tLoss: 0.012358\n",
      "Train Epoch: 2 [600/938 (64%)]\tLoss: 0.047114\n",
      "Train Epoch: 2 [650/938 (69%)]\tLoss: 0.009357\n",
      "Train Epoch: 2 [700/938 (75%)]\tLoss: 0.071844\n",
      "Train Epoch: 2 [750/938 (80%)]\tLoss: 0.087732\n",
      "Train Epoch: 2 [800/938 (85%)]\tLoss: 0.003857\n",
      "Train Epoch: 2 [850/938 (91%)]\tLoss: 0.014915\n",
      "Train Epoch: 2 [900/938 (96%)]\tLoss: 0.107729\n",
      "\n",
      "🔴 [Test set] Average loss: 0.0448, Accuracy: 9851/10000 (98.51%)\n",
      "\n",
      "Train Epoch: 3 [50/938 (5%)]\tLoss: 0.020489\n",
      "Train Epoch: 3 [100/938 (11%)]\tLoss: 0.092217\n",
      "Train Epoch: 3 [150/938 (16%)]\tLoss: 0.002607\n",
      "Train Epoch: 3 [200/938 (21%)]\tLoss: 0.037575\n",
      "Train Epoch: 3 [250/938 (27%)]\tLoss: 0.099038\n",
      "Train Epoch: 3 [300/938 (32%)]\tLoss: 0.009665\n",
      "Train Epoch: 3 [350/938 (37%)]\tLoss: 0.016750\n",
      "Train Epoch: 3 [400/938 (43%)]\tLoss: 0.091459\n",
      "Train Epoch: 3 [450/938 (48%)]\tLoss: 0.006461\n",
      "Train Epoch: 3 [500/938 (53%)]\tLoss: 0.015240\n",
      "Train Epoch: 3 [550/938 (59%)]\tLoss: 0.006559\n",
      "Train Epoch: 3 [600/938 (64%)]\tLoss: 0.129524\n",
      "Train Epoch: 3 [650/938 (69%)]\tLoss: 0.019970\n",
      "Train Epoch: 3 [700/938 (75%)]\tLoss: 0.013423\n",
      "Train Epoch: 3 [750/938 (80%)]\tLoss: 0.056138\n",
      "Train Epoch: 3 [800/938 (85%)]\tLoss: 0.010847\n",
      "Train Epoch: 3 [850/938 (91%)]\tLoss: 0.017695\n",
      "Train Epoch: 3 [900/938 (96%)]\tLoss: 0.135573\n",
      "\n",
      "🔴 [Test set] Average loss: 0.0366, Accuracy: 9877/10000 (98.77%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# --- 主循环 ---\n",
    "# 重新初始化模型和优化器\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # 1. 上课 (训练)\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval = 50)\n",
    "    \n",
    "    # 2. 考试 (评估)\n",
    "    acc = evaluate(model, device, test_loader)\n",
    "    \n",
    "    # 这里可以加一个“保存模型”的逻辑：如果 acc 是历史最高的，就 save 下来"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d3ff1",
   "metadata": {},
   "source": [
    "应该会看到：\n",
    "> Epoch 1 结束 -\\> Accuracy 从 10% 飙升到 95% 以上。\n",
    "> Epoch 2 结束 -\\> Accuracy 继续微涨，可能到 98%。\n",
    "> Epoch 3 结束 -\\> 收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed574f0d",
   "metadata": {},
   "source": [
    "## 🧠 今日复盘 (Self-Check)\n",
    "\n",
    "请对着代码回答以下问题，确认你是否真的“弄清楚了原理”：\n",
    "\n",
    "1.  **`argmax(dim=1)` 是什么意思？**\n",
    "      * 它是在找“概率最大”的那个类别的**索引**。比如 `[0.1, 0.8, 0.1]`，最大值 0.8 在索引 1，所以预测结果是 1。\n",
    "2.  **如果在 `evaluate` 函数里删掉 `with torch.no_grad():` 会怎样？**\n",
    "      * 代码也能跑，但是会消耗大量显存（因为记录了梯度），而且运行速度会变慢。\n",
    "3.  **为什么要除以 `len(test_loader.dataset)`？**\n",
    "      * 因为我们在算 Loss 时用了 `sum`（累加），为了得到“平均每个样本的 Loss”，必须除以样本总数。\n",
    "\n",
    "如果这三个问题都难不倒你，那么 S1W3D4 任务完成！你的模型现在既能学，又能考，已经是一个成熟的 AI 了！🤖"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
