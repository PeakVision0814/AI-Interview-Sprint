{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6995a39",
   "metadata": {},
   "source": [
    "# S1W2D3 - PyTorch 核心：`Tensor`与`Autograd`\n",
    "\n",
    "**今日目标**\n",
    "\n",
    "1. 熟练掌握`torch.Tensor`的基本操作，把它当作NumPy来使用\n",
    "2. 理解并亲手实践PyTorch的自动求导机制（Autograd）\n",
    "3. 搞清楚`requires_gard`，`.backward()`和`.gard`之间的“三角关系”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d809693",
   "metadata": {},
   "source": [
    "## PyTorch 的“砖块”：`torch.Tensor` (张量)**\n",
    "\n",
    "`Tensor` 是 PyTorch 中最基本的数据结构，它就是一个**多维数组**。\n",
    "  * **它和 NumPy 的 `ndarray` 有什么关系？**\n",
    "\n",
    "      * 它们几乎一模一样。你可以像操作 NumPy 数组一样对 `Tensor` 进行索引、切片、数学运算。\n",
    "      * 它们甚至可以高效地互相转换。\n",
    "\n",
    "  * **它和 NumPy 的 `ndarray` 有什么区别？(核心)**\n",
    "\n",
    "    1.  **GPU 加速**： `Tensor` 可以被无缝地移动到 NVIDIA GPU 上进行计算，实现百倍千倍的加速。\n",
    "    2.  **自动求导**： `Tensor` 是 PyTorch 自动求导系统 `Autograd` 的核心。\n",
    "\n",
    "**代码实践：`Tensor` 的创建与操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a028824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 (from list):\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Tensor 创建 ---\n",
    "# 1. 从 Python 列表创建\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"t1 (from list):\\n{t1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89956d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2 (zero):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "t3 (zero):\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. 创建全零或全一（需要指定形状）\n",
    "t2 = torch.zeros((2, 3))  # 2行3列\n",
    "t3 = torch.ones((2, 3)) # 2行3列\n",
    "print(f\"t2 (zero):\\n{t2}\\n\")\n",
    "print(f\"t3 (zero):\\n{t3}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c2f0131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3 (randn):\n",
      "tensor([[-0.7883, -0.7182, -0.6684],\n",
      "        [-1.4609,  0.5208,  1.6947]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. 创建随机数（非常实用）\n",
    "t4 = torch.randn((2,3))   # 随机数以0为均值，满足正态分布\n",
    "print(f\"t3 (randn):\\n{t4}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61013119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5 (from numpy):\n",
      "tensor([1, 2, 3])\n",
      "\n",
      "Back to numpy:\n",
      "[1 2 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. 和 NumPy 互相转换\n",
    "np_array = np.array([1, 2, 3])\n",
    "t5 = torch.from_numpy(np_array) # numpy -> tensor\n",
    "print(f\"t5 (from numpy):\\n{t5}\\n\")\n",
    "\n",
    "np_array_back = t5.numpy()      # tensor -> numpy\n",
    "print(f\"Back to numpy:\\n{np_array_back}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ec87aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 shape: torch.Size([2, 3])\n",
      "t1 dtype: torch.int64\n",
      "t4 dtype: torch.float32\n",
      "t1的第一行: tensor([1, 2, 3])\n",
      "t1的[0, 2]元素: 3\n",
      "\n",
      "Tensor相加:\n",
      "tensor([[4, 4],\n",
      "        [6, 6]])\n",
      "\n",
      "Tensor逐元素相乘:\n",
      "tensor([[3, 3],\n",
      "        [8, 8]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Tensor 属性与操作 ---\n",
    "print(f\"t1 shape: {t1.shape}\")      # 形状\n",
    "print(f\"t1 dtype: {t1.dtype}\")      # 数据类型 (默认 torch.int64)\n",
    "print(f\"t4 dtype: {t4.dtype}\")      # (默认 torch.float32)\n",
    "\n",
    "# 索引 (和NumPy一样)\n",
    "print(f\"t1的第一行: {t1[0]}\")\n",
    "print(f\"t1的[0, 2]元素: {t1[0, 2]}\\n\")\n",
    "\n",
    "# 数学运算 (和NumPy一样)\n",
    "t_a = torch.tensor([[1, 1], [2, 2]])\n",
    "t_b = torch.tensor([[3, 3], [4, 4]])\n",
    "print(f\"Tensor相加:\\n{t_a + t_b}\\n\")\n",
    "print(f\"Tensor逐元素相乘:\\n{t_a * t_b}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b764037",
   "metadata": {},
   "source": [
    "## 2. 通往 GPU 的“桥梁”：`.to('cuda')`\n",
    "\n",
    "这是 PyTorch 相比 NumPy 的**巨大优势**。要让计算在你的 RTX 3050 上飞起来，你只需要做一件事：把你的数据和模型都“搬”到 GPU 上。\n",
    "\n",
    "**代码实践 2：检查并使用 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc197b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们将要使用的设备是: cuda\n",
      "t_cpu 所在的设备: cpu\n",
      "t_gpu 所在的设备: cuda:0\n",
      "GPU 上的计算结果:\n",
      "tensor([[-1.6437, -0.2288],\n",
      "        [ 0.2626, -0.8079]], device='cuda:0')\n",
      "搬回 CPU 后的结果:\n",
      "tensor([[-1.6437, -0.2288],\n",
      "        [ 0.2626, -0.8079]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 检查 CUDA (NVIDIA GPU) 是否可用\n",
    "# 这是 PyTorch 代码的“标准开头”\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"我们将要使用的设备是: {device}\")\n",
    "\n",
    "# 2. 创建一个 Tensor，默认在 CPU 上\n",
    "t_cpu = torch.randn((2, 2))\n",
    "print(f\"t_cpu 所在的设备: {t_cpu.device}\")\n",
    "\n",
    "# 3. 将 Tensor 移动到 GPU 上\n",
    "if device == 'cuda':\n",
    "    t_gpu = t_cpu.to(device) # 或者 .to('cuda')\n",
    "    print(f\"t_gpu 所在的设备: {t_gpu.device}\")\n",
    "    \n",
    "    # 警告：CPU 上的 Tensor 不能和 GPU 上的 Tensor 直接计算\n",
    "    # 下面这行会报错:\n",
    "    # t_cpu + t_gpu \n",
    "    \n",
    "    # 必须在同一设备上\n",
    "    t_gpu_2 = torch.randn((2, 2)).to(device)\n",
    "    result_gpu = t_gpu + t_gpu_2\n",
    "    print(f\"GPU 上的计算结果:\\n{result_gpu}\")\n",
    "    \n",
    "    # 把结果从 GPU 搬回 CPU (比如为了用 numpy 或 print)\n",
    "    result_cpu = result_gpu.cpu()\n",
    "    print(f\"搬回 CPU 后的结果:\\n{result_cpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d19e36",
   "metadata": {},
   "source": [
    "## 3. PyTorch 的“魔法”：`Autograd` 自动求导\n",
    "\n",
    "这是今天的**核心**。`Autograd` 就是 PyTorch 对“反向传播”的工程实现。你再也不用手动算梯度了，PyTorch 会帮你搞定一切。\n",
    "\n",
    "**它是如何工作的？**\n",
    "当你告诉 PyTorch 某个 `Tensor` **需要梯度** 时（通过设置 `requires_grad=True`），PyTorch 就会在后台默默地为你构建一个**计算图 (Computation Graph)**。这个图记录了所有与该 `Tensor` 相关的计算。\n",
    "\n",
    "当你对最终的计算结果（比如 `loss`）调用 `.backward()` 时，PyTorch 就会沿着这个图反向传播，自动计算出图中所有 `requires_grad=True` 的 `Tensor` 的梯度，并把它们存放在各自的 `.grad` 属性中。\n",
    "\n",
    "**三个关键点：**\n",
    "\n",
    "1.  **`requires_grad=True`**: 像一个“开关”，告诉 `Autograd`：“请开始追踪我对这个 `Tensor` 的所有操作”。（模型参数默认是 `True`，输入数据默认是 `False`）。\n",
    "2.  **`loss.backward()`**: 像一个“扳机”，触发反向传播。PyTorch 会自动计算所有被追踪的 `Tensor` 的梯度。\n",
    "3.  **`tensor.grad`**: 像一个“信箱”，用来存放 `backward()` 计算出的梯度值。\n",
    "\n",
    "**代码实践 3：亲手实践自动求导**\n",
    "\n",
    "我们将手动复现昨天的理论：计算函数 $y = x^2 + 2x + 1$ 在 $x=2$ 时的导数 $\\frac{dy}{dx}$。\n",
    "（我们手动计算：$\\frac{dy}{dx} = 2x + 2$，当 $x=2$ 时，导数应为 $2 \\times 2 + 2 = 6$）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65af279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 2.0\n",
      "\n",
      "y (计算结果): 9.0\n",
      "\n",
      "x 的梯度 (dy/dx): 6.0\n",
      "\n",
      "--- 实践 4: 梯度累加 ---\n",
      "第二次 backward 后 x 的梯度: 11.0\n",
      "\n",
      "清零后 x 的梯度: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 创建 x，并打开“追踪”开关\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(f\"x: {x}\\n\")\n",
    "\n",
    "# 2. 定义函数 y\n",
    "# PyTorch 会自动记录这个计算过程\n",
    "y = x**2 + 2*x + 1\n",
    "print(f\"y (计算结果): {y}\\n\")\n",
    "\n",
    "# 3. 触发反向传播\n",
    "# PyTorch 会自动计算 dy/dx\n",
    "y.backward()\n",
    "\n",
    "# 4. 查看 x 的梯度 (结果存放在 .grad 属性中)\n",
    "# 结果应该是 6\n",
    "print(f\"x 的梯度 (dy/dx): {x.grad}\\n\") \n",
    "\n",
    "\n",
    "# --- 代码实践 4：(重要!) 梯度的累加特性 ---\n",
    "print(\"--- 实践 4: 梯度累加 ---\")\n",
    "# 假设我们又进行了一次计算 (比如在训练的下一个step)\n",
    "# 注意：x.grad 里现在的值是 6\n",
    "y2 = x**2 + x \n",
    "# (y2 的导数是 2x + 1，在 x=2 时，导数是 5)\n",
    "# x.grad.zero_()\n",
    "y2.backward()\n",
    "\n",
    "# 再次查看 x.grad\n",
    "# 你觉得会是 5 吗？\n",
    "print(f\"第二次 backward 后 x 的梯度: {x.grad}\\n\")\n",
    "# 结果会是 11 (即 6 + 5)\n",
    "\n",
    "# **结论 (面试高频):**\n",
    "# PyTorch 的梯度在 .grad 中是默认 **累加 (accumulate)** 的。\n",
    "# 这就是为什么在每个训练循环 (training loop) 的开始，\n",
    "# 我们必须显式地调用 `optimizer.zero_grad()` 来清空上一轮的梯度！\n",
    "\n",
    "# 如何手动清零？\n",
    "x.grad.zero_() # 使用 .zero_() (带下划线表示 in-place 原地操作)\n",
    "print(f\"清零后 x 的梯度: {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6621cb",
   "metadata": {},
   "source": [
    "## W2D3 今日行动清单\n",
    "\n",
    "1.  **✅ 运行代码：** 亲手将上面 4 个实践代码块在你的环境中（确保 PyTorch 和 CUDA 已装好）运行一遍。\n",
    "2.  **✅ 理解 GPU：** 确保你理解 `t.to(device)` 的作用，以及为什么 CPU 和 GPU 的张量不能直接通信。\n",
    "3.  **✅ 掌握 Autograd：** **这是今天的核心！** 确保你亲眼看到了 `x.grad` 是如何从 `None` 变为 `6.0`，再变为 `11.0`，最后又被清零的。这个小实验是理解整个 PyTorch 训练循环的关键。\n",
    "4.  **✅ 思考题 (面试模拟)：**\n",
    "      * “在 PyTorch 的训练循环中，为什么 `optimizer.zero_grad()` 这一行是必需的？它通常被放在哪里？”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
