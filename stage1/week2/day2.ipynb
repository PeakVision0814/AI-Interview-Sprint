{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3cfea9",
   "metadata": {},
   "source": [
    "# S1W2D2：神经网络\n",
    "\n",
    "**今日目标**：透彻理解模型训练的完整流程，能用自己的话清晰地向面试官解释从“犯错”到“改正”的全部过程。\n",
    "\n",
    "## 定义目标 - 损失函数（Loss Function）\n",
    "\n",
    "如果不知道“好”与“坏”的标准，模型就无法学习。损失函数就是这个标准，它告诉我们模型当前“错得有多离谱”。\n",
    "\n",
    "- 核心作用：将模型的预测结果与真实标签进行比较，输出一个**标量（单个数值）**来量化它们之间的差距。这个数值越大，说明模型错得越严重。\n",
    "- 训练目标: 我们的整个训练过程，就是为了调整参数（$w$ 和 $b$），让这个损失值尽可能地小。\n",
    "\n",
    "两种核心的损失函数（面试高频）\n",
    "\n",
    "1. 均方误差损失（Mean Squared Error，MSE Loss）：\n",
    "  - **适用场景**：回归任务（预测连续值，如房价、温度）。\n",
    "  - **计算方式**：$L(y, \\hat{y}) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "  - **直观理解**：计算每个样本“真实值($y$)”与“预测值 ($\\hat{y}$)”之差的平方，然后求平均。平方的作用是为了让差值都是正数，并且放大较大的误差。\n",
    "  - **PyTorch代码**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1318ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 62.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "real_prices = torch.tensor([100.0, 150.0])\n",
    "predicted_prices = torch.tensor([110.0, 145.0]) # 预测有一定误差\n",
    "\n",
    "loss = loss_fn(predicted_prices, real_prices)\n",
    "print(f\"MSE Loss: {loss}\") # 输出: tensor(62.5000), ((110-100)^2 + (145-150)^2) / 2 = (100+25)/2 = 62.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60c36f",
   "metadata": {},
   "source": [
    "2. 交叉熵损失 (Cross-Entropy Loss):\n",
    "\n",
    "- **适用场景**: 分类任务 (预测离散类别，如 MNIST 数字识别、猫狗分类)。\n",
    "- **直观理解**: 它衡量的是模型预测的“概率分布”与“真实标签的概率分布”之间的距离。如果模型对正确类别的预测概率很高（例如，图片是猫，模型预测猫的概率是 0.99），那么交叉熵损失就很小；反之则很大。\n",
    "- **重要提示**: 在 PyTorch 中，nn.CrossEntropyLoss 已经内置了 Softmax 操作。因此，模型输出层不需要手动添加 Softmax 层。\n",
    "- **PyTorch代码**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d48086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Entropy Loss: 0.24131132662296295\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 假设是3分类任务, batch_size=1\n",
    "# 模型的原始输出 (logits)，不需要手动做 Softmax\n",
    "model_outputs = torch.tensor([[2.0, -1.0, 0.5]]) # batch_size=1, num_classes=3\n",
    "# 真实标签 (正确类别是 0)\n",
    "real_labels = torch.tensor([0]) \n",
    "\n",
    "loss = loss_fn(model_outputs, real_labels)\n",
    "print(f\"Cross-Entropy Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6cc144",
   "metadata": {},
   "source": [
    "## 找到方向 - 梯度下降 (Gradient Descent)\n",
    "\n",
    "我们有了损失这个“目标”，现在需要一种方法来让损失变小。梯度下降就是这个方法。\n",
    "\n",
    "- 核心思想 (下山比喻)：\n",
    "  - **当前参数**($w, b$) 决定了你在损失函数这座“山”上的位置。\n",
    "  - **损失值**就是你所在位置的海拔。\n",
    "  - **梯度** (Gradient) 是一个向量，指向函数值上升最快的方向。那么，梯度的反方向就是函数值下降最快的方向，也就是“最陡的下坡方向”。\n",
    "  - **学习率** (Learning Rate, $\\alpha$) 就是你朝着下坡方向迈出的那一步的步长。\n",
    "  - **参数更新**: 不断重复 新参数 = 旧参数 - 学习率 * 梯度 这个过程，就能一步步走到山谷（损失最小值）。\n",
    "- **学习率** ($\\alpha$) 的重要性 (面试高频):\n",
    "  - 学习率太大: 步子迈得太大，可能会直接“跨过”山谷最低点，导致损失值在最低点附近来回震荡，甚至越来越大，无法收敛。\n",
    "  - 学习率太小: 步子太小，下山速度会非常慢，导致训练时间过长。\n",
    "- 核心变种：随机梯度下降 (Stochastic Gradient Descent, SGD)\n",
    "  - 传统的梯度下降需要计算整个训练集的损失才能更新一次参数，当数据集很大时，这非常耗时。\n",
    "  - SGD的思想是，每次只随机取一小批 (mini-batch) 数据（例如 64 张图片）来计算损失和梯度，并立即更新参数。这样做虽然每次更新的方向不一定绝对正确，但速度极快，总体上是朝着正确的方向前进的。我们现在使用的几乎都是 SGD 及其变体（如 Adam）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5ca54",
   "metadata": {},
   "source": [
    "## 高效计算 - 反向传播 (Backpropagation)\n",
    "我们知道了要沿着“梯度”下降，但一个百万级参数的网络，如何得到损失函数对每一个参数的梯度呢？手动求导是不可能的。\n",
    "- **核心作用**: 它不是一种优化算法，而是一种高效计算梯度的算法。\n",
    "- **工作原理**: 基于微积分的链式法则。它从最终的损失值开始，将“误差”从输出层反向传播到隐藏层，再到输入层，沿途计算出损失对网络中每一个参数（$w$ 和 $b$）的偏导数（即梯度）。\n",
    "- 你在 PyTorch 中的体验: 这个复杂的过程被封装成了一个极其简单的命令：\n",
    "\n",
    "```python\n",
    "# loss 是你计算出的损失值张量\n",
    "loss.backward() \n",
    "\n",
    "# 执行完上面这行后，所有 requires_grad=True 的参数\n",
    "# (比如 a_layer.weight) 的 .grad 属性就会被填上计算好的梯度值\n",
    "# print(a_layer.weight.grad)\n",
    "```\n",
    "\n",
    "- **面试要点**: 你不需要会手写反向传播的公式。但你必须清楚地知道：反向传播是为了高效计算梯度，而梯度下降是利用这些梯度来更新模型的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34faca7c",
   "metadata": {},
   "source": [
    "## 补充知识：数据划分 (The Playground Rules)\n",
    "\n",
    "为了科学地训练和评估模型，我们必须将数据集划分为三个独立的集合：\n",
    "\n",
    "1. 训练集 (Training Set):\n",
    "  - 作用: 模型学习的唯一数据来源。用于计算损失、计算梯度、更新模型参数。\n",
    "  - 比喻: 学生用来学习的课本和习题集。\n",
    "\n",
    "2. 验证集 (Validation Set):\n",
    "  - 作用: 在训练过程中，用来评估模型在“未见过”的数据上的表现，以便我们调整超参数（如学习率、网络层数等），并判断模型是否过拟合。\n",
    "  - 重要: 它不参与梯度的计算和参数的更新。\n",
    "  - 比喻: 学生用来检验学习效果的模拟考试。考得不好，可以回去调整学习方法（调整超参数），再重新看书做题。\n",
    "3. 测试集 (Test Set):\n",
    "  - 作用: 在模型训练完全结束后，用来评估模型的最终性能的“终极考卷”。\n",
    "  - 重要: 在整个训练和调参过程中，绝对不能使用测试集。 否则，就像是提前知道了考题答案，评估结果是虚高的、无效的。\n",
    "  - 比喻: 学生的最终高考。一考定音，报告最终成绩。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c2061",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08a161cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042352d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
