{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8eed1f7",
   "metadata": {},
   "source": [
    "# S1W2D1：神经网络的核心理论\n",
    "\n",
    "**今日目标**：建立对神经网络的宏观认知，理解它的基本组成部分和工作原理\n",
    "\n",
    "## 1. 核心比喻：神经网络是一个能从数据中学习的“超级函数”\n",
    "\n",
    "忘掉所有复杂的属于，先把神经网络想象成巨大的、拥有上千万个可调“旋钮”的函数盒子：\n",
    "\n",
    "$$\n",
    "f(输入) = 输出\n",
    "$$\n",
    "\n",
    "- **输入（Input）**：你为给它的数据，比如一张图片的像素值矩阵。\n",
    "- **输出（Output）**：它给出的预测结果，比如这张图片是“猫”的概率是0.9。\n",
    "- **“旋钮”（Knobs）**：这就是神经网络是参数（主要是权重`weights`和偏置`biases`）。\n",
    "- **学习（Learning）**：“训练”的过程，就是让机器看海量的数据（例如，成千上万张猫的图片），并自动地、一点一点地微调所有这些“旋钮”，知道这个函数盒子的输出越来越接近正确答案\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad246cfe",
   "metadata": {},
   "source": [
    "## 2. 神经网络的“解剖学”：基本构成\n",
    "\n",
    "一个典型的神经网络是由“层（Laters）”构成，信息从左到右单向流动。\n",
    "\n",
    "- **输入层（Input Layer）**：\n",
    "  - **职责**：接收原始数据，是网络的入口。\n",
    "  - **神经元数量**：由输入数据的特征维度决定。\n",
    "  - **例子**：对于 MNIST 数据集，每张图片是$28\\times 28$的灰度图，拉平成一维向量后，输入层就有$784$个神经元。\n",
    "- **隐藏层 (Hidden Layers)**:\n",
    "  - **职责**：网络的核心，负责从数据中提取越来越复杂的特征。是“深度学习”中“深”的来源。\n",
    "  - **例子**：第一层隐藏层可能只学习到边缘、颜色块等简单特征；更深的隐藏层则能将这些简单特征组合成眼睛、鼻子等更复杂的模式。\n",
    "- **输出层 (Output Layer)**:\n",
    "  - **职责**: 输出最终的预测结果。\n",
    "  - **神经元数量**: 由任务目标决定。\n",
    "  - **回归任务** (预测房价): 1 个神经元。\n",
    "  - **多分类任务** (识别 0-9 这 10 个数字): 10 个神经元，每个神经元对应一个类别的得分或概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261d752",
   "metadata": {},
   "source": [
    "## 3. 核心计算单元：神经元（Neuron）\n",
    "\n",
    "每个神经元的工作都可以简化为两步：\n",
    "\n",
    "1.  **线性计算 (加权求和):** 将所有输入 $x_i$ 乘以对应的权重 $w_i$，然后加上一个偏置 $b$。\n",
    "    $$\n",
    "    z = (x_1w_1 + x_2w_2 + \\dots + x_nw_n) + b\n",
    "    $$\n",
    "\n",
    "    - **权重 (Weight, $w$):** 代表了某个输入特征的重要性。权重越大，说明该特征对神经元的决策影响越大。\n",
    "    - **偏置 (Bias, $b$):** 像一个激活的“阀门”或“阈值”，控制神经元被激活的难易程度。\n",
    "\n",
    "2.  **非线性激活 (Activation):** 将线性计算的结果 $z$ 投入一个**激活函数**，得到神经元的最终输出。\n",
    "    $$\n",
    "    \\text{output} = \\text{ActivationFunction}(z)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3318ef98",
   "metadata": {},
   "source": [
    "## 4. 网络的灵魂：激活函数 (Activation Function)\n",
    "\n",
    "这是**面试必考点**。\n",
    "\n",
    "- **Q: 为什么必须要有激活函数？**\n",
    "\n",
    "- **A:** 为了给网络引入**非线性**。如果没有激活函数，无论神经网络有多少层，其本质都等同于一个单层的线性模型。它只能解决线性可分的问题（像用一条直线去分类数据点）。而现实世界的绝大多数问题都是非线性的，因此必须通过非线性激活函数来赋予网络拟合复杂函数的能力。\n",
    "\n",
    "**今日主角**：ReLU (Rectified Linear Unit, 修正线性单元)\n",
    "\n",
    "- **公式:** $f(x) = \\max(0, x)$\n",
    "- **解释:** 像一个“门卫”，如果输入值大于 0，就放行（原样输出）；如果小于等于 0，就拦住（输出 0）。\n",
    "- **优点:**\n",
    "  1.  **计算高效:** 只是一个简单的比较和取值操作。\n",
    "  2.  **缓解梯度消失:** 在正数区域，其导数恒为 1，这有助于梯度在深层网络中更顺畅地传播，让网络更容易训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2de62d",
   "metadata": {},
   "source": [
    "\n",
    "## S1W2D1 代码实践\n",
    "\n",
    "我们将用 PyTorch 来模拟上面学到的概念。PyTorch 是我们接下来要深入学习的框架，提前熟悉它的“感觉”很重要。\n",
    "\n",
    "**目标**： 不构建完整模型，只用代码来理解“神经元”、“层”和“激活函数”是如何工作的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebecf21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 实践 1: 模拟一个神经元 ---\n",
      "输入 (x): tensor([1.1607, 1.0556, 2.2617])\n",
      "权重 (w): tensor([1.0346, 2.1122, 1.0194])\n",
      "偏置 (b): tensor([-0.3751])\n",
      "线性计算结果 (z = w*x + b): tensor([5.3609])\n",
      "经过ReLU激活后的输出: tensor([5.3609])\n"
     ]
    }
   ],
   "source": [
    "# 导入 pytorch 库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- 实践 1: 模拟一个神经元的计算 ---\n",
    "print(\"--- 实践 1: 模拟一个神经元 ---\")\n",
    "# 假设我们有3个输入特征\n",
    "# torch.randn(3) 会创建一个包含3个随机数的一维张量(向量)\n",
    "input_features = torch.randn(3) \n",
    "print(f\"输入 (x): {input_features}\")\n",
    "\n",
    "# 每个输入特征都有一个对应的权重\n",
    "weights = torch.randn(3)\n",
    "print(f\"权重 (w): {weights}\")\n",
    "\n",
    "# 还有一个偏置项\n",
    "bias = torch.randn(1)\n",
    "print(f\"偏置 (b): {bias}\")\n",
    "\n",
    "# Step 1: 线性计算 (加权求和)\n",
    "# torch.dot() 计算两个向量的点积\n",
    "linear_result = torch.dot(input_features, weights) + bias\n",
    "print(f\"线性计算结果 (z = w*x + b): {linear_result}\")\n",
    "\n",
    "# Step 2: 非线性激活 (使用ReLU)\n",
    "# torch.relu() 就是PyTorch内置的ReLU函数\n",
    "output = torch.relu(linear_result)\n",
    "print(f\"经过ReLU激活后的输出: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544ba78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 实践 2: 使用 nn.Linear 层 ---\n",
      "一个批次的输入数据 (shape: torch.Size([2, 3])):\n",
      "tensor([[-1.5703, -0.5439, -0.4459],\n",
      "        [ 0.7427,  0.6890, -0.0231]])\n",
      "数据经过 nn.Linear 层后的输出 (shape: torch.Size([2, 4])):\n",
      "tensor([[ 0.5433, -0.1280,  0.5139, -0.6139],\n",
      "        [ 0.4668, -0.5509, -0.6062,  0.8006]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# --- 实践 2: 使用PyTorch的 nn.Linear 层来抽象一个“层” ---\n",
    "# nn.Linear 帮我们自动处理了权重和偏置的创建和计算\n",
    "print(\"--- 实践 2: 使用 nn.Linear 层 ---\")\n",
    "# 定义一个层：输入特征维度是3，输出特征维度是4 (即该层有4个神经元)\n",
    "# PyTorch会自动为我们创建 3x4 的权重矩阵和大小为4的偏置向量\n",
    "a_layer = nn.Linear(in_features=3, out_features=4)\n",
    "\n",
    "# 假设我们有一个批次(batch)的数据，包含2个样本，每个样本有3个特征\n",
    "# 形状为 (2, 3)\n",
    "batch_input = torch.randn(2, 3)\n",
    "print(f\"一个批次的输入数据 (shape: {batch_input.shape}):\\n{batch_input}\")\n",
    "\n",
    "# 数据流过这一层\n",
    "layer_output = a_layer(batch_input)\n",
    "print(f\"数据经过 nn.Linear 层后的输出 (shape: {layer_output.shape}):\\n{layer_output}\")\n",
    "# 注意：输出的形状变成了 (2, 4)，因为我们有2个样本，而层有4个神经元输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25733aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 实践 3: 串联层与激活函数 ---\n",
      "数据流过 '层 + 激活' 后的输出 (shape: torch.Size([2, 4])):\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.4935],\n",
      "        [0.5970, 0.0000, 0.0000, 0.7514]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# --- 实践 3: 将“层”和“激活函数”串联起来 ---\n",
    "print(\"--- 实践 3: 串联层与激活函数 ---\")\n",
    "# 使用 nn.Sequential 可以方便地将多个模块按顺序组合起来\n",
    "# 这就是一个最简单的单隐藏层神经网络的片段\n",
    "simple_network_fragment = nn.Sequential(\n",
    "    nn.Linear(in_features=3, out_features=4), # 线性层\n",
    "    nn.ReLU()                                # ReLU激活\n",
    ")\n",
    "\n",
    "# 让数据流过这个网络片段\n",
    "final_output = simple_network_fragment(batch_input)\n",
    "print(f\"数据流过 '层 + 激活' 后的输出 (shape: {final_output.shape}):\\n{final_output}\")\n",
    "# 观察输出，所有负数都被变成了0，这就是ReLU的作用！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f11a4",
   "metadata": {},
   "source": [
    "## **W2D1 今日行动清单**\n",
    "\n",
    "1.  **✅ 运行代码:** 亲手将上面的代码敲一遍或复制到环境中（如 Google Colab）运行一下，观察每一步的输入和输出，确保你理解其形状变化和数值变化。\n",
    "2.  **✅ 观看视频:** 观看 **3Blue1Brown 深度学习系列** 的 **第一集《神经网络的结构》**。建立直观理解。\n",
    "3.  **✅ 手绘网络:** 在纸上画一个 **2个输入 -\\> 3个神经元的隐藏层 -\\> 1个输出** 的网络结构图。\n",
    "4.  **✅ 思考题 (面试模拟):**\n",
    "      * **问题:** “如果一个多层神经网络中完全不使用激活函数，会发生什么？它的表达能力和什么模型是等价的？”\n",
    "      * **你的答案 (请先独立思考):**\n",
    "      * **参考答案:** 如果没有非线性激活函数，多层神经网络的每一次变换都是线性的（形如 $W \\cdot x + b$）。多个线性变换的连续叠加，其结果依然是一个线性变换。因此，无论网络有多少层，它最终都等价于一个**单层的线性模型**（如逻辑回归或线性回归）。它将失去拟合复杂非线性函数的能力，无法解决线性不可分的问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
